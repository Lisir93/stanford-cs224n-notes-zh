{"font_size":0.4,"font_color":"#FFFFFF","background_alpha":0.5,"background_color":"#9C27B0","Stroke":"none","body":[{"from":5.1,"to":9.24,"location":2,"content":"okay so great to see everyone back for"},{"from":9.24,"to":13.96,"location":2,"content":"lecture four of the class so for lect"},{"from":13.96,"to":17.32,"location":2,"content":"for today's lecture what I want to do"},{"from":17.32,"to":20.26,"location":2,"content":"for most of the time is actually get"},{"from":20.26,"to":23.68,"location":2,"content":"into the heart of these ideas of having"},{"from":23.68,"to":25.39,"location":2,"content":"the back propagation algorithm for"},{"from":25.39,"to":28.02,"location":2,"content":"neural nets and how we can construct"},{"from":28.02,"to":30.34,"location":2,"content":"computation graphs that allow us"},{"from":30.34,"to":33.88,"location":2,"content":"efficiently to do back propagation your"},{"from":33.88,"to":38.37,"location":2,"content":"nets to train the neural Nets so overall"},{"from":38.37,"to":40.75,"location":2,"content":"this is sort of what I plan to do it"},{"from":40.75,"to":43.12,"location":2,"content":"today so at the end of last lecture I"},{"from":43.12,"to":45.76,"location":2,"content":"slightly ran out of time and I started"},{"from":45.76,"to":48.12,"location":2,"content":"mumbling and waving my hands about the"},{"from":48.12,"to":50.59,"location":2,"content":"doing the derivatives with respect to"},{"from":50.59,"to":52.33,"location":2,"content":"the weight gradient so I kind of wanted"},{"from":52.33,"to":55.24,"location":2,"content":"to do that bit again so hopefully it"},{"from":55.24,"to":58.3,"location":2,"content":"actually communicates slightly better so"},{"from":58.3,"to":60.67,"location":2,"content":"we'll do that and talk a bit more about"},{"from":60.67,"to":62.77,"location":2,"content":"sort of just tips for doing matrix"},{"from":62.77,"to":65.8,"location":2,"content":"gradients and a particular issue that"},{"from":65.8,"to":68.35,"location":2,"content":"comes up with word vectors and so then"},{"from":68.35,"to":70.24,"location":2,"content":"the main part of the class will be"},{"from":70.24,"to":71.59,"location":2,"content":"talking about the back propagation"},{"from":71.59,"to":74.17,"location":2,"content":"algorithm and how it runs over"},{"from":74.17,"to":77.26,"location":2,"content":"computation graphs and then for the last"},{"from":77.26,"to":80.83,"location":2,"content":"part of the class is I'm not going to"},{"from":80.83,"to":81.91,"location":2,"content":"hide it"},{"from":81.91,"to":84.75,"location":2,"content":"this is sort of just a grab bag of"},{"from":84.75,"to":86.95,"location":2,"content":"miscellaneous stuff you should know"},{"from":86.95,"to":90.13,"location":2,"content":"about neural networks and training"},{"from":90.13,"to":94.6,"location":2,"content":"neural networks like I think you know we"},{"from":94.6,"to":96.19,"location":2,"content":"dream of a future of artificial"},{"from":96.19,"to":98.68,"location":2,"content":"intelligence where our machines are"},{"from":98.68,"to":100.87,"location":2,"content":"really intelligent and you can just say"},{"from":100.87,"to":102.7,"location":2,"content":"to them this is the data and this is my"},{"from":102.7,"to":105.55,"location":2,"content":"problem go and train me a model and it"},{"from":105.55,"to":108.67,"location":2,"content":"might work and in some future world that"},{"from":108.67,"to":111.16,"location":2,"content":"maybe what comes along it's something"},{"from":111.16,"to":112.96,"location":2,"content":"that's certainly being actively"},{"from":112.96,"to":115.27,"location":2,"content":"researched at the moment under the topic"},{"from":115.27,"to":117.91,"location":2,"content":"of auto ml I guess the question is"},{"from":117.91,"to":120.4,"location":2,"content":"whether it turns out that auto ml is a"},{"from":120.4,"to":123.12,"location":2,"content":"scalable solution or the climate change"},{"from":123.12,"to":126.1,"location":2,"content":"consequences of auto ml techniques are"},{"from":126.1,"to":128.17,"location":2,"content":"sufficiently bad that someone actually"},{"from":128.17,"to":131.85,"location":2,"content":"decides that these much lower power"},{"from":131.85,"to":134.77,"location":2,"content":"neural systems might actually be better"},{"from":134.77,"to":136.42,"location":2,"content":"still for doing some parts of the"},{"from":136.42,"to":137.95,"location":2,"content":"problem but"},{"from":137.95,"to":139.6,"location":2,"content":"either way we're not really there yet"},{"from":139.6,"to":142.75,"location":2,"content":"and the fact of the matter is when"},{"from":142.75,"to":144.46,"location":2,"content":"you're training neural networks there's"},{"from":144.46,"to":147.04,"location":2,"content":"just a whole bunch of stuff you have to"},{"from":147.04,"to":148.9,"location":2,"content":"know about initialization and"},{"from":148.9,"to":151,"location":2,"content":"nonlinearities and learning rates and so"},{"from":151,"to":154.15,"location":2,"content":"on and you know well I taught this class"},{"from":154.15,"to":159.34,"location":2,"content":"last time I somehow thought that people"},{"from":159.34,"to":161.8,"location":2,"content":"would pick this up by osmosis that if we"},{"from":161.8,"to":165.66,"location":2,"content":"get gave starter cut code to people and"},{"from":165.66,"to":169.86,"location":2,"content":"in our starter code we initialized how"},{"from":169.86,"to":172.42,"location":2,"content":"matrices and we set our learning rates"},{"from":172.42,"to":174.79,"location":2,"content":"that by osmosis people would understand"},{"from":174.79,"to":178.78,"location":2,"content":"that's what you have to do and do it and"},{"from":178.78,"to":181.81,"location":2,"content":"didn't really sort of teach in class the"},{"from":181.81,"to":184.09,"location":2,"content":"practical tips and tricks enough but it"},{"from":184.09,"to":185.71,"location":2,"content":"was perfectly obvious that when we got"},{"from":185.71,"to":189.04,"location":2,"content":"to final project time there at least for"},{"from":189.04,"to":191.44,"location":2,"content":"quite a few people osmosis hadn't worked"},{"from":191.44,"to":195.01,"location":2,"content":"so this time I'm at least willing to"},{"from":195.01,"to":197.5,"location":2,"content":"spend a few minutes on that and at least"},{"from":197.5,"to":199.27,"location":2,"content":"point out some of the things that are"},{"from":199.27,"to":202.93,"location":2,"content":"important and I mean just in general you"},{"from":202.93,"to":206.59,"location":2,"content":"know the reality of 2018 deep learning"},{"from":206.59,"to":210.64,"location":2,"content":"paths now wait 2019 now 2019 deep"},{"from":210.64,"to":213.43,"location":2,"content":"learning is deep learning is still kind"},{"from":213.43,"to":215.47,"location":2,"content":"of a craft there's quite a bit you have"},{"from":215.47,"to":218.38,"location":2,"content":"to know of techniques of doing things"},{"from":218.38,"to":220.9,"location":2,"content":"that lead neural net training to work"},{"from":220.9,"to":223.47,"location":2,"content":"successfully as opposed to your models"},{"from":223.47,"to":227.08,"location":2,"content":"failing to work successfully okay one"},{"from":227.08,"to":229.93,"location":2,"content":"final announcement and I go in through"},{"from":229.93,"to":233.41,"location":2,"content":"it so we've sort of been doing some"},{"from":233.41,"to":237.46,"location":2,"content":"further working on office our placement"},{"from":237.46,"to":239.05,"location":2,"content":"and I guess there are sort of multiple"},{"from":239.05,"to":241.75,"location":2,"content":"issues which include opportunities for"},{"from":241.75,"to":244.24,"location":2,"content":"local SCPD students without Stanford IDs"},{"from":244.24,"to":248.41,"location":2,"content":"we have to get to office hours so for"},{"from":248.41,"to":250.72,"location":2,"content":"the Thursday night office hour"},{"from":250.72,"to":253.39,"location":2,"content":"that's after this class if you'd like to"},{"from":253.39,"to":255.9,"location":2,"content":"go and talk about the second homework"},{"from":255.9,"to":258.64,"location":2,"content":"the Thursday night office hour is going"},{"from":258.64,"to":262.06,"location":2,"content":"to be in thought at Thornton 110 now I"},{"from":262.06,"to":264.7,"location":2,"content":"didn't know where Thornton was it made"},{"from":264.7,"to":267.19,"location":2,"content":"more sense to me when I translated that"},{"from":267.19,"to":269.44,"location":2,"content":"as that's the old Terman annex but"},{"from":269.44,"to":271.27,"location":2,"content":"that's probably just showing my age"},{"from":271.27,"to":273.76,"location":2,"content":"since probably none of you remember when"},{"from":273.76,"to":274.93,"location":2,"content":"there used to be a building called"},{"from":274.93,"to":276.7,"location":2,"content":"Thurmond so that probably doesn't help"},{"from":276.7,"to":279.28,"location":2,"content":"you either but you know if you're"},{"from":279.28,"to":281.17,"location":2,"content":"heading I don't know which direction"},{"from":281.17,"to":283.63,"location":2,"content":"we're facing if you're heading that way"},{"from":283.63,"to":286.75,"location":2,"content":"I guess and if you know where the pop"},{"from":286.75,"to":289.3,"location":2,"content":"your new guinea sculpture garden is the"},{"from":289.3,"to":292.24,"location":2,"content":"the sort of open grassy area before you"},{"from":292.24,"to":293.95,"location":2,"content":"get to the Papua New Guinea sculpture"},{"from":293.95,"to":294.55,"location":2,"content":"garden"},{"from":294.55,"to":296.62,"location":2,"content":"that's where Turman used to be and the"},{"from":296.62,"to":298.45,"location":2,"content":"building that still stands in there is"},{"from":298.45,"to":303.01,"location":2,"content":"thornton thornton 110 tonight i think it"},{"from":303.01,"to":308.44,"location":2,"content":"starts at 6:30 right 639 okay right so"},{"from":308.44,"to":310.33,"location":2,"content":"let me just finish off where we were"},{"from":310.33,"to":312.52,"location":2,"content":"last time so remember we had this window"},{"from":312.52,"to":315.43,"location":2,"content":"of five words and then we were putting"},{"from":315.43,"to":318.64,"location":2,"content":"it through a new net layer of Z equals W"},{"from":318.64,"to":321.43,"location":2,"content":"X plus B non-linearity of H equals f of"},{"from":321.43,"to":324.61,"location":2,"content":"X and then we are going to just get a"},{"from":324.61,"to":327.58,"location":2,"content":"score as to whether this has in its"},{"from":327.58,"to":331.39,"location":2,"content":"centre named entity like Paris which is"},{"from":331.39,"to":333.55,"location":2,"content":"sort of taking this dot product of a"},{"from":333.55,"to":335.35,"location":2,"content":"vector times the hidden layer so this"},{"from":335.35,"to":337.69,"location":2,"content":"was our model and then we were wanting"},{"from":337.69,"to":340.72,"location":2,"content":"to work out partial derivatives of s"},{"from":340.72,"to":343.15,"location":2,"content":"with respect to all of our variables and"},{"from":343.15,"to":346.18,"location":2,"content":"we did various of the cases but one we"},{"from":346.18,"to":348.28,"location":2,"content":"handy had done is the weights and the"},{"from":348.28,"to":349.84,"location":2,"content":"weights or all of this neural net layer"},{"from":349.84,"to":350.5,"location":2,"content":"here"},{"from":350.5,"to":354.37,"location":2,"content":"okay so chain rule the partial of the"},{"from":354.37,"to":360.07,"location":2,"content":"SDW of is the s times h d the h DZ times"},{"from":360.07,"to":364.03,"location":2,"content":"DZ d w and well if you remember last"},{"from":364.03,"to":367.21,"location":2,"content":"time we'd sort of done some computation"},{"from":367.21,"to":370.69,"location":2,"content":"of what those first two partial"},{"from":370.69,"to":373.33,"location":2,"content":"derivatives were and we said that we"},{"from":373.33,"to":376.54,"location":2,"content":"could just call those delta which is our"},{"from":376.54,"to":379.9,"location":2,"content":"error signal coming from above and that"},{"from":379.9,"to":381.79,"location":2,"content":"concept of having an error signal coming"},{"from":381.79,"to":384.1,"location":2,"content":"from above is something I'll get back to"},{"from":384.1,"to":385.54,"location":2,"content":"in the main part of the lecture and a"},{"from":385.54,"to":387.61,"location":2,"content":"sort of a central notion but the bit we"},{"from":387.61,"to":391.03,"location":2,"content":"haven't dealt with is this DZ DW and we"},{"from":391.03,"to":393.85,"location":2,"content":"started to look at it and I made the"},{"from":393.85,"to":397.72,"location":2,"content":"argument based on our shape convention"},{"from":397.72,"to":400.15,"location":2,"content":"that the shape of that should be the"},{"from":400.15,"to":402.73,"location":2,"content":"same shape as our W matrix so it should"},{"from":402.73,"to":404.98,"location":2,"content":"be the same in time"},{"from":404.98,"to":407.77,"location":2,"content":"in shape as this w matrix so we want to"},{"from":407.77,"to":412.99,"location":2,"content":"work out the partial of Z by W which is"},{"from":412.99,"to":419.92,"location":2,"content":"the same as this ZW x + B DW and so we"},{"from":419.92,"to":422.46,"location":2,"content":"want to work out what that derivative is"},{"from":422.46,"to":425.38,"location":2,"content":"and if that's not obvious one way to"},{"from":425.38,"to":428.2,"location":2,"content":"think about it is to go back to this"},{"from":428.2,"to":430.18,"location":2,"content":"elements of the matrix and actually"},{"from":430.18,"to":432.94,"location":2,"content":"first off work it out element wise and"},{"from":432.94,"to":435.31,"location":2,"content":"think out what it should be and then"},{"from":435.31,"to":436.69,"location":2,"content":"once you've thought out what it should"},{"from":436.69,"to":440.38,"location":2,"content":"be to rewrite it back in matrix form to"},{"from":440.38,"to":442.93,"location":2,"content":"give the compact answer so what we have"},{"from":442.93,"to":445.63,"location":2,"content":"is we have these inputs here and a bias"},{"from":445.63,"to":448.72,"location":2,"content":"term and we're going to do the matrix"},{"from":448.72,"to":451.06,"location":2,"content":"multiply of this vector to produce these"},{"from":451.06,"to":453.73,"location":2,"content":"and if you think about what's happening"},{"from":453.73,"to":456.37,"location":2,"content":"there so we've got this matrix of"},{"from":456.37,"to":459.37,"location":2,"content":"weights and for a particular weight a"},{"from":459.37,"to":462.22,"location":2,"content":"weight is first index is going to"},{"from":462.22,"to":465.37,"location":2,"content":"correspond to a position in the hidden"},{"from":465.37,"to":468.97,"location":2,"content":"layer and it's second index is going to"},{"from":468.97,"to":472.15,"location":2,"content":"correspond to a position in the input"},{"from":472.15,"to":475.96,"location":2,"content":"vector and one ways in the matrix ends"},{"from":475.96,"to":478.87,"location":2,"content":"up being part of what's used to compute"},{"from":478.87,"to":481.57,"location":2,"content":"one element of the hidden layer so the"},{"from":481.57,"to":483.49,"location":2,"content":"one element of the hidden layer you're"},{"from":483.49,"to":487.27,"location":2,"content":"taking a row of the matrix and you're"},{"from":487.27,"to":489.25,"location":2,"content":"multiplying it by the components of this"},{"from":489.25,"to":491.38,"location":2,"content":"vector so they sum together and the bias"},{"from":491.38,"to":493.51,"location":2,"content":"is added on but one element of the"},{"from":493.51,"to":495.94,"location":2,"content":"matrix is sort of only being used in a"},{"from":495.94,"to":498.93,"location":2,"content":"computation between one element of the"},{"from":498.93,"to":501.07,"location":2,"content":"important one element of the hidden"},{"from":501.07,"to":505.45,"location":2,"content":"vector okay so well that means if we're"},{"from":505.45,"to":507.04,"location":2,"content":"thinking about what's the partial"},{"from":507.04,"to":510.16,"location":2,"content":"derivative with respect to W IJ"},{"from":510.16,"to":516.45,"location":2,"content":"well it's only contributing to Zi and"},{"from":516.45,"to":521.04,"location":2,"content":"it's only it's only doing anything with"},{"from":521.04,"to":525.22,"location":2,"content":"XJ so that we end up with when we're"},{"from":525.22,"to":527.07,"location":2,"content":"getting the partial with respect to W IJ"},{"from":527.07,"to":530.11,"location":2,"content":"we can work that out with respect to"},{"from":530.11,"to":534.67,"location":2,"content":"just respect Zi and when we're going to"},{"from":534.67,"to":537.19,"location":2,"content":"look at this multiplication here what"},{"from":537.19,"to":538.81,"location":2,"content":"we're ending up is this sort of"},{"from":538.81,"to":542.95,"location":2,"content":"of terms wik times XK where there's sort"},{"from":542.95,"to":544.87,"location":2,"content":"of weights in that row of the matrix"},{"from":544.87,"to":547.39,"location":2,"content":"going across the positions of the vector"},{"from":547.39,"to":551.53,"location":2,"content":"so the only position in which W IJ is"},{"from":551.53,"to":557.62,"location":2,"content":"used is multiplying by XJ and at that"},{"from":557.62,"to":560.29,"location":2,"content":"point what we have in terms of sort of"},{"from":560.29,"to":563.14,"location":2,"content":"you know basic one variable doing a"},{"from":563.14,"to":565.48,"location":2,"content":"differentiation this is just like we"},{"from":565.48,"to":568.84,"location":2,"content":"have three X and we say what's the"},{"from":568.84,"to":571.51,"location":2,"content":"derivative of 3x max just X is confusing"},{"from":571.51,"to":573.34,"location":2,"content":"sorry I shouldn't say that it's like we"},{"from":573.34,"to":576.04,"location":2,"content":"have three W and what's the derivative"},{"from":576.04,"to":579.01,"location":2,"content":"of three W with respect to W it's three"},{"from":579.01,"to":582.22,"location":2,"content":"right so that we've have a term here"},{"from":582.22,"to":585.85,"location":2,"content":"which is will have been W will be W IJ"},{"from":585.85,"to":589.21,"location":2,"content":"times XJ and its derivative with respect"},{"from":589.21,"to":594.01,"location":2,"content":"to W IJ is just XJ that makes sense if"},{"from":594.01,"to":597.22,"location":2,"content":"you want to leave it fingers crossed"},{"from":597.22,"to":601.03,"location":2,"content":"okay so so for one element of this"},{"from":601.03,"to":604.57,"location":2,"content":"matrix we're just getting out XJ and at"},{"from":604.57,"to":608.77,"location":2,"content":"that point we say well of course we want"},{"from":608.77,"to":612.04,"location":2,"content":"to know what the Jacobian is for the"},{"from":612.04,"to":614.95,"location":2,"content":"full matrix W well if you start thinking"},{"from":614.95,"to":617.65,"location":2,"content":"about it this argument applies to every"},{"from":617.65,"to":622.33,"location":2,"content":"cell so that for every cell of the"},{"from":622.33,"to":626.68,"location":2,"content":"Jacobian for W it's going to be XJ so"},{"from":626.68,"to":631.72,"location":2,"content":"that means we're just going to be able"},{"from":631.72,"to":634.18,"location":2,"content":"to make use of that in calculating our"},{"from":634.18,"to":636.73,"location":2,"content":"Jacobian so the derivative for a single"},{"from":636.73,"to":641.23,"location":2,"content":"W IJ is Delta I XJ and that's true for"},{"from":641.23,"to":645.04,"location":2,"content":"all cells so we want to have a matrix"},{"from":645.04,"to":650.35,"location":2,"content":"for our Jacobian which has Delta I XJ in"},{"from":650.35,"to":652.48,"location":2,"content":"every cell Evert and the way we can"},{"from":652.48,"to":655.66,"location":2,"content":"create that is by using an outer product"},{"from":655.66,"to":659.89,"location":2,"content":"so if we have a row vector of the deltas"},{"from":659.89,"to":662.23,"location":2,"content":"the error signals from above and a"},{"from":662.23,"to":666.34,"location":2,"content":"column wait I say that wrong sorry if we"},{"from":666.34,"to":671.08,"location":2,"content":"have a column of the Delta arrow signals"},{"from":671.08,"to":672.7,"location":2,"content":"from above"},{"from":672.7,"to":678.01,"location":2,"content":"we have a row of X transpose vector when"},{"from":678.01,"to":680.83,"location":2,"content":"we multiply those together we get the"},{"from":680.83,"to":683.8,"location":2,"content":"outer product and we get Delta I XJ in"},{"from":683.8,"to":686.23,"location":2,"content":"each cell and that is our to Co be an"},{"from":686.23,"to":691.21,"location":2,"content":"answer for working out the Delta s Delta"},{"from":691.21,"to":692.95,"location":2,"content":"W that we started off with at the"},{"from":692.95,"to":697.42,"location":2,"content":"beginning okay and this and we get this"},{"from":697.42,"to":700.15,"location":2,"content":"form where it's a multiplication of an"},{"from":700.15,"to":703.12,"location":2,"content":"error signal from above and our computed"},{"from":703.12,"to":705.4,"location":2,"content":"local gradient signal and that's the"},{"from":705.4,"to":707.32,"location":2,"content":"pattern that we're going to see over and"},{"from":707.32,"to":709.6,"location":2,"content":"over again and that will exploit in our"},{"from":709.6,"to":714.7,"location":2,"content":"computation graphs okay all good okay"},{"from":714.7,"to":720.73,"location":2,"content":"so here's just you know homework two"},{"from":720.73,"to":722.88,"location":2,"content":"you're meant to do some of this stuff"},{"from":722.88,"to":725.98,"location":2,"content":"here it s sort of a couple of collected"},{"from":725.98,"to":730.33,"location":2,"content":"tips which I hope will help I mean"},{"from":730.33,"to":733.45,"location":2,"content":"keeping here track of your variables and"},{"from":733.45,"to":735.82,"location":2,"content":"their dimensionality is really useful"},{"from":735.82,"to":737.17,"location":2,"content":"because if you just can work out what"},{"from":737.17,"to":738.67,"location":2,"content":"the dimensionality of things should be"},{"from":738.67,"to":741.28,"location":2,"content":"you're often kind of halfway there"},{"from":741.28,"to":743.53,"location":2,"content":"I mean basically what you're doing is"},{"from":743.53,"to":746.26,"location":2,"content":"sort of applying the chain rule over and"},{"from":746.26,"to":749.23,"location":2,"content":"over again it always looks like this but"},{"from":749.23,"to":751.66,"location":2,"content":"doing it in this sort of matrix calculus"},{"from":751.66,"to":755.83,"location":2,"content":"sense of the chain rule in the homework"},{"from":755.83,"to":757.87,"location":2,"content":"you have to do a softmax which we"},{"from":757.87,"to":760.27,"location":2,"content":"haven't done in class something that I"},{"from":760.27,"to":762.79,"location":2,"content":"think you'll find useful if you want to"},{"from":762.79,"to":765.49,"location":2,"content":"break apart the softmax is to consider"},{"from":765.49,"to":769.24,"location":2,"content":"two cases one the cases - when you're"},{"from":769.24,"to":771.58,"location":2,"content":"working it out for the correct class and"},{"from":771.58,"to":774.64,"location":2,"content":"then the other case is for all the other"},{"from":774.64,"to":779.8,"location":2,"content":"incorrect classes yeah I'm in the in the"},{"from":779.8,"to":782.5,"location":2,"content":"little derivation I did before I said"},{"from":782.5,"to":784.54,"location":2,"content":"well let's work out an element-wise"},{"from":784.54,"to":786.82,"location":2,"content":"partial derivative because that should"},{"from":786.82,"to":788.53,"location":2,"content":"give me some sense of what's going on"},{"from":788.53,"to":790.66,"location":2,"content":"what the answer is I think that can be a"},{"from":790.66,"to":792.4,"location":2,"content":"really good thing to do if you're"},{"from":792.4,"to":794.62,"location":2,"content":"getting confused by matrix calculus and"},{"from":794.62,"to":800.02,"location":2,"content":"I sort of slightly skip past another"},{"from":800.02,"to":802.42,"location":2,"content":"slide last time I was talking about the"},{"from":802.42,"to":804.1,"location":2,"content":"shape convention that I talked about it"},{"from":804.1,"to":805.9,"location":2,"content":"for a moment but for"},{"from":805.9,"to":809.65,"location":2,"content":"or the homeworks you can work out your"},{"from":809.65,"to":812.14,"location":2,"content":"answer however you want you can work it"},{"from":812.14,"to":814.36,"location":2,"content":"out in terms of you know numerator"},{"from":814.36,"to":816.73,"location":2,"content":"ordered jacobians if that seems best to"},{"from":816.73,"to":819.25,"location":2,"content":"you but we'd like you to give the final"},{"from":819.25,"to":821.35,"location":2,"content":"answer to your assignment questions"},{"from":821.35,"to":824.26,"location":2,"content":"following the shape convention so that"},{"from":824.26,"to":826.81,"location":2,"content":"the derivatives should be shaped in a"},{"from":826.81,"to":829.9,"location":2,"content":"vector or matrix in the same way as the"},{"from":829.9,"to":832.03,"location":2,"content":"variable with respect to which you're"},{"from":832.03,"to":835.08,"location":2,"content":"working out your derivatives"},{"from":835.08,"to":838.78,"location":2,"content":"okay the last little bit for finishing"},{"from":838.78,"to":840.55,"location":2,"content":"up this example from last time I want to"},{"from":840.55,"to":842.89,"location":2,"content":"say a little bit about is what happens"},{"from":842.89,"to":847.27,"location":2,"content":"with words and one answer is nothing"},{"from":847.27,"to":850,"location":2,"content":"different but another answer is they are"},{"from":850,"to":851.62,"location":2,"content":"a little bit of a special case here"},{"from":851.62,"to":855.19,"location":2,"content":"because you know really we have a matrix"},{"from":855.19,"to":857.41,"location":2,"content":"of word vectors right we have a vector"},{"from":857.41,"to":860.71,"location":2,"content":"for each word and so then you can think"},{"from":860.71,"to":862.93,"location":2,"content":"of that as sort of this matrix of word"},{"from":862.93,"to":864.79,"location":2,"content":"vectors which row as a different word"},{"from":864.79,"to":867.64,"location":2,"content":"but we're not actually kind of"},{"from":867.64,"to":871,"location":2,"content":"connecting up that matrix directly to"},{"from":871,"to":873.79,"location":2,"content":"our classifier system instead of that"},{"from":873.79,"to":875.44,"location":2,"content":"what we connect connecting up to the"},{"from":875.44,"to":878.56,"location":2,"content":"classifier system is this window and the"},{"from":878.56,"to":880.93,"location":2,"content":"window we'll have it in at five words"},{"from":880.93,"to":883.24,"location":2,"content":"most commonly they're different words"},{"from":883.24,"to":885.13,"location":2,"content":"but you know occasionally the same word"},{"from":885.13,"to":888.16,"location":2,"content":"might appear in two positions in that"},{"from":888.16,"to":892.09,"location":2,"content":"window and so we can nevertheless do"},{"from":892.09,"to":894.37,"location":2,"content":"exactly the same thing and continue our"},{"from":894.37,"to":898.51,"location":2,"content":"gradients down and say okay let's work"},{"from":898.51,"to":902.5,"location":2,"content":"out the gradients of this word window"},{"from":902.5,"to":906.58,"location":2,"content":"vector and if these are of dimension D"},{"from":906.58,"to":909.52,"location":2,"content":"we'll have this sort of five D vector"},{"from":909.52,"to":912.73,"location":2,"content":"but you know then what do we do about it"},{"from":912.73,"to":915.1,"location":2,"content":"and the answer to what we do about it is"},{"from":915.1,"to":918.67,"location":2,"content":"we can just sort of split this window"},{"from":918.67,"to":921.97,"location":2,"content":"vector into five pieces and say aha"},{"from":921.97,"to":924.76,"location":2,"content":"we have five updates to word vectors"},{"from":924.76,"to":926.59,"location":2,"content":"we're just going to go off and apply"},{"from":926.59,"to":931.51,"location":2,"content":"them to the words vector matrix and you"},{"from":931.51,"to":934.47,"location":2,"content":"know if we if the same word occurs twice"},{"from":934.47,"to":938.14,"location":2,"content":"in that window we literally apply both"},{"from":938.14,"to":939.16,"location":2,"content":"of the updates so"},{"from":939.16,"to":941.62,"location":2,"content":"it is it's updated twice or maybe"},{"from":941.62,"to":943.15,"location":2,"content":"actually you want to sum them first and"},{"from":943.15,"to":944.89,"location":2,"content":"then do the update once but yeah that's"},{"from":944.89,"to":949.51,"location":2,"content":"a technical issue so so what that"},{"from":949.51,"to":951.87,"location":2,"content":"actually means is that we're extremely"},{"from":951.87,"to":956.38,"location":2,"content":"sparsely updating the word vector matrix"},{"from":956.38,"to":958.21,"location":2,"content":"because most of the word vector matrix"},{"from":958.21,"to":961.06,"location":2,"content":"will be unchanged and just a few rows of"},{"from":961.06,"to":964.81,"location":2,"content":"it will be being updated and if soon"},{"from":964.81,"to":966.37,"location":2,"content":"we're going to be you know doing stuff"},{"from":966.37,"to":969.4,"location":2,"content":"with PI torch and if you poke around PI"},{"from":969.4,"to":972.43,"location":2,"content":"torch even has some special stuff look"},{"from":972.43,"to":975.37,"location":2,"content":"for things like sparse SGD for meaning"},{"from":975.37,"to":977.05,"location":2,"content":"that you're sort of doing a very sparse"},{"from":977.05,"to":981.1,"location":2,"content":"updating like that but there's one other"},{"from":981.1,"to":983.77,"location":2,"content":"sort of interesting thing that you"},{"from":983.77,"to":986.65,"location":2,"content":"should know about for a lot of things"},{"from":986.65,"to":988.42,"location":2,"content":"that you do is just what actually"},{"from":988.42,"to":991.3,"location":2,"content":"happens if we push down these gradients"},{"from":991.3,"to":994.78,"location":2,"content":"into our word vectors well the idea is"},{"from":994.78,"to":996.94,"location":2,"content":"you know if we do that would be just"},{"from":996.94,"to":999.34,"location":2,"content":"like all other neural net learning that"},{"from":999.34,"to":1003.27,"location":2,"content":"we will sort of in principle say move"},{"from":1003.27,"to":1006.57,"location":2,"content":"the word vectors around in such a way as"},{"from":1006.57,"to":1009.51,"location":2,"content":"they're more useful in helping determine"},{"from":1009.51,"to":1012.21,"location":2,"content":"named entity classification in this case"},{"from":1012.21,"to":1014.21,"location":2,"content":"because that was our motivating example"},{"from":1014.21,"to":1017.13,"location":2,"content":"so you know it might for example learn"},{"from":1017.13,"to":1019.77,"location":2,"content":"that the word in is a very good"},{"from":1019.77,"to":1023.28,"location":2,"content":"indicator of named into default I'm"},{"from":1023.28,"to":1025.53,"location":2,"content":"sorry the place name following so after"},{"from":1025.53,"to":1028.35,"location":2,"content":"in you often get London Paris etc right"},{"from":1028.35,"to":1030,"location":2,"content":"so it's sort of got a special behavior"},{"from":1030,"to":1032.67,"location":2,"content":"that other prepositions don't as being a"},{"from":1032.67,"to":1035.16,"location":2,"content":"good location indicator and so it could"},{"from":1035.16,"to":1038.49,"location":2,"content":"sort of move its location around and say"},{"from":1038.49,"to":1041.25,"location":2,"content":"here are words that are good location"},{"from":1041.25,"to":1043.41,"location":2,"content":"indicators and therefore help our"},{"from":1043.41,"to":1046.71,"location":2,"content":"classifier work even better so on"},{"from":1046.71,"to":1048.99,"location":2,"content":"principle that's good and it's a good"},{"from":1048.99,"to":1052.14,"location":2,"content":"thing to do to update word vectors to"},{"from":1052.14,"to":1054.69,"location":2,"content":"help you perform better on a supervised"},{"from":1054.69,"to":1057.39,"location":2,"content":"tasks such as this named entity"},{"from":1057.39,"to":1060.27,"location":2,"content":"recognition classification but there's a"},{"from":1060.27,"to":1063.51,"location":2,"content":"catch which is that it doesn't always"},{"from":1063.51,"to":1066.12,"location":2,"content":"work actually and so why doesn't it"},{"from":1066.12,"to":1068.46,"location":2,"content":"always work well suppose that we're"},{"from":1068.46,"to":1071.79,"location":2,"content":"training a classifier you know it could"},{"from":1071.79,"to":1072.61,"location":2,"content":"be the"},{"from":1072.61,"to":1075.4,"location":2,"content":"one I just did or a softmax or logistic"},{"from":1075.4,"to":1079.11,"location":2,"content":"regression and we're wanting to classify"},{"from":1079.11,"to":1081.88,"location":2,"content":"movie reviews sentiment for positive or"},{"from":1081.88,"to":1085.69,"location":2,"content":"negative well you know if we have"},{"from":1085.69,"to":1088.3,"location":2,"content":"trained our word vectors we've got some"},{"from":1088.3,"to":1090.97,"location":2,"content":"word vector space and maybe in the word"},{"from":1090.97,"to":1094.87,"location":2,"content":"vector space TV telly and television are"},{"from":1094.87,"to":1097.15,"location":2,"content":"all very close together because they"},{"from":1097.15,"to":1100,"location":2,"content":"mean basically the same thing so that's"},{"from":1100,"to":1103.18,"location":2,"content":"great our word vectors are good but well"},{"from":1103.18,"to":1106.12,"location":2,"content":"suppose it was the case that in our"},{"from":1106.12,"to":1109.09,"location":2,"content":"training data for our classifier so this"},{"from":1109.09,"to":1111.76,"location":2,"content":"is our training data for movie sentiment"},{"from":1111.76,"to":1115.81,"location":2,"content":"review we had the word TV and telly but"},{"from":1115.81,"to":1118.72,"location":2,"content":"we didn't have the word television well"},{"from":1118.72,"to":1121.32,"location":2,"content":"then what's going to happen is well"},{"from":1121.32,"to":1124.27,"location":2,"content":"while we try and train our sentiment"},{"from":1124.27,"to":1127.69,"location":2,"content":"classifier if we push gradient back down"},{"from":1127.69,"to":1130.36,"location":2,"content":"into the word vectors what's likely to"},{"from":1130.36,"to":1133.9,"location":2,"content":"happen is that it will move around the"},{"from":1133.9,"to":1136.63,"location":2,"content":"word vectors of the words we saw in the"},{"from":1136.63,"to":1139.12,"location":2,"content":"training data but necessarily"},{"from":1139.12,"to":1141.22,"location":2,"content":"televisions not moving right because"},{"from":1141.22,"to":1143.38,"location":2,"content":"we're only pushing gradient down to"},{"from":1143.38,"to":1145.6,"location":2,"content":"words or in our training data so this"},{"from":1145.6,"to":1147.91,"location":2,"content":"word goes nowhere so it just stays where"},{"from":1147.91,"to":1150.79,"location":2,"content":"it was all along so if the result of our"},{"from":1150.79,"to":1153.91,"location":2,"content":"training is words get moved around so"},{"from":1153.91,"to":1155.5,"location":2,"content":"here are good words for indicating"},{"from":1155.5,"to":1159.19,"location":2,"content":"negative sentiment will actually if a"},{"from":1159.19,"to":1162.1,"location":2,"content":"test time when we're running our model"},{"from":1162.1,"to":1164.2,"location":2,"content":"if we evaluate on a sentence with"},{"from":1164.2,"to":1166.12,"location":2,"content":"television in it it's actually going to"},{"from":1166.12,"to":1168.55,"location":2,"content":"give the wrong answer whereas if we"},{"from":1168.55,"to":1171.13,"location":2,"content":"haven't changed the word vectors at all"},{"from":1171.13,"to":1175.09,"location":2,"content":"and it just left them where our word"},{"from":1175.09,"to":1177.82,"location":2,"content":"embedding learning system put them then"},{"from":1177.82,"to":1179.86,"location":2,"content":"it would have said television that's a"},{"from":1179.86,"to":1182.08,"location":2,"content":"word that means about the same as TV or"},{"from":1182.08,"to":1183.97,"location":2,"content":"tele I should treat it the same in my"},{"from":1183.97,"to":1185.98,"location":2,"content":"sentiment classifier and it would"},{"from":1185.98,"to":1189.06,"location":2,"content":"actually do a better job so it's sort of"},{"from":1189.06,"to":1191.86,"location":2,"content":"two-sided whether you've gained by"},{"from":1191.86,"to":1195.79,"location":2,"content":"training word vectors and so this is a"},{"from":1195.79,"to":1198.91,"location":2,"content":"summary that says that it's two-sided"},{"from":1198.91,"to":1202.03,"location":2,"content":"and practically what you should do so"},{"from":1202.03,"to":1204.84,"location":2,"content":"the first choice is gee"},{"from":1204.84,"to":1208.02,"location":2,"content":"it's a good idea to use pre-trained word"},{"from":1208.02,"to":1210.15,"location":2,"content":"vectors like the word to vech vech ters"},{"from":1210.15,"to":1213.09,"location":2,"content":"that used an assignment one or using the"},{"from":1213.09,"to":1215.19,"location":2,"content":"training methods that you're doing right"},{"from":1215.19,"to":1218.16,"location":2,"content":"now for homework two and the answer that"},{"from":1218.16,"to":1221.7,"location":2,"content":"is almost always yes and the reason for"},{"from":1221.7,"to":1223.98,"location":2,"content":"that is these word vector training"},{"from":1223.98,"to":1227.82,"location":2,"content":"methods are extremely easy to run on"},{"from":1227.82,"to":1230.22,"location":2,"content":"billions and words of text so we you"},{"from":1230.22,"to":1232.02,"location":2,"content":"know train these models like glove or"},{"from":1232.02,"to":1234.57,"location":2,"content":"word thick on billions or tens of"},{"from":1234.57,"to":1236.76,"location":2,"content":"billions of words and it's easy to do"},{"from":1236.76,"to":1239.28,"location":2,"content":"that for two reasons firstly because the"},{"from":1239.28,"to":1241.14,"location":2,"content":"training algorithms are very simple"},{"from":1241.14,"to":1243.72,"location":2,"content":"right that the word defect training"},{"from":1243.72,"to":1246,"location":2,"content":"algorithm skip Graham's very simple"},{"from":1246,"to":1249.36,"location":2,"content":"algorithm secondly because we don't need"},{"from":1249.36,"to":1251.43,"location":2,"content":"any expensive resources all we need is a"},{"from":1251.43,"to":1253.56,"location":2,"content":"big pile of text documents and we can"},{"from":1253.56,"to":1256.35,"location":2,"content":"run it on them so really easy to run it"},{"from":1256.35,"to":1258.77,"location":2,"content":"on you know five or fifty billion words"},{"from":1258.77,"to":1261.9,"location":2,"content":"whereas you know we can't do that for"},{"from":1261.9,"to":1263.61,"location":2,"content":"most of the classifiers that we want to"},{"from":1263.61,"to":1264.87,"location":2,"content":"build because if it's something like a"},{"from":1264.87,"to":1266.64,"location":2,"content":"sentiment classifier or a named Indy"},{"from":1266.64,"to":1269.67,"location":2,"content":"recognizer we need label training data"},{"from":1269.67,"to":1272.73,"location":2,"content":"to train our classifier and then we ask"},{"from":1272.73,"to":1274.56,"location":2,"content":"someone or how many words of label"},{"from":1274.56,"to":1276.42,"location":2,"content":"training data do you have for named"},{"from":1276.42,"to":1278.7,"location":2,"content":"entity recognition and they give us back"},{"from":1278.7,"to":1280.44,"location":2,"content":"a number like three hundred thousand"},{"from":1280.44,"to":1282.18,"location":2,"content":"words or 1 million words right it's"},{"from":1282.18,"to":1286.56,"location":2,"content":"orders of magnitude smaller ok so"},{"from":1286.56,"to":1289.47,"location":2,"content":"therefore we can gain using pre trained"},{"from":1289.47,"to":1291.75,"location":2,"content":"word vectors because they know about all"},{"from":1291.75,"to":1293.85,"location":2,"content":"the words that aren't in our supervised"},{"from":1293.85,"to":1296.19,"location":2,"content":"classifiers training data and they also"},{"from":1296.19,"to":1297.72,"location":2,"content":"know much more about the words that"},{"from":1297.72,"to":1299.61,"location":2,"content":"actually are in the training data but"},{"from":1299.61,"to":1302.37,"location":2,"content":"only rarely so the exception to that is"},{"from":1302.37,"to":1304.32,"location":2,"content":"if you have hundreds of millions of"},{"from":1304.32,"to":1306.45,"location":2,"content":"words of data then you can start off"},{"from":1306.45,"to":1309.48,"location":2,"content":"with random word vectors and go from"},{"from":1309.48,"to":1311.25,"location":2,"content":"there and so a case where this is"},{"from":1311.25,"to":1313.53,"location":2,"content":"actually commonly done is for machine"},{"from":1313.53,"to":1315.36,"location":2,"content":"translation which we do later in the"},{"from":1315.36,"to":1318.77,"location":2,"content":"class it's relatively easy for large"},{"from":1318.77,"to":1321.63,"location":2,"content":"languages to get hundreds of millions of"},{"from":1321.63,"to":1323.7,"location":2,"content":"words of translated text if you want to"},{"from":1323.7,"to":1326.64,"location":2,"content":"build something like German English or"},{"from":1326.64,"to":1328.89,"location":2,"content":"Chinese English machine translation"},{"from":1328.89,"to":1331.62,"location":2,"content":"system not hard to get a hundred and"},{"from":1331.62,"to":1334.2,"location":2,"content":"fifty million words of translated text"},{"from":1334.2,"to":1336,"location":2,"content":"and so that's sort of sufficiently much"},{"from":1336,"to":1338.55,"location":2,"content":"data that people commonly just"},{"from":1338.55,"to":1341.88,"location":2,"content":"start with word vectors being randomly"},{"from":1341.88,"to":1345,"location":2,"content":"initialized and start training their"},{"from":1345,"to":1348.78,"location":2,"content":"translation system okay so in the second"},{"from":1348.78,"to":1350.79,"location":2,"content":"question is okay I'm using pre trained"},{"from":1350.79,"to":1354.54,"location":2,"content":"word vectors when I train my supervised"},{"from":1354.54,"to":1357.42,"location":2,"content":"classifier should I push gradients down"},{"from":1357.42,"to":1359.64,"location":2,"content":"into the word vectors and up and update"},{"from":1359.64,"to":1361.38,"location":2,"content":"them which is often referred to as"},{"from":1361.38,"to":1365.31,"location":2,"content":"fine-tuning the word vectors or should I"},{"from":1365.31,"to":1367.56,"location":2,"content":"not should I just sort of throw away"},{"from":1367.56,"to":1369.06,"location":2,"content":"those gradients and not push them down"},{"from":1369.06,"to":1371.97,"location":2,"content":"into the word vectors and you know the"},{"from":1371.97,"to":1374.01,"location":2,"content":"answer that is it depends and it just"},{"from":1374.01,"to":1376.44,"location":2,"content":"depends on the size so if you only have"},{"from":1376.44,"to":1379.82,"location":2,"content":"a small training data set"},{"from":1379.82,"to":1382.89,"location":2,"content":"typically it's best to just treat the"},{"from":1382.89,"to":1386.46,"location":2,"content":"pre trained word vectors as fixed and"},{"from":1386.46,"to":1388.77,"location":2,"content":"not do any updating of them at all if"},{"from":1388.77,"to":1392.22,"location":2,"content":"you have a large data set then you can"},{"from":1392.22,"to":1395.31,"location":2,"content":"normally gain by doing fine-tuning of"},{"from":1395.31,"to":1397.38,"location":2,"content":"the word vectors and of course the"},{"from":1397.38,"to":1400.53,"location":2,"content":"answer here is what counts as large you"},{"from":1400.53,"to":1402.63,"location":2,"content":"know if certainly if you're down in the"},{"from":1402.63,"to":1404.37,"location":2,"content":"regime of a hundred thousand words a"},{"from":1404.37,"to":1406.02,"location":2,"content":"couple hundred thousand words you're"},{"from":1406.02,"to":1409.23,"location":2,"content":"small if your time to be over a million"},{"from":1409.23,"to":1411.21,"location":2,"content":"words then maybe you're large but you"},{"from":1411.21,"to":1412.65,"location":2,"content":"know and practice people do it both ways"},{"from":1412.65,"to":1414.18,"location":2,"content":"and see which number is higher and"},{"from":1414.18,"to":1419.28,"location":2,"content":"that's what they stick with ya then the"},{"from":1419.28,"to":1422.13,"location":2,"content":"sort of there's the sort of point here"},{"from":1422.13,"to":1424.92,"location":2,"content":"that's just worth underlining is yeah so"},{"from":1424.92,"to":1427.83,"location":2,"content":"in principle we can back propagate this"},{"from":1427.83,"to":1431.66,"location":2,"content":"gradient to every variable in our model"},{"from":1431.66,"to":1435.75,"location":2,"content":"it's actually a theorem that we can"},{"from":1435.75,"to":1438.81,"location":2,"content":"arbitrarily decide to throw any subset"},{"from":1438.81,"to":1443.88,"location":2,"content":"of those gradients away and we're still"},{"from":1443.88,"to":1446.73,"location":2,"content":"improving the log likelihood of our"},{"from":1446.73,"to":1448.71,"location":2,"content":"model right it kind of can't be"},{"from":1448.71,"to":1450.99,"location":2,"content":"inconsistent you can just so pick some"},{"from":1450.99,"to":1453.6,"location":2,"content":"subset and say only train those 37 and"},{"from":1453.6,"to":1455.37,"location":2,"content":"throw away all the rest and the"},{"from":1455.37,"to":1457.95,"location":2,"content":"algorithm will still improve the log"},{"from":1457.95,"to":1459.75,"location":2,"content":"likelihood of the model perhaps not by"},{"from":1459.75,"to":1461.49,"location":2,"content":"as much as if you train the rest of the"},{"from":1461.49,"to":1464.58,"location":2,"content":"variables as well but yeah it can't"},{"from":1464.58,"to":1466.2,"location":2,"content":"actually do any harm not to train"},{"from":1466.2,"to":1468.75,"location":2,"content":"anything that's one of the reasons why"},{"from":1468.75,"to":1470.91,"location":2,"content":"often people don't notice bugs in their"},{"from":1470.91,"to":1472.62,"location":2,"content":"code as well is"},{"from":1472.62,"to":1474.39,"location":2,"content":"because if your code is kind of broken"},{"from":1474.39,"to":1476.73,"location":2,"content":"and only half of the variables are being"},{"from":1476.73,"to":1479.04,"location":2,"content":"updated it'll still seem to be training"},{"from":1479.04,"to":1481.35,"location":2,"content":"something and improving it's just not"},{"from":1481.35,"to":1483.36,"location":2,"content":"doing as well as it could be doing if"},{"from":1483.36,"to":1487.08,"location":2,"content":"you'd code it correctly okay"},{"from":1487.08,"to":1491.34,"location":2,"content":"so at this point that sort of almost"},{"from":1491.34,"to":1494.04,"location":2,"content":"shown you back propagation right so back"},{"from":1494.04,"to":1496.8,"location":2,"content":"propagation is really taking derivatives"},{"from":1496.8,"to":1499.35,"location":2,"content":"with a generalized chain role with the"},{"from":1499.35,"to":1501.63,"location":2,"content":"one further trick which we sort of"},{"from":1501.63,"to":1504.51,"location":2,"content":"represented with that Delta which is G"},{"from":1504.51,"to":1508.47,"location":2,"content":"you want to be clever in doing this so"},{"from":1508.47,"to":1510.81,"location":2,"content":"you minimize computation by reusing"},{"from":1510.81,"to":1514.77,"location":2,"content":"shared stuff but now what I want to move"},{"from":1514.77,"to":1516.51,"location":2,"content":"on is to sort of look at how we can do"},{"from":1516.51,"to":1519.21,"location":2,"content":"that much more systematically which is"},{"from":1519.21,"to":1521.37,"location":2,"content":"this idea we have a computation graph"},{"from":1521.37,"to":1522.75,"location":2,"content":"and we're going to run a back"},{"from":1522.75,"to":1524.31,"location":2,"content":"propagation algorithm through the"},{"from":1524.31,"to":1529.5,"location":2,"content":"computation graph so this is kind of"},{"from":1529.5,"to":1534.36,"location":2,"content":"like an abstract syntax tree expression"},{"from":1534.36,"to":1536.49,"location":2,"content":"tree that you might see in a compilers"},{"from":1536.49,"to":1538.56,"location":2,"content":"class or something like that right so"},{"from":1538.56,"to":1542.31,"location":2,"content":"when we have an arithmetic expression of"},{"from":1542.31,"to":1544.35,"location":2,"content":"the kind that we're going to compute we"},{"from":1544.35,"to":1546.57,"location":2,"content":"can make this tipped over on its side"},{"from":1546.57,"to":1549.51,"location":2,"content":"tree representation so we've got the X"},{"from":1549.51,"to":1551.88,"location":2,"content":"and W variables we're going to multiply"},{"from":1551.88,"to":1554.13,"location":2,"content":"them there's the B variable we're going"},{"from":1554.13,"to":1556.11,"location":2,"content":"to add it to the previous partial result"},{"from":1556.11,"to":1557.58,"location":2,"content":"we're going to stick it through our"},{"from":1557.58,"to":1559.89,"location":2,"content":"non-linearity F and then we're going to"},{"from":1559.89,"to":1561.87,"location":2,"content":"multiply it by U and that was the"},{"from":1561.87,"to":1564.18,"location":2,"content":"computation that we're doing in our"},{"from":1564.18,"to":1567.84,"location":2,"content":"neural network so the source nodes are"},{"from":1567.84,"to":1570.6,"location":2,"content":"inputs the interior nodes of this tree"},{"from":1570.6,"to":1573.42,"location":2,"content":"are operations and then we've got these"},{"from":1573.42,"to":1576.12,"location":2,"content":"edges that pass along the results of our"},{"from":1576.12,"to":1578.04,"location":2,"content":"computation and so this is the"},{"from":1578.04,"to":1580.11,"location":2,"content":"computation graph for precisely the"},{"from":1580.11,"to":1582.27,"location":2,"content":"example I've been doing for the last"},{"from":1582.27,"to":1585.96,"location":2,"content":"lecture and electron or Eilish okay so"},{"from":1585.96,"to":1587.64,"location":2,"content":"there are two things that we want to be"},{"from":1587.64,"to":1590.46,"location":2,"content":"able to do the first one is we want to"},{"from":1590.46,"to":1592.23,"location":2,"content":"be able to start with these variables"},{"from":1592.23,"to":1595.02,"location":2,"content":"and do this computation and calculate"},{"from":1595.02,"to":1597.54,"location":2,"content":"what s is that's the part that's dead"},{"from":1597.54,"to":1600.84,"location":2,"content":"simple that's referred to as forward"},{"from":1600.84,"to":1603.3,"location":2,"content":"propagation so forward propagation is"},{"from":1603.3,"to":1606.54,"location":2,"content":"just expression evaluation as you do"},{"from":1606.54,"to":1608.76,"location":2,"content":"in any programming language interpreter"},{"from":1608.76,"to":1612.72,"location":2,"content":"that's not hard at all but the"},{"from":1612.72,"to":1614.94,"location":2,"content":"difference here is hey we want to do a"},{"from":1614.94,"to":1617.25,"location":2,"content":"learning algorithm so we're going to do"},{"from":1617.25,"to":1619.56,"location":2,"content":"the opposite of that as well"},{"from":1619.56,"to":1622.35,"location":2,"content":"what we want to be able to do is also"},{"from":1622.35,"to":1625.02,"location":2,"content":"backward propagation or back propagation"},{"from":1625.02,"to":1627.06,"location":2,"content":"or just back prop it's commonly called"},{"from":1627.06,"to":1630.78,"location":2,"content":"which is we want to be able to go from"},{"from":1630.78,"to":1634.2,"location":2,"content":"the final part the final part here and"},{"from":1634.2,"to":1637.08,"location":2,"content":"then at each step we want to be"},{"from":1637.08,"to":1639.9,"location":2,"content":"calculating these partial derivatives"},{"from":1639.9,"to":1642.15,"location":2,"content":"and passing them back through the graph"},{"from":1642.15,"to":1644.61,"location":2,"content":"and so this was sort of the notion"},{"from":1644.61,"to":1647.04,"location":2,"content":"before that we had an error signal right"},{"from":1647.04,"to":1649.05,"location":2,"content":"so starting from up here we've"},{"from":1649.05,"to":1652.35,"location":2,"content":"calculated a partial of s by Z which is"},{"from":1652.35,"to":1655.47,"location":2,"content":"this with respect to that and so that's"},{"from":1655.47,"to":1657.87,"location":2,"content":"sort of our calculated error signal up"},{"from":1657.87,"to":1659.79,"location":2,"content":"to here and then we want to pass that"},{"from":1659.79,"to":1663.83,"location":2,"content":"further back to start computing our"},{"from":1663.83,"to":1666.72,"location":2,"content":"gradients further back right and we"},{"from":1666.72,"to":1670.29,"location":2,"content":"started off right here with the partial"},{"from":1670.29,"to":1673.05,"location":2,"content":"of s by s what's the partial of s but is"},{"from":1673.05,"to":1677.52,"location":2,"content":"going to be one okay yes so the rate at"},{"from":1677.52,"to":1679.32,"location":2,"content":"which s changes the rate at which s"},{"from":1679.32,"to":1681.69,"location":2,"content":"changes so we just start off with one"},{"from":1681.69,"to":1684.15,"location":2,"content":"and then we want to work out how this"},{"from":1684.15,"to":1689.91,"location":2,"content":"gradient changes as we go along so what"},{"from":1689.91,"to":1692.46,"location":2,"content":"we're doing here is when we're working"},{"from":1692.46,"to":1695.73,"location":2,"content":"out things for one node that a node is"},{"from":1695.73,"to":1697.44,"location":2,"content":"going to have passed into it it's"},{"from":1697.44,"to":1699.54,"location":2,"content":"upstream gradient which is its error"},{"from":1699.54,"to":1702.9,"location":2,"content":"signal so that's the partial of our"},{"from":1702.9,"to":1708.21,"location":2,"content":"final final result which was our loss by"},{"from":1708.21,"to":1711.54,"location":2,"content":"the variable that was the output of this"},{"from":1711.54,"to":1713.58,"location":2,"content":"computation node so that's the partial"},{"from":1713.58,"to":1717.75,"location":2,"content":"of s by H here and then we did some"},{"from":1717.75,"to":1720.36,"location":2,"content":"operation here here's the non-linearity"},{"from":1720.36,"to":1723.18,"location":2,"content":"but it might be something else and so"},{"from":1723.18,"to":1725.1,"location":2,"content":"what we wanted then work out is a"},{"from":1725.1,"to":1727.98,"location":2,"content":"downstream gradient which is the partial"},{"from":1727.98,"to":1730.8,"location":2,"content":"of s by Z which was the input of this"},{"from":1730.8,"to":1733.08,"location":2,"content":"function and well then the question is"},{"from":1733.08,"to":1736.74,"location":2,"content":"how do we do that and the answer that is"},{"from":1736.74,"to":1739.26,"location":2,"content":"we use the chain rule of course right so"},{"from":1739.26,"to":1740.12,"location":2,"content":"at"},{"from":1740.12,"to":1742.94,"location":2,"content":"we have a concept of a local gradient so"},{"from":1742.94,"to":1748.67,"location":2,"content":"here's H is the output z is the input so"},{"from":1748.67,"to":1750.62,"location":2,"content":"this function here and this is our"},{"from":1750.62,"to":1752.6,"location":2,"content":"non-linearity right so this is whatever"},{"from":1752.6,"to":1754.97,"location":2,"content":"we're using as our non-linearity like a"},{"from":1754.97,"to":1757.13,"location":2,"content":"logistic or at an age we're calculating"},{"from":1757.13,"to":1760.22,"location":2,"content":"H in terms of Z and we can work out the"},{"from":1760.22,"to":1762.35,"location":2,"content":"partial of H by Z so that's our local"},{"from":1762.35,"to":1765.26,"location":2,"content":"gradient and so then if we have both the"},{"from":1765.26,"to":1768.05,"location":2,"content":"upstream gradient and the local gradient"},{"from":1768.05,"to":1770.81,"location":2,"content":"we can then work out the downstream"},{"from":1770.81,"to":1773.48,"location":2,"content":"gradient because we know the partial of"},{"from":1773.48,"to":1778.4,"location":2,"content":"s by Z is going to be DSD a times D H DZ"},{"from":1778.4,"to":1781.07,"location":2,"content":"and so then we'll be able to pass down"},{"from":1781.07,"to":1784.78,"location":2,"content":"the downstream gradient to the next node"},{"from":1784.78,"to":1788.78,"location":2,"content":"ok so our basic rule which is just the"},{"from":1788.78,"to":1792.25,"location":2,"content":"chain rule written in different terms is"},{"from":1792.25,"to":1795.38,"location":2,"content":"downstream gradient equals upstream"},{"from":1795.38,"to":1799.01,"location":2,"content":"gradient times local gradient easy as"},{"from":1799.01,"to":1804.95,"location":2,"content":"that ok so this was the very simplest"},{"from":1804.95,"to":1807.65,"location":2,"content":"case where we have a node with one input"},{"from":1807.65,"to":1811.34,"location":2,"content":"and one output so that's a function like"},{"from":1811.34,"to":1813.83,"location":2,"content":"our logistic function but we also want"},{"from":1813.83,"to":1815.48,"location":2,"content":"to have things work out for a general"},{"from":1815.48,"to":1817.76,"location":2,"content":"computation graph so how are we going to"},{"from":1817.76,"to":1821.84,"location":2,"content":"do that well the next case is what about"},{"from":1821.84,"to":1824.66,"location":2,"content":"if we have multiple inputs so if we're"},{"from":1824.66,"to":1828.62,"location":2,"content":"calculating something like Z equals W"},{"from":1828.62,"to":1832.94,"location":2,"content":"times X we're actually yes Z and X of"},{"from":1832.94,"to":1838.7,"location":2,"content":"themselves vectors and W is a matrix but"},{"from":1838.7,"to":1841.55,"location":2,"content":"we're treating X as only important W as"},{"from":1841.55,"to":1844.82,"location":2,"content":"an input and Z is our output right we"},{"from":1844.82,"to":1846.02,"location":2,"content":"kind of group vectors and matrices"},{"from":1846.02,"to":1849.89,"location":2,"content":"together well if you have multiple"},{"from":1849.89,"to":1852.95,"location":2,"content":"inputs you then end up with multiple"},{"from":1852.95,"to":1855.77,"location":2,"content":"local gradients so you can work out the"},{"from":1855.77,"to":1858.26,"location":2,"content":"partial of Z with respect to X or the"},{"from":1858.26,"to":1861.26,"location":2,"content":"partial of Z u with respect to W and so"},{"from":1861.26,"to":1864.29,"location":2,"content":"you essentially you take the upstream"},{"from":1864.29,"to":1867.8,"location":2,"content":"gradient you multiply it by each of the"},{"from":1867.8,"to":1870.8,"location":2,"content":"local gradients and you pass it down the"},{"from":1870.8,"to":1873.47,"location":2,"content":"respective path and we calculate these"},{"from":1873.47,"to":1874.04,"location":2,"content":"differ"},{"from":1874.04,"to":1877.33,"location":2,"content":"and downstream gradients to pass along"},{"from":1877.33,"to":1881.26,"location":2,"content":"that making sense"},{"from":1881.26,"to":1885.8,"location":2,"content":"yeah okay I'll chug"},{"from":1885.8,"to":1889.43,"location":2,"content":"okay so let's sort of look at an example"},{"from":1889.43,"to":1891.44,"location":2,"content":"of this and then we'll see one another"},{"from":1891.44,"to":1894.02,"location":2,"content":"case so here's a little baby example"},{"from":1894.02,"to":1896.18,"location":2,"content":"this isn't kind of really looking like a"},{"from":1896.18,"to":1899.78,"location":2,"content":"neural net but we've got three imports x"},{"from":1899.78,"to":1902.81,"location":2,"content":"y and z and x and y get added together"},{"from":1902.81,"to":1906.68,"location":2,"content":"and y and z get maxed and then we take"},{"from":1906.68,"to":1908.96,"location":2,"content":"the results of those two operations and"},{"from":1908.96,"to":1911.6,"location":2,"content":"we multiply them together so overall"},{"from":1911.6,"to":1914.63,"location":2,"content":"what we're calculating is X plus y times"},{"from":1914.63,"to":1918.56,"location":2,"content":"the max of y plus C but you know we have"},{"from":1918.56,"to":1921.89,"location":2,"content":"here a general technique and we can"},{"from":1921.89,"to":1926.81,"location":2,"content":"apply it in any cases okay so if we want"},{"from":1926.81,"to":1928.76,"location":2,"content":"to have this graph and we want to run it"},{"from":1928.76,"to":1931.85,"location":2,"content":"forward well we need to know the values"},{"from":1931.85,"to":1935.42,"location":2,"content":"of x y and z so for my example x equals"},{"from":1935.42,"to":1941.75,"location":2,"content":"1 y equals to z equals 0 so we take the"},{"from":1941.75,"to":1944.18,"location":2,"content":"values of those variables and push them"},{"from":1944.18,"to":1947.24,"location":2,"content":"on to the calculations for the forward"},{"from":1947.24,"to":1950.21,"location":2,"content":"arrows and then well the first thing we"},{"from":1950.21,"to":1952.01,"location":2,"content":"do is add and the result of that is"},{"from":1952.01,"to":1953.93,"location":2,"content":"three and so we can put that onto the"},{"from":1953.93,"to":1956.45,"location":2,"content":"arrow that's the output of ad max it's 2"},{"from":1956.45,"to":1959.27,"location":2,"content":"is the output of the value of air x is 6"},{"from":1959.27,"to":1961.46,"location":2,"content":"and so the forward pass we have"},{"from":1961.46,"to":1964.52,"location":2,"content":"evaluated the expression it's value is 6"},{"from":1964.52,"to":1967.4,"location":2,"content":"that wasn't hard ok so then the next"},{"from":1967.4,"to":1970.73,"location":2,"content":"step is we then want to run that"},{"from":1970.73,"to":1975.71,"location":2,"content":"propagation to work out gradients and so"},{"from":1975.71,"to":1979.93,"location":2,"content":"we sort of want to know how to sort of"},{"from":1979.93,"to":1987.11,"location":2,"content":"work out these local gradients so a is"},{"from":1987.11,"to":1990.68,"location":2,"content":"out right a is the result of some so"},{"from":1990.68,"to":1992.86,"location":2,"content":"here's a as the result of some so a"},{"from":1992.86,"to":1996.47,"location":2,"content":"equals x plus y so if you're taking da"},{"from":1996.47,"to":2001.66,"location":2,"content":"DX that's just 1 and da dy is also 1"},{"from":2001.66,"to":2003.33,"location":2,"content":"that makes sense"},{"from":2003.33,"to":2007.33,"location":2,"content":"the max is slightly trickier because"},{"from":2007.33,"to":2010.96,"location":2,"content":"where there's some slope some gradient"},{"from":2010.96,"to":2012.85,"location":2,"content":"for the max depends on which one's"},{"from":2012.85,"to":2017.52,"location":2,"content":"bigger so if y is bigger than Z D Delta"},{"from":2017.52,"to":2022.06,"location":2,"content":"the partial of B by Z partial B by Y is"},{"from":2022.06,"to":2027.31,"location":2,"content":"1 otherwise at 0 and conversely for the"},{"from":2027.31,"to":2030.46,"location":2,"content":"partial of B by Z so now that one's a"},{"from":2030.46,"to":2034.21,"location":2,"content":"little bit dependent and then we do the"},{"from":2034.21,"to":2039.49,"location":2,"content":"multiplication case at the end and work"},{"from":2039.49,"to":2043.66,"location":2,"content":"out its partials with respect to a and B"},{"from":2043.66,"to":2047.71,"location":2,"content":"and since that's a B which has the"},{"from":2047.71,"to":2050.44,"location":2,"content":"values 2 and 3 if you're taking the"},{"from":2050.44,"to":2052.69,"location":2,"content":"partial of F by a it equals B which is 2"},{"from":2052.69,"to":2056.11,"location":2,"content":"and vice versa okay so that means we can"},{"from":2056.11,"to":2058.63,"location":2,"content":"work out the local gradients at each"},{"from":2058.63,"to":2062.65,"location":2,"content":"node and so then we want to use those to"},{"from":2062.65,"to":2064.96,"location":2,"content":"calculate our gradients backwards in the"},{"from":2064.96,"to":2067.42,"location":2,"content":"back propagation path so we start at the"},{"from":2067.42,"to":2069.85,"location":2,"content":"top the partial of F with respect to F"},{"from":2069.85,"to":2074.05,"location":2,"content":"is 1 because if you move if you know by"},{"from":2074.05,"to":2077.5,"location":2,"content":"1/10 then you've moved the F by 1/10 so"},{"from":2077.5,"to":2080.26,"location":2,"content":"that's a Counsel's out as 1 okay so then"},{"from":2080.26,"to":2083.77,"location":2,"content":"we want to pass backwards so the first"},{"from":2083.77,"to":2085.5,"location":2,"content":"thing that we have is this sort of"},{"from":2085.5,"to":2088.33,"location":2,"content":"multiply node and so we work we know"},{"from":2088.33,"to":2090.64,"location":2,"content":"it's local gradients the partial of F by"},{"from":2090.64,"to":2096.45,"location":2,"content":"a is 2 and the partial of F by B is 3"},{"from":2096.45,"to":2100.06,"location":2,"content":"and so we get those values so formally"},{"from":2100.06,"to":2103.15,"location":2,"content":"we're taking the local gradients"},{"from":2103.15,"to":2104.58,"location":2,"content":"multiplying them by the upstream"},{"from":2104.58,"to":2107.62,"location":2,"content":"gradients and getting our 3 and 2 and"},{"from":2107.62,"to":2109.81,"location":2,"content":"notice the fact that sort of effectively"},{"from":2109.81,"to":2111.4,"location":2,"content":"what happened is the values on the two"},{"from":2111.4,"to":2114.79,"location":2,"content":"arcs swaps and then we sort of continue"},{"from":2114.79,"to":2117.88,"location":2,"content":"back okay there's a max node so our"},{"from":2117.88,"to":2120.61,"location":2,"content":"upstream gradient is now 3 and then we"},{"from":2120.61,"to":2123.25,"location":2,"content":"want to multiply by the local gradient"},{"from":2123.25,"to":2126.7,"location":2,"content":"and since the max of these two is 2"},{"from":2126.7,"to":2130.24,"location":2,"content":"there's a slope of 1 on this side so we"},{"from":2130.24,"to":2132.91,"location":2,"content":"get 3 there's no gradient on this side"},{"from":2132.91,"to":2136.05,"location":2,"content":"and we get 0 and then we do the similar"},{"from":2136.05,"to":2139.33,"location":2,"content":"calculation on the other side where we"},{"from":2139.33,"to":2141.04,"location":2,"content":"have local gradients of 1"},{"from":2141.04,"to":2144.22,"location":2,"content":"and so both of them come out of two and"},{"from":2144.22,"to":2146.89,"location":2,"content":"then the one other thing to do is we"},{"from":2146.89,"to":2148.87,"location":2,"content":"notice well wait a minute"},{"from":2148.87,"to":2151.39,"location":2,"content":"there are two arcs that started from the"},{"from":2151.39,"to":2154.66,"location":2,"content":"Y both of which we've back propagated"},{"from":2154.66,"to":2157.57,"location":2,"content":"some gradient on and so what do we do"},{"from":2157.57,"to":2161.08,"location":2,"content":"about that what we do about that is we"},{"from":2161.08,"to":2165.1,"location":2,"content":"sum so the partial of F by X is 2 the"},{"from":2165.1,"to":2167.98,"location":2,"content":"partial of F by Z is 0 but the partial"},{"from":2167.98,"to":2172.32,"location":2,"content":"of F by Y is the sum of the two and five"},{"from":2172.32,"to":2175.24,"location":2,"content":"right and so this isn't complete voodoo"},{"from":2175.24,"to":2178.38,"location":2,"content":"this is something that should make sense"},{"from":2178.38,"to":2182.02,"location":2,"content":"in terms of what gradients are right so"},{"from":2182.02,"to":2184.48,"location":2,"content":"that what we're saying is what we're"},{"from":2184.48,"to":2187.24,"location":2,"content":"calculating is if you wiggle X a little"},{"from":2187.24,"to":2190.15,"location":2,"content":"bit how big an effect does that have on"},{"from":2190.15,"to":2192.64,"location":2,"content":"the outcome of the whole thing and so"},{"from":2192.64,"to":2194.17,"location":2,"content":"you know we should be able to work this"},{"from":2194.17,"to":2197.59,"location":2,"content":"out so our X started off as one but"},{"from":2197.59,"to":2200.14,"location":2,"content":"let's suppose we wiggle it up a bit to"},{"from":2200.14,"to":2203.35,"location":2,"content":"make it one point one well according to"},{"from":2203.35,"to":2206.41,"location":2,"content":"this our output should change by about"},{"from":2206.41,"to":2208.81,"location":2,"content":"zero point two it should be magnified by"},{"from":2208.81,"to":2210.76,"location":2,"content":"two and we should be able to work that"},{"from":2210.76,"to":2213.66,"location":2,"content":"out right so it's then one point one"},{"from":2213.66,"to":2217.69,"location":2,"content":"plus two so that's then three point one"},{"from":2217.69,"to":2220.66,"location":2,"content":"and then we've got the two here that"},{"from":2220.66,"to":2223.33,"location":2,"content":"multiplies by it and it's six point two"},{"from":2223.33,"to":2225.4,"location":2,"content":"and lo and behold it went up by point to"},{"from":2225.4,"to":2227.83,"location":2,"content":"right so that seems correct and if we"},{"from":2227.83,"to":2230.68,"location":2,"content":"try and do the same for well let's do"},{"from":2230.68,"to":2232.96,"location":2,"content":"the Z it's easy so if we wiggle the Z"},{"from":2232.96,"to":2235.96,"location":2,"content":"which had a value of the zero by 0.1"},{"from":2235.96,"to":2239.56,"location":2,"content":"this is zero point one when we max it if"},{"from":2239.56,"to":2242.2,"location":2,"content":"this is still two and so a calculated"},{"from":2242.2,"to":2245.11,"location":2,"content":"value doesn't change it's still six so"},{"from":2245.11,"to":2247.36,"location":2,"content":"the gradient here is zero we doing this"},{"from":2247.36,"to":2251.49,"location":2,"content":"does nothing and then the final one is y"},{"from":2251.49,"to":2255.7,"location":2,"content":"so it's starting off value as two so if"},{"from":2255.7,"to":2257.35,"location":2,"content":"we wiggle it a little and make a two"},{"from":2257.35,"to":2261.52,"location":2,"content":"point one our claim is that the results"},{"from":2261.52,"to":2264.61,"location":2,"content":"sort of change by about 0.5 it should be"},{"from":2264.61,"to":2267.34,"location":2,"content":"multiplied by five times so if we make"},{"from":2267.34,"to":2270.49,"location":2,"content":"this two point one we then have two"},{"from":2270.49,"to":2273.49,"location":2,"content":"point one plus one would be three point"},{"from":2273.49,"to":2274.32,"location":2,"content":"one"},{"from":2274.32,"to":2278.67,"location":2,"content":"when we get the max here it also be 2.1"},{"from":2278.67,"to":2281.91,"location":2,"content":"and so it have to point one times three"},{"from":2281.91,"to":2284.46,"location":2,"content":"point one and that's to harder rithmetic"},{"from":2284.46,"to":2288.86,"location":2,"content":"for me to do in my head that if we take"},{"from":2288.86,"to":2294,"location":2,"content":"two point one times three point one it"},{"from":2294,"to":2296.79,"location":2,"content":"comes out of six point five one so"},{"from":2296.79,"to":2299.34,"location":2,"content":"basically it's gone up by half right if"},{"from":2299.34,"to":2301.53,"location":2,"content":"we don't expect the answers to be exact"},{"from":2301.53,"to":2303.24,"location":2,"content":"of course right because you know that's"},{"from":2303.24,"to":2305.07,"location":2,"content":"not the way calculus works right well"},{"from":2305.07,"to":2307.74,"location":2,"content":"I'm just that that it's showing that"},{"from":2307.74,"to":2310.2,"location":2,"content":"we're getting the gradients right okay"},{"from":2310.2,"to":2313.65,"location":2,"content":"so this actually works so what are the"},{"from":2313.65,"to":2317.76,"location":2,"content":"techniques that we need to know so we've"},{"from":2317.76,"to":2319.71,"location":2,"content":"sort of already seen them all so you"},{"from":2319.71,"to":2322.38,"location":2,"content":"know we discussed when there are"},{"from":2322.38,"to":2324.75,"location":2,"content":"multiple incoming arcs how he saw work"},{"from":2324.75,"to":2327.48,"location":2,"content":"out the different local derivatives"},{"from":2327.48,"to":2329.91,"location":2,"content":"their main other case that we need to"},{"from":2329.91,"to":2333.9,"location":2,"content":"know is if in the function computation"},{"from":2333.9,"to":2336.42,"location":2,"content":"there's a branch outward the result of"},{"from":2336.42,"to":2339.12,"location":2,"content":"something is used in multiple places and"},{"from":2339.12,"to":2341.37,"location":2,"content":"so this was like the case here I mean"},{"from":2341.37,"to":2343.65,"location":2,"content":"here this was an initial variable but"},{"from":2343.65,"to":2345.09,"location":2,"content":"you know it could have been computed by"},{"from":2345.09,"to":2346.95,"location":2,"content":"something further back so if this thing"},{"from":2346.95,"to":2350.37,"location":2,"content":"is used in multiple places and you have"},{"from":2350.37,"to":2353.1,"location":2,"content":"the computation going out in different"},{"from":2353.1,"to":2356.19,"location":2,"content":"ways it's just this simple rule that"},{"from":2356.19,"to":2358.17,"location":2,"content":"when you do back propagation backwards"},{"from":2358.17,"to":2361.11,"location":2,"content":"use some the gradients that you get from"},{"from":2361.11,"to":2364.11,"location":2,"content":"the different outward branches okay so"},{"from":2364.11,"to":2367.77,"location":2,"content":"if a equals x plus y and well that's one"},{"from":2367.77,"to":2369.48,"location":2,"content":"we showed you before that we're doing"},{"from":2369.48,"to":2372.15,"location":2,"content":"this sum operation to work out the total"},{"from":2372.15,"to":2377.1,"location":2,"content":"partial of F by Y okay and if you sort"},{"from":2377.1,"to":2380.16,"location":2,"content":"of think about it just a little bit more"},{"from":2380.16,"to":2383,"location":2,"content":"there are sort of these obvious patterns"},{"from":2383,"to":2386.55,"location":2,"content":"which we saw in this very simple example"},{"from":2386.55,"to":2391.8,"location":2,"content":"so if you've got a plus that really the"},{"from":2391.8,"to":2394.8,"location":2,"content":"upstream gradient is going to be sort of"},{"from":2394.8,"to":2397.44,"location":2,"content":"heading down every one of these grant"},{"from":2397.44,"to":2400.38,"location":2,"content":"branches when you have multiple branches"},{"from":2400.38,"to":2403.32,"location":2,"content":"or things being summed now in this case"},{"from":2403.32,"to":2406.74,"location":2,"content":"it just is copied unchanged but that's"},{"from":2406.74,"to":2408.09,"location":2,"content":"because our comp"},{"from":2408.09,"to":2410.73,"location":2,"content":"tation was x plus y you know it could be"},{"from":2410.73,"to":2412.65,"location":2,"content":"more complicated but we're passing it"},{"from":2412.65,"to":2415.41,"location":2,"content":"down down each of those branches so plus"},{"from":2415.41,"to":2420.03,"location":2,"content":"distributes upstream gradient when you"},{"from":2420.03,"to":2422.22,"location":2,"content":"have a max that's kind of like a"},{"from":2422.22,"to":2425.1,"location":2,"content":"ralphing operation because max is going"},{"from":2425.1,"to":2427.71,"location":2,"content":"to be sending the gradient to in the"},{"from":2427.71,"to":2429.87,"location":2,"content":"direction that's the max and other"},{"from":2429.87,"to":2431.52,"location":2,"content":"things are going to get no gradient"},{"from":2431.52,"to":2434.82,"location":2,"content":"being passed down to them and then when"},{"from":2434.82,"to":2438.51,"location":2,"content":"you have a multiplication this has this"},{"from":2438.51,"to":2440.94,"location":2,"content":"kind of fun effect that what you do is"},{"from":2440.94,"to":2442.98,"location":2,"content":"switch the gradient ride and so this"},{"from":2442.98,"to":2445.41,"location":2,"content":"reflects the fact that when you have u"},{"from":2445.41,"to":2449.37,"location":2,"content":"times V regardless of whether u and V"},{"from":2449.37,"to":2452.97,"location":2,"content":"are vectors or just scalars that the"},{"from":2452.97,"to":2455.1,"location":2,"content":"derivative of the result with respect to"},{"from":2455.1,"to":2457.41,"location":2,"content":"u is V and the derivative of a spot"},{"from":2457.41,"to":2460.82,"location":2,"content":"result with respect to V is U and so the"},{"from":2460.82,"to":2465.45,"location":2,"content":"gradient signal is the flip of the two"},{"from":2465.45,"to":2469.95,"location":2,"content":"numbers on the different sides okay so"},{"from":2469.95,"to":2474,"location":2,"content":"this is sort of most of how we have"},{"from":2474,"to":2477.27,"location":2,"content":"these computation graphs and we can work"},{"from":2477.27,"to":2479.43,"location":2,"content":"out back propagation backwards in them"},{"from":2479.43,"to":2482.67,"location":2,"content":"there's sort of one more part of this to"},{"from":2482.67,"to":2486.39,"location":2,"content":"do which is to say gee we want to do"},{"from":2486.39,"to":2489.81,"location":2,"content":"this efficiently so there's a bad way to"},{"from":2489.81,"to":2492,"location":2,"content":"do this which is to say oh well we"},{"from":2492,"to":2494.25,"location":2,"content":"wanted to calculate the partial vez by B"},{"from":2494.25,"to":2497.19,"location":2,"content":"and so we can calculate that part roll"},{"from":2497.19,"to":2499.17,"location":2,"content":"which was essentially what I was doing"},{"from":2499.17,"to":2504,"location":2,"content":"on last time slides we say partial of F"},{"from":2504,"to":2508.74,"location":2,"content":"by B equals the partial of s by H times"},{"from":2508.74,"to":2511.71,"location":2,"content":"the partial of H by Z times the partial"},{"from":2511.71,"to":2514.35,"location":2,"content":"of Z by B and we have all of those"},{"from":2514.35,"to":2516.15,"location":2,"content":"partials we work them all out and"},{"from":2516.15,"to":2518.88,"location":2,"content":"multiply them together and then someone"},{"from":2518.88,"to":2522.75,"location":2,"content":"says what's the partial of s by W and we"},{"from":2522.75,"to":2524.31,"location":2,"content":"say huh that's the chain rule again I'll"},{"from":2524.31,"to":2528.32,"location":2,"content":"do it all again it's the partial of F by"},{"from":2528.32,"to":2531.87,"location":2,"content":"H times the partial of H by Z times the"},{"from":2531.87,"to":2539.73,"location":2,"content":"partial of Z by X no mo right lost it"},{"from":2539.73,"to":2541.89,"location":2,"content":"but you do a big long list of them"},{"from":2541.89,"to":2543.6,"location":2,"content":"and you calculate it all again that's"},{"from":2543.6,"to":2546.12,"location":2,"content":"not what we want to do instead we want"},{"from":2546.12,"to":2548.52,"location":2,"content":"to say I look there's this shared stuff"},{"from":2548.52,"to":2550.92,"location":2,"content":"there's this error signal coming from"},{"from":2550.92,"to":2554.25,"location":2,"content":"above and we can work out the error"},{"from":2554.25,"to":2556.35,"location":2,"content":"signal the upstream gradient for this"},{"from":2556.35,"to":2558.93,"location":2,"content":"node we can use it to calculate the"},{"from":2558.93,"to":2561.21,"location":2,"content":"upstream gradient for this node we can"},{"from":2561.21,"to":2563.19,"location":2,"content":"use this to calculate the upstream"},{"from":2563.19,"to":2566.4,"location":2,"content":"gradient for this node and then using"},{"from":2566.4,"to":2568.44,"location":2,"content":"the local gradients of which they're too"},{"from":2568.44,"to":2570.75,"location":2,"content":"calculated this node we can then"},{"from":2570.75,"to":2575.19,"location":2,"content":"calculate this one and that one and then"},{"from":2575.19,"to":2579.09,"location":2,"content":"from here having knowing this upstream"},{"from":2579.09,"to":2581.31,"location":2,"content":"gradient we can use the local gradients"},{"from":2581.31,"to":2583.77,"location":2,"content":"at this node to compute this one and"},{"from":2583.77,"to":2586.92,"location":2,"content":"that one and so we're sort of doing this"},{"from":2586.92,"to":2589.05,"location":2,"content":"efficient computer science like"},{"from":2589.05,"to":2591.84,"location":2,"content":"computation where we don't do any"},{"from":2591.84,"to":2595.53,"location":2,"content":"repeated work that makes sense yeah okay"},{"from":2595.53,"to":2600.75,"location":2,"content":"and so if that is the whole of backprop"},{"from":2600.75,"to":2604.91,"location":2,"content":"so here's sort of a slightly sketchy"},{"from":2604.91,"to":2607.31,"location":2,"content":"graph which is sort of just"},{"from":2607.31,"to":2610.59,"location":2,"content":"recapitulating this thing so if you have"},{"from":2610.59,"to":2615.35,"location":2,"content":"any computation that you want to perform"},{"from":2615.35,"to":2620.94,"location":2,"content":"well the hope is that you can sort your"},{"from":2620.94,"to":2623.7,"location":2,"content":"nodes into what's called a topological"},{"from":2623.7,"to":2627.12,"location":2,"content":"sort which means the things that are"},{"from":2627.12,"to":2629.67,"location":2,"content":"arguments variables that are arguments"},{"from":2629.67,"to":2632.25,"location":2,"content":"are sorted before variables that are"},{"from":2632.25,"to":2634.98,"location":2,"content":"results that depend on that argument you"},{"from":2634.98,"to":2636.45,"location":2,"content":"know providing you have something"},{"from":2636.45,"to":2638.37,"location":2,"content":"there's an acyclic graph you'll be able"},{"from":2638.37,"to":2640.98,"location":2,"content":"to do that if you have a psychic graph"},{"from":2640.98,"to":2642.26,"location":2,"content":"you're in trouble"},{"from":2642.26,"to":2644.73,"location":2,"content":"well I mean they're actually techniques"},{"from":2644.73,"to":2646.5,"location":2,"content":"people use to roll out those graphs but"},{"from":2646.5,"to":2647.82,"location":2,"content":"I'm not going to go into that now"},{"from":2647.82,"to":2650.19,"location":2,"content":"so we've sorted the nodes which is kind"},{"from":2650.19,"to":2652.5,"location":2,"content":"of loosely represented here from bottom"},{"from":2652.5,"to":2652.98,"location":2,"content":"to top"},{"from":2652.98,"to":2657.09,"location":2,"content":"in a topological sort area sort ok so"},{"from":2657.09,"to":2659.85,"location":2,"content":"then for the forward prop we sort of go"},{"from":2659.85,"to":2661.74,"location":2,"content":"through the nodes and they're"},{"from":2661.74,"to":2666.36,"location":2,"content":"topological sort order and we if it's a"},{"from":2666.36,"to":2668.34,"location":2,"content":"variable we just said its value to what"},{"from":2668.34,"to":2670.74,"location":2,"content":"it's very about variable value is if"},{"from":2670.74,"to":2673.05,"location":2,"content":"it's computed from other variables their"},{"from":2673.05,"to":2675.34,"location":2,"content":"values must have been set already"},{"from":2675.34,"to":2676.99,"location":2,"content":"because there earlier in the topological"},{"from":2676.99,"to":2680.26,"location":2,"content":"sort and then we compute the value of"},{"from":2680.26,"to":2682.54,"location":2,"content":"those nodes according to their"},{"from":2682.54,"to":2685.57,"location":2,"content":"predecessors and we pass it up and work"},{"from":2685.57,"to":2687.94,"location":2,"content":"out the final output the loss function"},{"from":2687.94,"to":2690.31,"location":2,"content":"of our neural network and that is our"},{"from":2690.31,"to":2693.34,"location":2,"content":"forward pass okay so then after that we"},{"from":2693.34,"to":2695.11,"location":2,"content":"do our backward pass and so for the"},{"from":2695.11,"to":2698.89,"location":2,"content":"backward pass we initialize the output"},{"from":2698.89,"to":2700.93,"location":2,"content":"gradient with one the top thing is"},{"from":2700.93,"to":2703.06,"location":2,"content":"always one the partial of Z with respect"},{"from":2703.06,"to":2706.69,"location":2,"content":"to Z and then we now sort of go through"},{"from":2706.69,"to":2709.48,"location":2,"content":"the nodes in Reverse topological sort"},{"from":2709.48,"to":2712.6,"location":2,"content":"and so therefore each of them will all"},{"from":2712.6,"to":2716.16,"location":2,"content":"read anything that's anything that's"},{"from":2716.16,"to":2718.39,"location":2,"content":"playing with its complex and I think"},{"from":2718.39,"to":2720.19,"location":2,"content":"it's above it everything that we"},{"from":2720.19,"to":2722.25,"location":2,"content":"calculated based on it in terms of"},{"from":2722.25,"to":2725.16,"location":2,"content":"forward pass will already have had"},{"from":2725.16,"to":2730.51,"location":2,"content":"calculated its its gradient as a product"},{"from":2730.51,"to":2732.55,"location":2,"content":"of upstream gradient times local"},{"from":2732.55,"to":2735.75,"location":2,"content":"gradient and then we can use that to"},{"from":2735.75,"to":2739.72,"location":2,"content":"compute the next thing down and so"},{"from":2739.72,"to":2743.23,"location":2,"content":"basically that what the overall role is"},{"from":2743.23,"to":2746.71,"location":2,"content":"for any node you work out its set of"},{"from":2746.71,"to":2749.02,"location":2,"content":"successes the things that are above it"},{"from":2749.02,"to":2751.66,"location":2,"content":"that it that depend on it and then you"},{"from":2751.66,"to":2754.24,"location":2,"content":"say okay the partial of Z with respect"},{"from":2754.24,"to":2759.04,"location":2,"content":"to X is simply the sum over the set of"},{"from":2759.04,"to":2763,"location":2,"content":"successes of the local gradient that you"},{"from":2763,"to":2766.27,"location":2,"content":"calculate at the node times the upstream"},{"from":2766.27,"to":2769.21,"location":2,"content":"gradient of that node and in the"},{"from":2769.21,"to":2771.67,"location":2,"content":"examples that I gave before there was"},{"from":2771.67,"to":2774.88,"location":2,"content":"never never multiple upstream gradients"},{"from":2774.88,"to":2777.16,"location":2,"content":"but if you imagine a a general big graph"},{"from":2777.16,"to":2779.11,"location":2,"content":"they could actually be sort of different"},{"from":2779.11,"to":2780.85,"location":2,"content":"upstream gradients that are being used"},{"from":2780.85,"to":2785.68,"location":2,"content":"in for the various successes so we apply"},{"from":2785.68,"to":2788.65,"location":2,"content":"that backwards and then we've worked out"},{"from":2788.65,"to":2792.46,"location":2,"content":"in back propagation the gradient of"},{"from":2792.46,"to":2796.18,"location":2,"content":"every the gradient of the final results"},{"from":2796.18,"to":2798.16,"location":2,"content":"Z with respect to every node in our"},{"from":2798.16,"to":2801.67,"location":2,"content":"graph and the thing to notice about this"},{"from":2801.67,"to":2804.37,"location":2,"content":"is if you're doing it right and"},{"from":2804.37,"to":2806.92,"location":2,"content":"efficiently the big o order of"},{"from":2806.92,"to":2809.37,"location":2,"content":"complexity of doing that proper"},{"from":2809.37,"to":2812.55,"location":2,"content":"is exactly the same as doing forward"},{"from":2812.55,"to":2815.58,"location":2,"content":"propagation eye expression evaluation so"},{"from":2815.58,"to":2818.88,"location":2,"content":"it's not some super expensive complex"},{"from":2818.88,"to":2820.35,"location":2,"content":"procedure that you couldn't imagine"},{"from":2820.35,"to":2824.12,"location":2,"content":"doing and scaling up you're actually in"},{"from":2824.12,"to":2827.97,"location":2,"content":"exactly the same complexity order okay"},{"from":2827.97,"to":2831.39,"location":2,"content":"so as I presented here this procedure"},{"from":2831.39,"to":2834.33,"location":2,"content":"you could just think of something that"},{"from":2834.33,"to":2838.4,"location":2,"content":"you're running on an arbitrary graph and"},{"from":2838.4,"to":2840.66,"location":2,"content":"calculating this forward pass and the"},{"from":2840.66,"to":2843.24,"location":2,"content":"backwards pass I mean almost without"},{"from":2843.24,"to":2845.52,"location":2,"content":"exception that the kind of neural nets"},{"from":2845.52,"to":2848.19,"location":2,"content":"that we actually use have a regular"},{"from":2848.19,"to":2850.47,"location":2,"content":"layer like structure and that's then"},{"from":2850.47,"to":2853.26,"location":2,"content":"precisely why it makes sense to work out"},{"from":2853.26,"to":2856.65,"location":2,"content":"these gradients in terms of vectors"},{"from":2856.65,"to":2859.02,"location":2,"content":"matrices and jacobians as the kind we"},{"from":2859.02,"to":2862.11,"location":2,"content":"were before okay"},{"from":2862.11,"to":2864.9,"location":2,"content":"so since we have this sort of really"},{"from":2864.9,"to":2867.99,"location":2,"content":"nice algorithm now this sort of means"},{"from":2867.99,"to":2871.23,"location":2,"content":"that we can do this just computationally"},{"from":2871.23,"to":2873.75,"location":2,"content":"and so we don't have to think or know"},{"from":2873.75,"to":2876.66,"location":2,"content":"how to do math and we can just have our"},{"from":2876.66,"to":2880.56,"location":2,"content":"computers do all of us with this so that"},{"from":2880.56,"to":2884.12,"location":2,"content":"using this graph structure we can just"},{"from":2884.12,"to":2888.11,"location":2,"content":"automatically work out how to apply"},{"from":2888.11,"to":2890.79,"location":2,"content":"backprop and there's sort of two cases"},{"from":2890.79,"to":2894.72,"location":2,"content":"of this right so if what was calculated"},{"from":2894.72,"to":2898.62,"location":2,"content":"at each node is given as a symbolic"},{"from":2898.62,"to":2901.68,"location":2,"content":"expression we could actually have our"},{"from":2901.68,"to":2905.45,"location":2,"content":"computer work out for us what the"},{"from":2905.45,"to":2908.1,"location":2,"content":"derivative of that symbolic expression"},{"from":2908.1,"to":2910.83,"location":2,"content":"is so it could actually calculate the"},{"from":2910.83,"to":2912.78,"location":2,"content":"gradient of that node and that's"},{"from":2912.78,"to":2915.33,"location":2,"content":"referred to as often as automatic"},{"from":2915.33,"to":2917.84,"location":2,"content":"differentiation so this is kind of like"},{"from":2917.84,"to":2920.07,"location":2,"content":"Mathematica Wolfram Alpha if you know"},{"from":2920.07,"to":2921.99,"location":2,"content":"how you do your math homework on it you"},{"from":2921.99,"to":2923.55,"location":2,"content":"just type in your expression say what's"},{"from":2923.55,"to":2925.35,"location":2,"content":"the derivative and it gives it back to"},{"from":2925.35,"to":2928.23,"location":2,"content":"you right it's working doing symbolic"},{"from":2928.23,"to":2930.24,"location":2,"content":"computation and working out the"},{"from":2930.24,"to":2933.54,"location":2,"content":"derivative for you so that so that"},{"from":2933.54,"to":2935.22,"location":2,"content":"method could be used to work out the"},{"from":2935.22,"to":2938.04,"location":2,"content":"local gradients and then we can use the"},{"from":2938.04,"to":2941.55,"location":2,"content":"graph structure and our rule upstream"},{"from":2941.55,"to":2942.57,"location":2,"content":"gradient times"},{"from":2942.57,"to":2944.61,"location":2,"content":"local gradient gives downstream gradient"},{"from":2944.61,"to":2947.76,"location":2,"content":"ie the chain rule to then propagate it"},{"from":2947.76,"to":2949.41,"location":2,"content":"through the graph and do the whole"},{"from":2949.41,"to":2952.79,"location":2,"content":"backward pass completely automatically"},{"from":2952.79,"to":2957.72,"location":2,"content":"and so that sounds great"},{"from":2957.72,"to":2961.23,"location":2,"content":"slight disappointment current deep"},{"from":2961.23,"to":2963.12,"location":2,"content":"learning frameworks don't quite give you"},{"from":2963.12,"to":2965.01,"location":2,"content":"that there was actually a famous"},{"from":2965.01,"to":2966.75,"location":2,"content":"framework that attempted to give you"},{"from":2966.75,"to":2969.18,"location":2,"content":"that so the theano framework that was"},{"from":2969.18,"to":2971.27,"location":2,"content":"developed at the University of Montreal"},{"from":2971.27,"to":2973.68,"location":2,"content":"those they've now abandoned in the"},{"from":2973.68,"to":2976.08,"location":2,"content":"modern era of large technology"},{"from":2976.08,"to":2977.79,"location":2,"content":"corporation deep learning frameworks"},{"from":2977.79,"to":2980.7,"location":2,"content":"Tiano did precisely that it did the full"},{"from":2980.7,"to":2984.03,"location":2,"content":"thing of automatic differentiation for"},{"from":2984.03,"to":2986.4,"location":2,"content":"reasons that we could either think of"},{"from":2986.4,"to":2988.41,"location":2,"content":"good or bad current deep learning"},{"from":2988.41,"to":2990.81,"location":2,"content":"frameworks like tensor flow or PI torch"},{"from":2990.81,"to":2993.03,"location":2,"content":"actually do a little bit less than that"},{"from":2993.03,"to":2996.24,"location":2,"content":"so what they do is say well for an"},{"from":2996.24,"to":2998.49,"location":2,"content":"individual for the computations at an"},{"from":2998.49,"to":3001.4,"location":2,"content":"individual node you have to do the"},{"from":3001.4,"to":3003.8,"location":2,"content":"calculus for yourself for this"},{"from":3003.8,"to":3006.17,"location":2,"content":"individual node you have to write the"},{"from":3006.17,"to":3009.26,"location":2,"content":"forward propagation say you know return"},{"from":3009.26,"to":3012.59,"location":2,"content":"x plus y and you have to write the"},{"from":3012.59,"to":3014.63,"location":2,"content":"backward propagation saying the local"},{"from":3014.63,"to":3019.48,"location":2,"content":"gradients 1 and 1/2 the 2 inputs x and y"},{"from":3019.48,"to":3023.27,"location":2,"content":"but providing you or someone else has"},{"from":3023.27,"to":3025.48,"location":2,"content":"written out the forward and backward"},{"from":3025.48,"to":3028.91,"location":2,"content":"local step at this node then tensorflow"},{"from":3028.91,"to":3031.43,"location":2,"content":"or pi torch does all the rest of it for"},{"from":3031.43,"to":3033.14,"location":2,"content":"you and runs the back propagation"},{"from":3033.14,"to":3036.92,"location":2,"content":"algorithm and then you know effectively"},{"from":3036.92,"to":3039.92,"location":2,"content":"that sort of saves you having to have a"},{"from":3039.92,"to":3042.97,"location":2,"content":"big symbolic computation engine because"},{"from":3042.97,"to":3046.93,"location":2,"content":"somewhat the person coding the node"},{"from":3046.93,"to":3049.94,"location":2,"content":"computations is writing a bit of code as"},{"from":3049.94,"to":3051.65,"location":2,"content":"you might normally imagine doing it"},{"from":3051.65,"to":3054.26,"location":2,"content":"whether in you know C or Pascal of"},{"from":3054.26,"to":3058.31,"location":2,"content":"saying return ik x plus y and you know"},{"from":3058.31,"to":3061.4,"location":2,"content":"local gradient return 1 right and and"},{"from":3061.4,"to":3062.96,"location":2,"content":"you don't actually have to have a whole"},{"from":3062.96,"to":3066.92,"location":2,"content":"symbolic computation engine okay so that"},{"from":3066.92,"to":3068.57,"location":2,"content":"means the overall picture looks like"},{"from":3068.57,"to":3072.47,"location":2,"content":"this right so schematically we have a"},{"from":3072.47,"to":3076.13,"location":2,"content":"computation graph and to calculate the"},{"from":3076.13,"to":3077.13,"location":2,"content":"for"},{"from":3077.13,"to":3082.57,"location":2,"content":"computation we sort of put inputs into"},{"from":3082.57,"to":3084.43,"location":2,"content":"our computation graphed where there's"},{"from":3084.43,"to":3087.37,"location":2,"content":"sort of X and y variables and then we"},{"from":3087.37,"to":3090.88,"location":2,"content":"run through the nodes and topologically"},{"from":3090.88,"to":3094.54,"location":2,"content":"sorted order and for each node we"},{"from":3094.54,"to":3097.69,"location":2,"content":"calculate its forward and necessarily"},{"from":3097.69,"to":3099.46,"location":2,"content":"the things that depends on then have"},{"from":3099.46,"to":3101.41,"location":2,"content":"already been computed and we just do"},{"from":3101.41,"to":3104.05,"location":2,"content":"expression evaluation forward and then"},{"from":3104.05,"to":3107.8,"location":2,"content":"we return the final gate in the graph"},{"from":3107.8,"to":3110.17,"location":2,"content":"which is our loss function or objective"},{"from":3110.17,"to":3112.84,"location":2,"content":"function but then also we have the"},{"from":3112.84,"to":3115.48,"location":2,"content":"backward pass and for the backward pass"},{"from":3115.48,"to":3117.51,"location":2,"content":"we go in the nodes in Reverse"},{"from":3117.51,"to":3120.88,"location":2,"content":"topological only sorted order and for"},{"from":3120.88,"to":3123.61,"location":2,"content":"each of those nodes we've returned their"},{"from":3123.61,"to":3126.67,"location":2,"content":"backward value and for the top node we"},{"from":3126.67,"to":3129.01,"location":2,"content":"return backward value of one and that"},{"from":3129.01,"to":3131.95,"location":2,"content":"will then give us our gradients and so"},{"from":3131.95,"to":3137.26,"location":2,"content":"that means for any node any piece of"},{"from":3137.26,"to":3139.75,"location":2,"content":"computation that we perform we need to"},{"from":3139.75,"to":3143.5,"location":2,"content":"write a little bit of code that says"},{"from":3143.5,"to":3145.3,"location":2,"content":"what it's doing on the forward pass and"},{"from":3145.3,"to":3147.82,"location":2,"content":"what it's doing on the backward pass so"},{"from":3147.82,"to":3151.27,"location":2,"content":"on the forward pass this is our"},{"from":3151.27,"to":3153.19,"location":2,"content":"multiplication so we're just saying"},{"from":3153.19,"to":3156.61,"location":2,"content":"return x times y so that's pretty easy"},{"from":3156.61,"to":3158.74,"location":2,"content":"that's what you're used to doing but"},{"from":3158.74,"to":3161.71,"location":2,"content":"well we also need to do the backward"},{"from":3161.71,"to":3164.62,"location":2,"content":"pass as local gradients of return what"},{"from":3164.62,"to":3168.34,"location":2,"content":"is the partial of L with respect to Z"},{"from":3168.34,"to":3171.28,"location":2,"content":"and with respect to X and well to do"},{"from":3171.28,"to":3173.29,"location":2,"content":"that we have to do a little bit more"},{"from":3173.29,"to":3175.81,"location":2,"content":"work so we have to do a little bit more"},{"from":3175.81,"to":3178.75,"location":2,"content":"work first of all on the forward pass so"},{"from":3178.75,"to":3181.45,"location":2,"content":"in the forward pass we have to remember"},{"from":3181.45,"to":3184.63,"location":2,"content":"to sort of stuff away in some variables"},{"from":3184.63,"to":3186.85,"location":2,"content":"what values we computed in the forth"},{"from":3186.85,"to":3189.4,"location":2,"content":"with what values were given it to us in"},{"from":3189.4,"to":3191.14,"location":2,"content":"the forward pass or else we won't be"},{"from":3191.14,"to":3193.63,"location":2,"content":"able to calculate the backward pass so"},{"from":3193.63,"to":3198.07,"location":2,"content":"we store away the values of x and y and"},{"from":3198.07,"to":3200.26,"location":2,"content":"so then when we're doing the backward"},{"from":3200.26,"to":3203.89,"location":2,"content":"pass we are passed into us the upstream"},{"from":3203.89,"to":3206.32,"location":2,"content":"gradient the error signal and now we"},{"from":3206.32,"to":3210.19,"location":2,"content":"just do calculate up"},{"from":3210.19,"to":3212.47,"location":2,"content":"extreme gradient times local gradient"},{"from":3212.47,"to":3214.78,"location":2,"content":"upstream gradient times local gradient"},{"from":3214.78,"to":3219.55,"location":2,"content":"and we return backwards those downstream"},{"from":3219.55,"to":3223.3,"location":2,"content":"gradients and so providing we do that"},{"from":3223.3,"to":3226.63,"location":2,"content":"for all the nodes of our graph we then"},{"from":3226.63,"to":3229.51,"location":2,"content":"have something that the system can run"},{"from":3229.51,"to":3232.69,"location":2,"content":"for us as a deep learning system and so"},{"from":3232.69,"to":3236.05,"location":2,"content":"what that means in practice is that you"},{"from":3236.05,"to":3237.64,"location":2,"content":"know any of these deep learning"},{"from":3237.64,"to":3239.74,"location":2,"content":"frameworks come with a whole box of"},{"from":3239.74,"to":3242.74,"location":2,"content":"tools it says here is a fully connected"},{"from":3242.74,"to":3245.47,"location":2,"content":"forward layer here is a sigmoid unit"},{"from":3245.47,"to":3247.63,"location":2,"content":"here is other more complicated things"},{"from":3247.63,"to":3249.58,"location":2,"content":"we'll do later like convolutions and"},{"from":3249.58,"to":3251.74,"location":2,"content":"recurrent layers and to the extent that"},{"from":3251.74,"to":3254.32,"location":2,"content":"you're using one of those somebody else"},{"from":3254.32,"to":3256.3,"location":2,"content":"has done this work for you right that"},{"from":3256.3,"to":3261.31,"location":2,"content":"they've defined nodes or a layer of"},{"from":3261.31,"to":3263.74,"location":2,"content":"nodes that have forward and backward"},{"from":3263.74,"to":3266.32,"location":2,"content":"already written foot for them and to the"},{"from":3266.32,"to":3269.8,"location":2,"content":"extent that that's true that means that"},{"from":3269.8,"to":3271.99,"location":2,"content":"making neural nets is heaps of farmers"},{"from":3271.99,"to":3273.52,"location":2,"content":"just like Lego right you just stick"},{"from":3273.52,"to":3275.98,"location":2,"content":"these layers together and say got it on"},{"from":3275.98,"to":3277.48,"location":2,"content":"some data and train it you know it's so"},{"from":3277.48,"to":3279.52,"location":2,"content":"easy that my high school student is"},{"from":3279.52,"to":3281.59,"location":2,"content":"building these things right you don't"},{"from":3281.59,"to":3284.26,"location":2,"content":"have to understand much really but you"},{"from":3284.26,"to":3285.73,"location":2,"content":"know to the extent that you actually"},{"from":3285.73,"to":3287.41,"location":2,"content":"want to do some original research and"},{"from":3287.41,"to":3289.12,"location":2,"content":"think I've got this really cool idea of"},{"from":3289.12,"to":3291.04,"location":2,"content":"how to do things differently I'm going"},{"from":3291.04,"to":3292.72,"location":2,"content":"to define my own kind of different"},{"from":3292.72,"to":3295.09,"location":2,"content":"computation well then you have to do"},{"from":3295.09,"to":3298.18,"location":2,"content":"this and define your class and as well"},{"from":3298.18,"to":3299.71,"location":2,"content":"as sort of saying how to compute the"},{"from":3299.71,"to":3302.08,"location":2,"content":"forward value you have to pull out your"},{"from":3302.08,"to":3304.39,"location":2,"content":"copy of Wolfram Alpha and work out what"},{"from":3304.39,"to":3306.64,"location":2,"content":"the derivatives are and put that into"},{"from":3306.64,"to":3310.39,"location":2,"content":"the backward pass yeah okay"},{"from":3310.39,"to":3312.76,"location":2,"content":"so here's just one little more note on"},{"from":3312.76,"to":3317.23,"location":2,"content":"that you know in the early days of deep"},{"from":3317.23,"to":3321.34,"location":2,"content":"learning say prior to 2014 what we"},{"from":3321.34,"to":3323.29,"location":2,"content":"always just to say to everybody very"},{"from":3323.29,"to":3325.87,"location":2,"content":"sternly is you should check all your"},{"from":3325.87,"to":3328.18,"location":2,"content":"gradients by doing numeric gradient"},{"from":3328.18,"to":3330.88,"location":2,"content":"checks it's really really important and"},{"from":3330.88,"to":3334.42,"location":2,"content":"so what that meant was well you know if"},{"from":3334.42,"to":3336.22,"location":2,"content":"you want to know whether you've coded"},{"from":3336.22,"to":3339.31,"location":2,"content":"your backward pass right an easy way to"},{"from":3339.31,"to":3342.94,"location":2,"content":"check whether you've coded it right is"},{"from":3342.94,"to":3344.05,"location":2,"content":"to do"},{"from":3344.05,"to":3347.53,"location":2,"content":"this numeric gradient where you're sort"},{"from":3347.53,"to":3350.32,"location":2,"content":"of estimating the slope by wiggling it a"},{"from":3350.32,"to":3352.87,"location":2,"content":"bit and wiggling the input a bit and"},{"from":3352.87,"to":3355.51,"location":2,"content":"seeing what effect it has so I'm working"},{"from":3355.51,"to":3357.97,"location":2,"content":"out the value of the function that f of"},{"from":3357.97,"to":3360.88,"location":2,"content":"X plus h for H very small like e to the"},{"from":3360.88,"to":3364.33,"location":2,"content":"minus 4 and an f of X minus H and then"},{"from":3364.33,"to":3366.37,"location":2,"content":"dividing by 2h and I'm saying well what"},{"from":3366.37,"to":3368.2,"location":2,"content":"is the slope at this point and I'm"},{"from":3368.2,"to":3370.24,"location":2,"content":"getting a numerical estimate of the"},{"from":3370.24,"to":3373.99,"location":2,"content":"gradient with respect to my variable X"},{"from":3373.99,"to":3377.5,"location":2,"content":"here now so this is what you will have"},{"from":3377.5,"to":3380.05,"location":2,"content":"seen in high school when you did the"},{"from":3380.05,"to":3382.84,"location":2,"content":"sort of first estimates of gradients"},{"from":3382.84,"to":3384.88,"location":2,"content":"where you sort of worked out f of X plus"},{"from":3384.88,"to":3387.49,"location":2,"content":"h divided by H and you're doing rise"},{"from":3387.49,"to":3389.98,"location":2,"content":"over run and got a point estimate of the"},{"from":3389.98,"to":3393.22,"location":2,"content":"gradient exactly the same thing except"},{"from":3393.22,"to":3396.25,"location":2,"content":"for the fact in this case rather than"},{"from":3396.25,"to":3398.77,"location":2,"content":"doing it one-sided like that we're doing"},{"from":3398.77,"to":3401.17,"location":2,"content":"it two-sided it turns out that if you"},{"from":3401.17,"to":3403.42,"location":2,"content":"actually want to do this two-sided is"},{"from":3403.42,"to":3407.38,"location":2,"content":"asymptotically hugely better and so"},{"from":3407.38,"to":3409.3,"location":2,"content":"you're always better off doing two-sided"},{"from":3409.3,"to":3411.67,"location":2,"content":"gradient checks rather than one side of"},{"from":3411.67,"to":3414.73,"location":2,"content":"gradient checks so since you saw that"},{"from":3414.73,"to":3416.47,"location":2,"content":"since it's hard to implement this wrong"},{"from":3416.47,"to":3418.57,"location":2,"content":"this is a good way to check that your"},{"from":3418.57,"to":3420.61,"location":2,"content":"gradients are correct if you've defined"},{"from":3420.61,"to":3425.02,"location":2,"content":"them yourselves as a technique to use it"},{"from":3425.02,"to":3428.29,"location":2,"content":"for anything it's completely completely"},{"from":3428.29,"to":3431.05,"location":2,"content":"hopeless because we're thinking of doing"},{"from":3431.05,"to":3433.75,"location":2,"content":"this over our deep learning model for a"},{"from":3433.75,"to":3436.69,"location":2,"content":"fully connected layer what this means is"},{"from":3436.69,"to":3439.03,"location":2,"content":"that if you've got this sort of like a W"},{"from":3439.03,"to":3442.11,"location":2,"content":"matrix of N by M and you want to"},{"from":3442.11,"to":3446.08,"location":2,"content":"calculate your partial derivatives to"},{"from":3446.08,"to":3448.3,"location":2,"content":"check if they're correct it means that"},{"from":3448.3,"to":3450.58,"location":2,"content":"you have to do this for every element of"},{"from":3450.58,"to":3452.92,"location":2,"content":"the matrix so you have to calculate the"},{"from":3452.92,"to":3456.19,"location":2,"content":"eventual loss first jiggling w11 then"},{"from":3456.19,"to":3459.76,"location":2,"content":"jiggling w12 then jiggling 1 w 1 3 1 4"},{"from":3459.76,"to":3462.7,"location":2,"content":"etc so you have in the complex network"},{"from":3462.7,"to":3464.47,"location":2,"content":"you'll end up literally doing millions"},{"from":3464.47,"to":3466.87,"location":2,"content":"of function evaluations to check the"},{"from":3466.87,"to":3470.59,"location":2,"content":"gradients at one point in time so you"},{"from":3470.59,"to":3472.87,"location":2,"content":"know it's it's not like what I"},{"from":3472.87,"to":3474.97,"location":2,"content":"advertised for back prop when I said"},{"from":3474.97,"to":3477.75,"location":2,"content":"it's just as efficient as calculating"},{"from":3477.75,"to":3481.8,"location":2,"content":"the forward value doing this is forward"},{"from":3481.8,"to":3484.59,"location":2,"content":"value computation time x number of"},{"from":3484.59,"to":3486.81,"location":2,"content":"parameters in our model which is often"},{"from":3486.81,"to":3489.06,"location":2,"content":"huge for deep learning networks so this"},{"from":3489.06,"to":3490.65,"location":2,"content":"is something that you only want to have"},{"from":3490.65,"to":3493.5,"location":2,"content":"in side if statements that you could"},{"from":3493.5,"to":3495.45,"location":2,"content":"turn off so you could just so run it to"},{"from":3495.45,"to":3497.34,"location":2,"content":"check that your code isn't brick"},{"from":3497.34,"to":3501.99,"location":2,"content":"I am D buggy you know in honesty this is"},{"from":3501.99,"to":3504.03,"location":2,"content":"just much less needed now because you"},{"from":3504.03,"to":3505.65,"location":2,"content":"know by and large you can plug together"},{"from":3505.65,"to":3507.81,"location":2,"content":"your components and layers and PI torch"},{"from":3507.81,"to":3511.53,"location":2,"content":"and other people wrote the code right"},{"from":3511.53,"to":3514.17,"location":2,"content":"and it will work so you probably don't"},{"from":3514.17,"to":3515.82,"location":2,"content":"need to do this all the time but it is"},{"from":3515.82,"to":3517.95,"location":2,"content":"still a useful thing to look at to know"},{"from":3517.95,"to":3523.53,"location":2,"content":"about if things are going wrong yeah"},{"from":3523.53,"to":3525.48,"location":2,"content":"okay so if we've now mastered the core"},{"from":3525.48,"to":3527.31,"location":2,"content":"technology of neural nets we saw know"},{"from":3527.31,"to":3529.62,"location":2,"content":"basically everything we need to know"},{"from":3529.62,"to":3532.1,"location":2,"content":"about neural nets and I sort of just"},{"from":3532.1,"to":3535.56,"location":2,"content":"summarized it there just to sort of"},{"from":3535.56,"to":3542.07,"location":2,"content":"emphasize once more you know I think"},{"from":3542.07,"to":3545.55,"location":2,"content":"some people think why do we even need to"},{"from":3545.55,"to":3547.68,"location":2,"content":"learn all this stuff about gradients and"},{"from":3547.68,"to":3549.6,"location":2,"content":"there's a sense in which so don't really"},{"from":3549.6,"to":3551.01,"location":2,"content":"because these modern deep learning"},{"from":3551.01,"to":3553.32,"location":2,"content":"frameworks will compute all the"},{"from":3553.32,"to":3555.63,"location":2,"content":"gradients for you you know we make you"},{"from":3555.63,"to":3558.24,"location":2,"content":"suffer in homework 2 but in homework 3"},{"from":3558.24,"to":3560.85,"location":2,"content":"you can have your gradients and computed"},{"from":3560.85,"to":3563.4,"location":2,"content":"for you but you know I so you know it's"},{"from":3563.4,"to":3565.14,"location":2,"content":"sort of just like well why should you"},{"from":3565.14,"to":3568.05,"location":2,"content":"take a class on compilers right that"},{"from":3568.05,"to":3570.12,"location":2,"content":"there's actually something useful in"},{"from":3570.12,"to":3572.76,"location":2,"content":"understanding what goes on under the"},{"from":3572.76,"to":3575.25,"location":2,"content":"hood even though most of the time we're"},{"from":3575.25,"to":3577.32,"location":2,"content":"just perfectly happy to let the C"},{"from":3577.32,"to":3578.52,"location":2,"content":"compiler do its thing"},{"from":3578.52,"to":3581.58,"location":2,"content":"without being experts on x86 assembler"},{"from":3581.58,"to":3584.55,"location":2,"content":"every day of the world week but you know"},{"from":3584.55,"to":3586.16,"location":2,"content":"there is more to it than that"},{"from":3586.16,"to":3588.21,"location":2,"content":"you know because even though back"},{"from":3588.21,"to":3590.31,"location":2,"content":"propagation is great once you're"},{"from":3590.31,"to":3592.53,"location":2,"content":"building complex models back propagation"},{"from":3592.53,"to":3595.56,"location":2,"content":"doesn't always work as you would expect"},{"from":3595.56,"to":3597.87,"location":2,"content":"it to perfectly it may be the wrong word"},{"from":3597.87,"to":3599.67,"location":2,"content":"because you know mathematically it's"},{"from":3599.67,"to":3602.16,"location":2,"content":"perfect but it might not be achieving"},{"from":3602.16,"to":3604.35,"location":2,"content":"what you're wanting it to and well if"},{"from":3604.35,"to":3606.3,"location":2,"content":"you want to sort of in debug and improve"},{"from":3606.3,"to":3608.16,"location":2,"content":"models it's kind of crucial to"},{"from":3608.16,"to":3610.35,"location":2,"content":"understand what's going on there's a"},{"from":3610.35,"to":3611.31,"location":2,"content":"nice medium"},{"from":3611.31,"to":3613.83,"location":2,"content":"by Andre Kapaa fee of yes you should"},{"from":3613.83,"to":3616.8,"location":2,"content":"understand backdrop but on the syllabus"},{"from":3616.8,"to":3621.29,"location":2,"content":"page that talks about this and indeed"},{"from":3621.29,"to":3622.95,"location":2,"content":"week after next"},{"from":3622.95,"to":3624.75,"location":2,"content":"Abby is actually going to lecture about"},{"from":3624.75,"to":3626.67,"location":2,"content":"recurrent neural networks and you know"},{"from":3626.67,"to":3629.25,"location":2,"content":"one of the places where you can easily"},{"from":3629.25,"to":3632.61,"location":2,"content":"fail and doing that propagation turns up"},{"from":3632.61,"to":3637.71,"location":2,"content":"there it's a good example ok does anyone"},{"from":3637.71,"to":3639.9,"location":2,"content":"have any questions about back"},{"from":3639.9,"to":3646.89,"location":2,"content":"propagation and computation graphs okay"},{"from":3646.89,"to":3651.69,"location":2,"content":"if not the remainder of the time is the"},{"from":3651.69,"to":3653.97,"location":2,"content":"grab bag of things that you really"},{"from":3653.97,"to":3656.16,"location":2,"content":"should know about if you're going to be"},{"from":3656.16,"to":3658.53,"location":2,"content":"doing deep learning and so yeah this is"},{"from":3658.53,"to":3661.64,"location":2,"content":"just itsy-bitsy and but let me say them"},{"from":3661.64,"to":3666.06,"location":2,"content":"so up until now when we've had loss"},{"from":3666.06,"to":3669,"location":2,"content":"functions and we've been maximizing the"},{"from":3669,"to":3671.28,"location":2,"content":"likelihood of our data and stuff like"},{"from":3671.28,"to":3673.86,"location":2,"content":"that we've sort of just had this part"},{"from":3673.86,"to":3676.8,"location":2,"content":"here which is the likelihood of our data"},{"from":3676.8,"to":3682.25,"location":2,"content":"and we've worked to maximize it however"},{"from":3682.25,"to":3687.36,"location":2,"content":"in practice that works badly usually and"},{"from":3687.36,"to":3689.52,"location":2,"content":"we need to do something else which is"},{"from":3689.52,"to":3692.61,"location":2,"content":"regularize our models and if you've done"},{"from":3692.61,"to":3695.01,"location":2,"content":"the machine learning class or something"},{"from":3695.01,"to":3696.27,"location":2,"content":"like that you will have seen"},{"from":3696.27,"to":3698.91,"location":2,"content":"regularization and there are various"},{"from":3698.91,"to":3701.93,"location":2,"content":"techniques to do regularization but"},{"from":3701.93,"to":3704.34,"location":2,"content":"compared to anything else regularization"},{"from":3704.34,"to":3707.79,"location":2,"content":"is even more important for deep learning"},{"from":3707.79,"to":3711.99,"location":2,"content":"models right so the general idea is if"},{"from":3711.99,"to":3713.7,"location":2,"content":"you have a lot of parameters in your"},{"from":3713.7,"to":3716.96,"location":2,"content":"model those parameters can just"},{"from":3716.96,"to":3719.58,"location":2,"content":"essentially memorize what's in the data"},{"from":3719.58,"to":3721.8,"location":2,"content":"that you trained it and so they're very"},{"from":3721.8,"to":3724.62,"location":2,"content":"good at predicting the answers the model"},{"from":3724.62,"to":3726.12,"location":2,"content":"becomes very good at predicting their"},{"from":3726.12,"to":3728.76,"location":2,"content":"answers to the data you trained it on"},{"from":3728.76,"to":3732.21,"location":2,"content":"but the model may become poor at working"},{"from":3732.21,"to":3734.49,"location":2,"content":"in the real world and different examples"},{"from":3734.49,"to":3738.14,"location":2,"content":"and somehow we want to stop that and"},{"from":3738.14,"to":3741,"location":2,"content":"this problem is especially bad for deep"},{"from":3741,"to":3743.22,"location":2,"content":"learning models because typically deep"},{"from":3743.22,"to":3744.66,"location":2,"content":"learning models have"},{"from":3744.66,"to":3746.91,"location":2,"content":"vast numbers of parameters so in the"},{"from":3746.91,"to":3748.92,"location":2,"content":"good old days when statisticians ruled"},{"from":3748.92,"to":3752.16,"location":2,"content":"the show they told people that it was"},{"from":3752.16,"to":3754.95,"location":2,"content":"completely ridiculous to have a number"},{"from":3754.95,"to":3757.08,"location":2,"content":"of parameters that approached your"},{"from":3757.08,"to":3758.91,"location":2,"content":"number of training examples you know you"},{"from":3758.91,"to":3760.53,"location":2,"content":"should never have more parameters in"},{"from":3760.53,"to":3762.57,"location":2,"content":"your model than one-tenth of the number"},{"from":3762.57,"to":3764.91,"location":2,"content":"of your training examples from what's"},{"from":3764.91,"to":3767.13,"location":2,"content":"the kind of rules of thumb you are told"},{"from":3767.13,"to":3769.83,"location":2,"content":"so that you had lots of examples with"},{"from":3769.83,"to":3772.53,"location":2,"content":"which Westar made every parameter that's"},{"from":3772.53,"to":3775.08,"location":2,"content":"just not true deep learning models it's"},{"from":3775.08,"to":3777.12,"location":2,"content":"just really common that we train deep"},{"from":3777.12,"to":3779.28,"location":2,"content":"learning models that have ten times as"},{"from":3779.28,"to":3781.41,"location":2,"content":"many parameters as there we have"},{"from":3781.41,"to":3784.65,"location":2,"content":"training examples but miraculously it"},{"from":3784.65,"to":3787.14,"location":2,"content":"works in fact it works brilliantly those"},{"from":3787.14,"to":3790.23,"location":2,"content":"highly over parameterised models and"},{"from":3790.23,"to":3792.69,"location":2,"content":"it's one of the big secret sources of my"},{"from":3792.69,"to":3795.15,"location":2,"content":"deep learning has been so brilliant but"},{"from":3795.15,"to":3797.58,"location":2,"content":"it only works if we regularize the model"},{"from":3797.58,"to":3800.52,"location":2,"content":"so if you train a model without"},{"from":3800.52,"to":3803.82,"location":2,"content":"sufficient regularization what you find"},{"from":3803.82,"to":3806.46,"location":2,"content":"is that your training it and working out"},{"from":3806.46,"to":3809.43,"location":2,"content":"your loss on the training data and the"},{"from":3809.43,"to":3811.02,"location":2,"content":"model keeps on getting better and better"},{"from":3811.02,"to":3812.72,"location":2,"content":"and better and better"},{"from":3812.72,"to":3816.6,"location":2,"content":"necessarily our algorithm has to improve"},{"from":3816.6,"to":3818.67,"location":2,"content":"loss on the training data so the worst"},{"from":3818.67,"to":3820.29,"location":2,"content":"thing that could happen is that the"},{"from":3820.29,"to":3822.93,"location":2,"content":"graph could become absolutely fat flat"},{"from":3822.93,"to":3825.9,"location":2,"content":"what you will find is with most models"},{"from":3825.9,"to":3828.27,"location":2,"content":"that we train they have so many"},{"from":3828.27,"to":3830.55,"location":2,"content":"parameters that this will just keep on"},{"from":3830.55,"to":3833.49,"location":2,"content":"going down until the loss is sort of"},{"from":3833.49,"to":3835.35,"location":2,"content":"approaching the numerical precision of"},{"from":3835.35,"to":3837.48,"location":2,"content":"zero if you leave a training for long"},{"from":3837.48,"to":3839.97,"location":2,"content":"enough it just learns the correct answer"},{"from":3839.97,"to":3842.07,"location":2,"content":"that every example because because"},{"from":3842.07,"to":3844.77,"location":2,"content":"effectively can memorize examples okay"},{"from":3844.77,"to":3847.65,"location":2,"content":"but if you then say let me test out this"},{"from":3847.65,"to":3850.05,"location":2,"content":"model on some different data what you"},{"from":3850.05,"to":3853.17,"location":2,"content":"find is this red curve that up until a"},{"from":3853.17,"to":3857.37,"location":2,"content":"certain point that you're also building"},{"from":3857.37,"to":3858.99,"location":2,"content":"a model that's better at predicting on"},{"from":3858.99,"to":3861.54,"location":2,"content":"different data but after some point this"},{"from":3861.54,"to":3863.94,"location":2,"content":"curve starts to curve up again and"},{"from":3863.94,"to":3865.23,"location":2,"content":"ignore that bit where it seems to curve"},{"from":3865.23,"to":3866.73,"location":2,"content":"down again that was a mistake in the"},{"from":3866.73,"to":3869.34,"location":2,"content":"drawing and so this has then referred to"},{"from":3869.34,"to":3873.23,"location":2,"content":"as overfitting that them from here on"},{"from":3873.23,"to":3876.03,"location":2,"content":"the training model was just learning to"},{"from":3876.03,"to":3878.52,"location":2,"content":"memorize whatever was in"},{"from":3878.52,"to":3880.35,"location":2,"content":"the training data but not in a way that"},{"from":3880.35,"to":3884.97,"location":2,"content":"lets it generalize to other examples and"},{"from":3884.97,"to":3887.25,"location":2,"content":"so this is not what we want we want to"},{"from":3887.25,"to":3889.68,"location":2,"content":"try and avoid overfitting as much as"},{"from":3889.68,"to":3891.36,"location":2,"content":"possible and there are various"},{"from":3891.36,"to":3893.37,"location":2,"content":"regularization techniques that we use"},{"from":3893.37,"to":3896.01,"location":2,"content":"for that and a simple starting one is"},{"from":3896.01,"to":3898.74,"location":2,"content":"this one here where we penalize the"},{"from":3898.74,"to":3902.13,"location":2,"content":"log-likelihood by saying you're going to"},{"from":3902.13,"to":3905.01,"location":2,"content":"be penalized to the extent that you move"},{"from":3905.01,"to":3908.13,"location":2,"content":"parameters away from zero so the default"},{"from":3908.13,"to":3910.38,"location":2,"content":"state of nature is all parameters are"},{"from":3910.38,"to":3913.14,"location":2,"content":"zero so they're ignored in computations"},{"from":3913.14,"to":3915.36,"location":2,"content":"you can have parameters that have big"},{"from":3915.36,"to":3918.27,"location":2,"content":"values but you'll be penalized a bit for"},{"from":3918.27,"to":3920.04,"location":2,"content":"and this is referred to as l2"},{"from":3920.04,"to":3922.59,"location":2,"content":"regularization and you know that's sort"},{"from":3922.59,"to":3924.18,"location":2,"content":"of a starting point of something"},{"from":3924.18,"to":3925.17,"location":2,"content":"sensible you could do with"},{"from":3925.17,"to":3926.22,"location":2,"content":"regularization"},{"from":3926.22,"to":3929.1,"location":2,"content":"but there's more to say later and we'll"},{"from":3929.1,"to":3931.53,"location":2,"content":"talk in the sort of lecture before we"},{"from":3931.53,"to":3933.81,"location":2,"content":"discuss final projects of other clever"},{"from":3933.81,"to":3936.24,"location":2,"content":"and regularization techniques and neural"},{"from":3936.24,"to":3940.52,"location":2,"content":"networks ok grab bag number two"},{"from":3940.52,"to":3944.07,"location":2,"content":"vectorization is the term that you hear"},{"from":3944.07,"to":3946.95,"location":2,"content":"but it's not only vectors this is also"},{"from":3946.95,"to":3950.43,"location":2,"content":"matrix ization and higher dimensional"},{"from":3950.43,"to":3953.19,"location":2,"content":"matrices what are called tensors in this"},{"from":3953.19,"to":3956.25,"location":2,"content":"field tensor ization getting deep"},{"from":3956.25,"to":3958.65,"location":2,"content":"learning systems to run fast and"},{"from":3958.65,"to":3962.84,"location":2,"content":"efficiently is only possible if we"},{"from":3962.84,"to":3966.63,"location":2,"content":"vectorize things and what does that mean"},{"from":3966.63,"to":3969.96,"location":2,"content":"what that means is you know the"},{"from":3969.96,"to":3971.73,"location":2,"content":"straightforward way to write a lot of"},{"from":3971.73,"to":3975.21,"location":2,"content":"code that you saw in your first CS class"},{"from":3975.21,"to":3979.95,"location":2,"content":"is you say for I in range in calculate"},{"from":3979.95,"to":3985.11,"location":2,"content":"random Randi 1 but when we want to be"},{"from":3985.11,"to":3991.94,"location":2,"content":"clever people that are doing things fast"},{"from":3991.94,"to":3995.64,"location":2,"content":"we say rather than work out this w dot"},{"from":3995.64,"to":3999.33,"location":2,"content":"one-word vector at a time and do it in a"},{"from":3999.33,"to":4002.45,"location":2,"content":"for loop we could instead put all of our"},{"from":4002.45,"to":4005.6,"location":2,"content":"word vectors into one matrix and then do"},{"from":4005.6,"to":4009.86,"location":2,"content":"simply one matrix matrix multiply of W"},{"from":4009.86,"to":4012.53,"location":2,"content":"by our word vector matrix"},{"from":4012.53,"to":4016.31,"location":2,"content":"and even if you run your code on your"},{"from":4016.31,"to":4020,"location":2,"content":"laptop on the CPU you will find out that"},{"from":4020,"to":4020.81,"location":2,"content":"if you do it"},{"from":4020.81,"to":4023.3,"location":2,"content":"the vectorized way things will become"},{"from":4023.3,"to":4025.97,"location":2,"content":"hugely faster so in this example it"},{"from":4025.97,"to":4027.73,"location":2,"content":"became over an order of magnitude faster"},{"from":4027.73,"to":4030.53,"location":2,"content":"when doing it with a vector vectorized"},{"from":4030.53,"to":4034.4,"location":2,"content":"rather than with a for loop and those"},{"from":4034.4,"to":4037.31,"location":2,"content":"gains are only compounded when we run"},{"from":4037.31,"to":4040.28,"location":2,"content":"code on a GPU that you will get no gains"},{"from":4040.28,"to":4043.01,"location":2,"content":"and speed at all on a GPU unless your"},{"from":4043.01,"to":4044.69,"location":2,"content":"code is vectorized but if it is"},{"from":4044.69,"to":4046.67,"location":2,"content":"vectorized then you can hope to have"},{"from":4046.67,"to":4048.53,"location":2,"content":"results of how oh yeah this runs 40"},{"from":4048.53,"to":4053.35,"location":2,"content":"times faster than it did on the CPU okay"},{"from":4053.35,"to":4056.45,"location":2,"content":"yeah so always try to use vectors and"},{"from":4056.45,"to":4060.26,"location":2,"content":"matrices not for loops of course it's"},{"from":4060.26,"to":4061.94,"location":2,"content":"useful when developing stuff to time"},{"from":4061.94,"to":4065.63,"location":2,"content":"your code and find out what's slow okay"},{"from":4065.63,"to":4070.76,"location":2,"content":"point three okay so we discussed this"},{"from":4070.76,"to":4075.22,"location":2,"content":"idea last time and the time before that"},{"from":4075.22,"to":4078.89,"location":2,"content":"after after having the sort of a fine"},{"from":4078.89,"to":4082.16,"location":2,"content":"layer where we took you know go from X"},{"from":4082.16,"to":4084.62,"location":2,"content":"to W X plus B that's referred to as an"},{"from":4084.62,"to":4086.53,"location":2,"content":"affine layer so we're doing this"},{"from":4086.53,"to":4089.45,"location":2,"content":"multiplying a vector by matrix matrix"},{"from":4089.45,"to":4093.92,"location":2,"content":"and adding biases we necessarily to have"},{"from":4093.92,"to":4096.85,"location":2,"content":"power in a deep network have to have"},{"from":4096.85,"to":4099.53,"location":2,"content":"some form of non-linearity"},{"from":4099.53,"to":4101.48,"location":2,"content":"and so I just wanted to go through a bit"},{"from":4101.48,"to":4104.3,"location":2,"content":"of background on non-linearity isn't"},{"from":4104.3,"to":4107.78,"location":2,"content":"what people use and what to use so if"},{"from":4107.78,"to":4109.91,"location":2,"content":"you're sort of starting from the idea of"},{"from":4109.91,"to":4112.63,"location":2,"content":"what we know as logistic regression"},{"from":4112.63,"to":4115.04,"location":2,"content":"what's commonly referred to as the"},{"from":4115.04,"to":4117.71,"location":2,"content":"sigmoid curve or maybe more precisely"},{"from":4117.71,"to":4121.67,"location":2,"content":"look the logistic function is this"},{"from":4121.67,"to":4124.15,"location":2,"content":"picture here so it's something that"},{"from":4124.15,"to":4127.04,"location":2,"content":"squashes any real number positive or"},{"from":4127.04,"to":4129.71,"location":2,"content":"negative into the range zero to one it"},{"from":4129.71,"to":4133.34,"location":2,"content":"gives you a probability output these"},{"from":4133.34,"to":4137.3,"location":2,"content":"this use of this logistic function was"},{"from":4137.3,"to":4139.91,"location":2,"content":"really really common in early neural"},{"from":4139.91,"to":4142.13,"location":2,"content":"nets if you go back to 80s 90s neural"},{"from":4142.13,"to":4142.55,"location":2,"content":"Nets"},{"from":4142.55,"to":4146.03,"location":2,"content":"there were sigmoid functions absolutely"},{"from":4146.03,"to":4151.37,"location":2,"content":"everywhere in more recent times 90% of"},{"from":4151.37,"to":4153.83,"location":2,"content":"the time nobody uses these and they've"},{"from":4153.83,"to":4155.21,"location":2,"content":"been found to sort of actually work"},{"from":4155.21,"to":4157.79,"location":2,"content":"quite poorly the only place these are"},{"from":4157.79,"to":4162.11,"location":2,"content":"used is when you actually want a value"},{"from":4162.11,"to":4164.96,"location":2,"content":"between 0 & 1 as your output so we'll"},{"from":4164.96,"to":4167.57,"location":2,"content":"talk later about how you have gating in"},{"from":4167.57,"to":4169.76,"location":2,"content":"networks and so gating as a place where"},{"from":4169.76,"to":4171.59,"location":2,"content":"you want to have a probability between"},{"from":4171.59,"to":4173.96,"location":2,"content":"two things and then you will use one of"},{"from":4173.96,"to":4176.15,"location":2,"content":"those but you use them absolutely"},{"from":4176.15,"to":4180.92,"location":2,"content":"nowhere else here is the 10 H curve so"},{"from":4180.92,"to":4184.13,"location":2,"content":"the formula for 10h looks like a scary"},{"from":4184.13,"to":4186.02,"location":2,"content":"thing with lots of Exponential's in it"},{"from":4186.02,"to":4189.02,"location":2,"content":"and it doesn't really look much like a"},{"from":4189.02,"to":4193.7,"location":2,"content":"logistic curve whatsoever but if you dig"},{"from":4193.7,"to":4195.59,"location":2,"content":"up your math textbook you can convince"},{"from":4195.59,"to":4198.56,"location":2,"content":"yourself that a 10h curve is actually"},{"from":4198.56,"to":4200.78,"location":2,"content":"exactly the same as the logistic curve"},{"from":4200.78,"to":4204.44,"location":2,"content":"apart from you multiply it by two so it"},{"from":4204.44,"to":4206.6,"location":2,"content":"has a range of two rather than one and"},{"from":4206.6,"to":4208.91,"location":2,"content":"you shift it down one so this is sort of"},{"from":4208.91,"to":4211.13,"location":2,"content":"just a rescaled logistic but it's now"},{"from":4211.13,"to":4214.16,"location":2,"content":"symmetric between 1 and -1 and the fact"},{"from":4214.16,"to":4216.14,"location":2,"content":"it's symmetric in the output actually"},{"from":4216.14,"to":4218.66,"location":2,"content":"helps a lot for putting into neural"},{"from":4218.66,"to":4223.04,"location":2,"content":"networks so teenagers are still"},{"from":4223.04,"to":4225.44,"location":2,"content":"reasonably widely used and quite a"},{"from":4225.44,"to":4229.46,"location":2,"content":"number of places in neural networks so"},{"from":4229.46,"to":4231.47,"location":2,"content":"10h should be a friend of yours and you"},{"from":4231.47,"to":4234.14,"location":2,"content":"should know about that but you know one"},{"from":4234.14,"to":4237.58,"location":2,"content":"of the bad things about using"},{"from":4237.58,"to":4239.75,"location":2,"content":"transcendental functions like the"},{"from":4239.75,"to":4242.6,"location":2,"content":"sigmoid or 10h is you know they involve"},{"from":4242.6,"to":4247.37,"location":2,"content":"this expensive math operations that slow"},{"from":4247.37,"to":4250.61,"location":2,"content":"you down like it's sort of a nuisance to"},{"from":4250.61,"to":4252.56,"location":2,"content":"be kind of computing Exponential's and"},{"from":4252.56,"to":4254.42,"location":2,"content":"ten HS and new computer things are kind"},{"from":4254.42,"to":4257.78,"location":2,"content":"of slow so people started playing around"},{"from":4257.78,"to":4260.42,"location":2,"content":"with ways of to make things faster and"},{"from":4260.42,"to":4262.85,"location":2,"content":"so someone came up with this idea like"},{"from":4262.85,"to":4265.51,"location":2,"content":"maybe we could come up with a hard 10 H"},{"from":4265.51,"to":4268.22,"location":2,"content":"where it's just sort of flat out here"},{"from":4268.22,"to":4270.86,"location":2,"content":"and then it has a linear slope and then"},{"from":4270.86,"to":4272.69,"location":2,"content":"it's flat at the top you know it sort of"},{"from":4272.69,"to":4275.27,"location":2,"content":"looks like a 10 H but we've just squared"},{"from":4275.27,"to":4278.27,"location":2,"content":"it off and well this is really cheap to"},{"from":4278.27,"to":4279.81,"location":2,"content":"compute right you say"},{"from":4279.81,"to":4285.78,"location":2,"content":"less than -1 return -1 return +1 or just"},{"from":4285.78,"to":4287.75,"location":2,"content":"return the number no complex"},{"from":4287.75,"to":4290.82,"location":2,"content":"transcendentals the funny thing is it"},{"from":4290.82,"to":4292.17,"location":2,"content":"turns out that this actually works"},{"from":4292.17,"to":4295.32,"location":2,"content":"pretty well you might be scared and you"},{"from":4295.32,"to":4297.81,"location":2,"content":"might justify ibly b squared because if"},{"from":4297.81,"to":4300.84,"location":2,"content":"you start thinking about gradients once"},{"from":4300.84,"to":4302.46,"location":2,"content":"you're over here there's no gradient"},{"from":4302.46,"to":4307.22,"location":2,"content":"right it's tube completely flat at 0 so"},{"from":4307.22,"to":4309.99,"location":2,"content":"things go dead as soon as they're out at"},{"from":4309.99,"to":4311.94,"location":2,"content":"one of the ends so it's sort of"},{"from":4311.94,"to":4313.59,"location":2,"content":"important to stay in this middle section"},{"from":4313.59,"to":4315.99,"location":2,"content":"at least for a while and then it's just"},{"from":4315.99,"to":4318.84,"location":2,"content":"got a slope of 1 right it's a constant"},{"from":4318.84,"to":4321.36,"location":2,"content":"slope of 1 but this is enough of a"},{"from":4321.36,"to":4324.84,"location":2,"content":"linearity that actually it works well in"},{"from":4324.84,"to":4327.72,"location":2,"content":"neural networks and you can train neural"},{"from":4327.72,"to":4330.72,"location":2,"content":"networks so that sent the whole field in"},{"from":4330.72,"to":4333.18,"location":2,"content":"the opposite direction and people"},{"from":4333.18,"to":4337.05,"location":2,"content":"thought oh if that works maybe we can"},{"from":4337.05,"to":4339.81,"location":2,"content":"make things even simpler and that led to"},{"from":4339.81,"to":4342.3,"location":2,"content":"the now-famous what's referred to"},{"from":4342.3,"to":4345.09,"location":2,"content":"everywhere as a reloj sorry I miss"},{"from":4345.09,"to":4347.04,"location":2,"content":"there's a mistake in my editing there"},{"from":4347.04,"to":4349.65,"location":2,"content":"delete off hard 10 H that was in the"},{"from":4349.65,"to":4352.62,"location":2,"content":"slides by mistake the r-la unit everyone"},{"from":4352.62,"to":4354.36,"location":2,"content":"calls it reloj which stands for"},{"from":4354.36,"to":4358.5,"location":2,"content":"rectified linear unit so the the reloj"},{"from":4358.5,"to":4360.06,"location":2,"content":"is essentially the simplest"},{"from":4360.06,"to":4363.77,"location":2,"content":"non-linearity you can have so the relu"},{"from":4363.77,"to":4368.07,"location":2,"content":"is zero slope zero as soon as you're in"},{"from":4368.07,"to":4370.5,"location":2,"content":"the negative regime and it's just a line"},{"from":4370.5,"to":4372.72,"location":2,"content":"slope one when you're in the positive"},{"from":4372.72,"to":4376.71,"location":2,"content":"regime I mean when I first saw this I"},{"from":4376.71,"to":4378.6,"location":2,"content":"mean it sort of blew my mind it could"},{"from":4378.6,"to":4380.97,"location":2,"content":"possibly work because it's sort of I"},{"from":4380.97,"to":4383.55,"location":2,"content":"guess I was brought up on these sort of"},{"from":4383.55,"to":4386.37,"location":2,"content":"10 HS and sigmoids and there's sort of"},{"from":4386.37,"to":4388.35,"location":2,"content":"these arguments about the slope and you"},{"from":4388.35,"to":4391.62,"location":2,"content":"get these gradients and you can move"},{"from":4391.62,"to":4394.53,"location":2,"content":"around with the gradient and how is it"},{"from":4394.53,"to":4396.48,"location":2,"content":"meant to work if half of this function"},{"from":4396.48,"to":4399,"location":2,"content":"just has output zero and no gradient and"},{"from":4399,"to":4400.83,"location":2,"content":"the other half is just this straight"},{"from":4400.83,"to":4403.83,"location":2,"content":"line and in particular when you're in"},{"from":4403.83,"to":4406.41,"location":2,"content":"the positive regime this is just an"},{"from":4406.41,"to":4409.8,"location":2,"content":"identity function and you know I sort of"},{"from":4409.8,"to":4412.23,"location":2,"content":"argued before that if you've just"},{"from":4412.23,"to":4413.13,"location":2,"content":"compose"},{"from":4413.13,"to":4415.89,"location":2,"content":"linear transforms you don't get any"},{"from":4415.89,"to":4418.29,"location":2,"content":"power but providing we're in these the"},{"from":4418.29,"to":4421.05,"location":2,"content":"right-hand part of the regime since this"},{"from":4421.05,"to":4422.82,"location":2,"content":"is an identity function that's exactly"},{"from":4422.82,"to":4424.32,"location":2,"content":"what we're doing we're just composing"},{"from":4424.32,"to":4427.2,"location":2,"content":"linear transforms so you sort of believe"},{"from":4427.2,"to":4429.42,"location":2,"content":"it just couldn't possibly work but it"},{"from":4429.42,"to":4431.06,"location":2,"content":"turns out that this works brilliantly"},{"from":4431.06,"to":4435.39,"location":2,"content":"and this is now by far the default"},{"from":4435.39,"to":4438.3,"location":2,"content":"choice when people are building feed for"},{"from":4438.3,"to":4441.11,"location":2,"content":"deep networks that people use really"},{"from":4441.11,"to":4444.54,"location":2,"content":"nonlinearities and they are very fast"},{"from":4444.54,"to":4448.05,"location":2,"content":"they train very quickly and they perform"},{"from":4448.05,"to":4450.72,"location":2,"content":"very well and so effectively you know"},{"from":4450.72,"to":4453.45,"location":2,"content":"the it is it is simply just each you"},{"from":4453.45,"to":4456.33,"location":2,"content":"depending on the inputs each unit is"},{"from":4456.33,"to":4458.82,"location":2,"content":"just either dead or it's passing things"},{"from":4458.82,"to":4460.89,"location":2,"content":"on as an identity function but there's"},{"from":4460.89,"to":4463.86,"location":2,"content":"enough of a linear non-linearity that"},{"from":4463.86,"to":4465.33,"location":2,"content":"you can do arbitrary function"},{"from":4465.33,"to":4467.49,"location":2,"content":"approximation still with a deep learning"},{"from":4467.49,"to":4469.83,"location":2,"content":"network and people now make precisely"},{"from":4469.83,"to":4473,"location":2,"content":"the opposite argument which is because"},{"from":4473,"to":4479.13,"location":2,"content":"this unit just has a slope of 1 over"},{"from":4479.13,"to":4481.98,"location":2,"content":"it's nonzero range that means the"},{"from":4481.98,"to":4484.68,"location":2,"content":"gradient is passed back very efficiently"},{"from":4484.68,"to":4487.56,"location":2,"content":"to the inputs and therefore them the"},{"from":4487.56,"to":4490.74,"location":2,"content":"models train very efficiently whereas"},{"from":4490.74,"to":4493.11,"location":2,"content":"when you were with these kind of curves"},{"from":4493.11,"to":4495.15,"location":2,"content":"when you're over here there's very"},{"from":4495.15,"to":4497.43,"location":2,"content":"little slope so your models might train"},{"from":4497.43,"to":4502.02,"location":2,"content":"very slowly ok so you know for"},{"from":4502.02,"to":4503.91,"location":2,"content":"feed-forward network try this before you"},{"from":4503.91,"to":4506.4,"location":2,"content":"try anything else but there's sort of"},{"from":4506.4,"to":4508.68,"location":2,"content":"them being a sub literature that says"},{"from":4508.68,"to":4511.44,"location":2,"content":"well maybe that's too simple and we"},{"from":4511.44,"to":4513.57,"location":2,"content":"could do a bit better and so that led to"},{"from":4513.57,"to":4516.09,"location":2,"content":"the leaky reloj which said maybe we"},{"from":4516.09,"to":4518.1,"location":2,"content":"should put a tiny bit of slope over here"},{"from":4518.1,"to":4520.29,"location":2,"content":"so it's not completely dead so you can"},{"from":4520.29,"to":4524.01,"location":2,"content":"make it something like 1 1/100 as the"},{"from":4524.01,"to":4526.35,"location":2,"content":"slope is this part and then people had"},{"from":4526.35,"to":4529.11,"location":2,"content":"well let's build off that maybe we could"},{"from":4529.11,"to":4531.48,"location":2,"content":"actually put another parameter into our"},{"from":4531.48,"to":4533.28,"location":2,"content":"neural network and we could have a"},{"from":4533.28,"to":4536.37,"location":2,"content":"parametric reloj so there's some slope"},{"from":4536.37,"to":4539.25,"location":2,"content":"over here but we're also going to back"},{"from":4539.25,"to":4543.39,"location":2,"content":"propagate into our non-linearity which"},{"from":4543.39,"to":4545.67,"location":2,"content":"has this extra alpha parameter which is"},{"from":4545.67,"to":4546.8,"location":2,"content":"how many much"},{"from":4546.8,"to":4550.22,"location":2,"content":"and so variously people have used these"},{"from":4550.22,"to":4553.34,"location":2,"content":"you can sort of find tin papers on"},{"from":4553.34,"to":4555.5,"location":2,"content":"archive where people say you can get"},{"from":4555.5,"to":4557.03,"location":2,"content":"better results from using one or other"},{"from":4557.03,"to":4559.73,"location":2,"content":"of these you can also find papers where"},{"from":4559.73,"to":4561.53,"location":2,"content":"people say it made no difference for"},{"from":4561.53,"to":4564.23,"location":2,"content":"them versus just using a reloj so I"},{"from":4564.23,"to":4566.33,"location":2,"content":"think basically you can start off with a"},{"from":4566.33,"to":4570.37,"location":2,"content":"reloj and work from there yeah so"},{"from":4570.37,"to":4574.97,"location":2,"content":"parameter initialization it's when so"},{"from":4574.97,"to":4576.32,"location":2,"content":"when we have these matrices of"},{"from":4576.32,"to":4579.62,"location":2,"content":"parameters in our model it's vital vital"},{"from":4579.62,"to":4584.14,"location":2,"content":"vital that you have to initialize those"},{"from":4584.14,"to":4586.73,"location":2,"content":"parameter weights with small random"},{"from":4586.73,"to":4589.85,"location":2,"content":"values this was precisely the lesson"},{"from":4589.85,"to":4591.89,"location":2,"content":"that some people hadn't discovered when"},{"from":4591.89,"to":4593.93,"location":2,"content":"it came to final project time so I'll"},{"from":4593.93,"to":4597.68,"location":2,"content":"emphasize it vital vital so if you just"},{"from":4597.68,"to":4599.57,"location":2,"content":"start off with the weights being zero"},{"from":4599.57,"to":4601.73,"location":2,"content":"you kind of have these complete"},{"from":4601.73,"to":4603.8,"location":2,"content":"symmetries right that everything will be"},{"from":4603.8,"to":4606.26,"location":2,"content":"calculated the same everything will move"},{"from":4606.26,"to":4608.35,"location":2,"content":"the same and you're not actually"},{"from":4608.35,"to":4611.51,"location":2,"content":"training this complex network with a lot"},{"from":4611.51,"to":4613.55,"location":2,"content":"of Units that are specializing to learn"},{"from":4613.55,"to":4615.74,"location":2,"content":"different things so somehow you have to"},{"from":4615.74,"to":4618.17,"location":2,"content":"break the symmetry and we do that by"},{"from":4618.17,"to":4621.02,"location":2,"content":"giving small random weights so you know"},{"from":4621.02,"to":4623.27,"location":2,"content":"there's sort of some fine points when"},{"from":4623.27,"to":4625.22,"location":2,"content":"you have biases you may as well just"},{"from":4625.22,"to":4627.38,"location":2,"content":"start them at zero as neutral and see"},{"from":4627.38,"to":4629.78,"location":2,"content":"how the system learn the bias that you"},{"from":4629.78,"to":4633.2,"location":2,"content":"want et cetera but in general the"},{"from":4633.2,"to":4637.04,"location":2,"content":"weights you want to initialize the small"},{"from":4637.04,"to":4641.6,"location":2,"content":"random values you'll find in height or"},{"from":4641.6,"to":4644.87,"location":2,"content":"other deep learning practice packages a"},{"from":4644.87,"to":4647.45,"location":2,"content":"common initialization that's used and"},{"from":4647.45,"to":4649.24,"location":2,"content":"often recommended as this Xavier"},{"from":4649.24,"to":4652.67,"location":2,"content":"initialization and so the trick of this"},{"from":4652.67,"to":4656.36,"location":2,"content":"is that for a lot of models in a lot of"},{"from":4656.36,"to":4658.94,"location":2,"content":"places think of some of these things"},{"from":4658.94,"to":4661.85,"location":2,"content":"like these ones and these you'd like the"},{"from":4661.85,"to":4664.31,"location":2,"content":"values in the network to sort of stay"},{"from":4664.31,"to":4668.6,"location":2,"content":"small in this sort of middle range here"},{"from":4668.6,"to":4671.12,"location":2,"content":"and well if you kind of have a matrix"},{"from":4671.12,"to":4675.31,"location":2,"content":"with big values in it and you multiply"},{"from":4675.31,"to":4678.05,"location":2,"content":"vector by this matrix you know things"},{"from":4678.05,"to":4679.7,"location":2,"content":"might get bigger and then if you put in"},{"from":4679.7,"to":4680.69,"location":2,"content":"through another layer"},{"from":4680.69,"to":4682.76,"location":2,"content":"get bigger again and then sort of"},{"from":4682.76,"to":4685.19,"location":2,"content":"everything I'll be too big and you'll"},{"from":4685.19,"to":4687.22,"location":2,"content":"have problems so really Xavier"},{"from":4687.22,"to":4689.75,"location":2,"content":"initializations seeking to avoid that by"},{"from":4689.75,"to":4693.59,"location":2,"content":"saying how many inputs are there to this"},{"from":4693.59,"to":4696.8,"location":2,"content":"node how many outputs are there we want"},{"from":4696.8,"to":4698.45,"location":2,"content":"to sort of tamp it down the"},{"from":4698.45,"to":4701,"location":2,"content":"initialization based on the inputs and"},{"from":4701,"to":4703.37,"location":2,"content":"the outputs because effectively we'll be"},{"from":4703.37,"to":4707.57,"location":2,"content":"using this number that many times it's a"},{"from":4707.57,"to":4710.65,"location":2,"content":"good thing to use you can use that"},{"from":4710.65,"to":4712.66,"location":2,"content":"optimizers"},{"from":4712.66,"to":4716.54,"location":2,"content":"up till now we saw just talked about"},{"from":4716.54,"to":4721.52,"location":2,"content":"plain SGD you know normally plain SGD"},{"from":4721.52,"to":4724.88,"location":2,"content":"actually works just fine but often if"},{"from":4724.88,"to":4727.34,"location":2,"content":"you want to use just plain SGD you have"},{"from":4727.34,"to":4729.68,"location":2,"content":"to spend time tuning the learning rate"},{"from":4729.68,"to":4732.73,"location":2,"content":"that alpha that we multiplied the"},{"from":4732.73,"to":4736.22,"location":2,"content":"gradient by for complex nets and"},{"from":4736.22,"to":4738.77,"location":2,"content":"situations or to avoid worry there's"},{"from":4738.77,"to":4740.78,"location":2,"content":"sort of now this big family and more"},{"from":4740.78,"to":4744.79,"location":2,"content":"sophisticated adaptive optimizers and so"},{"from":4744.79,"to":4746.93,"location":2,"content":"effectively they're scaling the"},{"from":4746.93,"to":4749,"location":2,"content":"parameter adjustment by accumulated"},{"from":4749,"to":4751.1,"location":2,"content":"gradients which have the effect that"},{"from":4751.1,"to":4754.13,"location":2,"content":"they learn per parameter learning rate"},{"from":4754.13,"to":4756.86,"location":2,"content":"so that they conceived which parameters"},{"from":4756.86,"to":4759.23,"location":2,"content":"would be useful to move more and which"},{"from":4759.23,"to":4761.18,"location":2,"content":"ones less depending on the sensitivity"},{"from":4761.18,"to":4763.49,"location":2,"content":"of those parameters so where things are"},{"from":4763.49,"to":4765.41,"location":2,"content":"flat you can be trying to move quickly"},{"from":4765.41,"to":4767.63,"location":2,"content":"where things are bouncing around a lot"},{"from":4767.63,"to":4768.95,"location":2,"content":"you can be trying to move just a little"},{"from":4768.95,"to":4771.11,"location":2,"content":"so as not to overshoot and so there's a"},{"from":4771.11,"to":4773.21,"location":2,"content":"whole family of these a degrade rmsprop"},{"from":4773.21,"to":4775.4,"location":2,"content":"Atom they're actually other ones there's"},{"from":4775.4,"to":4778.16,"location":2,"content":"a de Max and a lot of them I mean Adam"},{"from":4778.16,"to":4780.53,"location":2,"content":"is one fairly reliable one that many"},{"from":4780.53,"to":4783.89,"location":2,"content":"people use and that's not bad and then"},{"from":4783.89,"to":4786.14,"location":2,"content":"one more slide and I'm done yeah so"},{"from":4786.14,"to":4789.74,"location":2,"content":"learning rates so normally you have to"},{"from":4789.74,"to":4792.56,"location":2,"content":"choose a learning rate so one choice is"},{"from":4792.56,"to":4794.93,"location":2,"content":"just have a constant learning rate you"},{"from":4794.93,"to":4796.85,"location":2,"content":"pick a number maybe 10 to the minus 3"},{"from":4796.85,"to":4800.57,"location":2,"content":"and say that's my learning rate you want"},{"from":4800.57,"to":4802.34,"location":2,"content":"your learning rate to be order of"},{"from":4802.34,"to":4805.16,"location":2,"content":"magnitude right if your learning rate is"},{"from":4805.16,"to":4809.72,"location":2,"content":"too big your model might diverge or not"},{"from":4809.72,"to":4812.24,"location":2,"content":"converge because adjusts of leaps you"},{"from":4812.24,"to":4813.92,"location":2,"content":"around by huge cramp"},{"from":4813.92,"to":4816.76,"location":2,"content":"movements and you completely miss the"},{"from":4816.76,"to":4819.86,"location":2,"content":"good parts of your function space if"},{"from":4819.86,"to":4821.69,"location":2,"content":"your model if your learning rate is too"},{"from":4821.69,"to":4825.35,"location":2,"content":"small your model may not train by the"},{"from":4825.35,"to":4827.45,"location":2,"content":"assignment deadline and then you'll be"},{"from":4827.45,"to":4830.39,"location":2,"content":"unhappy so if you saw it you know"},{"from":4830.39,"to":4833.72,"location":2,"content":"commonly people sort of try powers of"},{"from":4833.72,"to":4835.82,"location":2,"content":"ten and sees how it looks right they"},{"from":4835.82,"to":4840.56,"location":2,"content":"might try you know 0.0 1.0 01.00 1 and"},{"from":4840.56,"to":4843.5,"location":2,"content":"see look at how the loss is declining"},{"from":4843.5,"to":4845.93,"location":2,"content":"and see what seems to work in general"},{"from":4845.93,"to":4847.76,"location":2,"content":"you want to use the fastest learning"},{"from":4847.76,"to":4849.68,"location":2,"content":"rate that isn't making things become"},{"from":4849.68,"to":4853.46,"location":2,"content":"unstable commonly you get better results"},{"from":4853.46,"to":4857.27,"location":2,"content":"by decreasing the learning rate as you"},{"from":4857.27,"to":4859.73,"location":2,"content":"train so sometimes people just do that"},{"from":4859.73,"to":4862.43,"location":2,"content":"by hand so we used the term epoch for a"},{"from":4862.43,"to":4864.41,"location":2,"content":"full pass through your training data and"},{"from":4864.41,"to":4866.99,"location":2,"content":"people might say half the learning rate"},{"from":4866.99,"to":4869.03,"location":2,"content":"after every three epochs as you train"},{"from":4869.03,"to":4871.55,"location":2,"content":"and that can work pretty well you can"},{"from":4871.55,"to":4874.82,"location":2,"content":"use formulas to get per epoch rate"},{"from":4874.82,"to":4877.79,"location":2,"content":"learning rates there are even fancier"},{"from":4877.79,"to":4880.07,"location":2,"content":"methods you can look up cyclic learning"},{"from":4880.07,"to":4882.65,"location":2,"content":"rates online if you want which sort of"},{"from":4882.65,"to":4884.21,"location":2,"content":"actually makes the learning rate"},{"from":4884.21,"to":4885.98,"location":2,"content":"sometimes bigger and then sometimes"},{"from":4885.98,"to":4887.93,"location":2,"content":"smaller and people have found that that"},{"from":4887.93,"to":4890.09,"location":2,"content":"can be useful for getting you out of bad"},{"from":4890.09,"to":4894.02,"location":2,"content":"regions and interesting ways the one"},{"from":4894.02,"to":4896.21,"location":2,"content":"other thing to know is if you're using"},{"from":4896.21,"to":4899.36,"location":2,"content":"one of the fancier optimizers they still"},{"from":4899.36,"to":4901.58,"location":2,"content":"ask you for a learning rate but that"},{"from":4901.58,"to":4904.7,"location":2,"content":"learning rate is the initial learning"},{"from":4904.7,"to":4907.22,"location":2,"content":"rate which typically the optimizer will"},{"from":4907.22,"to":4910.91,"location":2,"content":"shrink as you train so commonly if"},{"from":4910.91,"to":4913.91,"location":2,"content":"you're using something like atom you"},{"from":4913.91,"to":4916.4,"location":2,"content":"might be starting off by saying the"},{"from":4916.4,"to":4919.13,"location":2,"content":"learning raises zero point one sort of a"},{"from":4919.13,"to":4921.32,"location":2,"content":"bigger number and it'll be shrinking it"},{"from":4921.32,"to":4924.89,"location":2,"content":"later as the training goes along ok all"},{"from":4924.89,"to":4929.41,"location":2,"content":"done see you next week"}]}