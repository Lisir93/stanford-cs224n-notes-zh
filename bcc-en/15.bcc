{"font_size":0.4,"font_color":"#FFFFFF","background_alpha":0.5,"background_color":"#9C27B0","Stroke":"none","body":[{"from":4.76,"to":9.99,"location":2,"content":"So today we're gonna be learning about Natural Language Generation."},{"from":9.99,"to":12.46,"location":2,"content":"And uh, this is probably going to be a little different to"},{"from":12.46,"to":16.38,"location":2,"content":"my previous lectures because this is going to be much more of a kind of survey,"},{"from":16.38,"to":17.75,"location":2,"content":"of lots of cutting edge, uh,"},{"from":17.75,"to":21.06,"location":2,"content":"research topics that are happening in NLG right now."},{"from":21.06,"to":22.8,"location":2,"content":"So before we get to that, uh,"},{"from":22.8,"to":24.32,"location":2,"content":"we've got a few announcements."},{"from":24.32,"to":26.37,"location":2,"content":"Uh, so I guess the main announcement is just,"},{"from":26.37,"to":28.29,"location":2,"content":"thank you all so much for your hard work."},{"from":28.29,"to":31.02,"location":2,"content":"I know, um, the last week or two have been pretty tough."},{"from":31.02,"to":33.45,"location":2,"content":"Uh, assignment five was really quite difficult,"},{"from":33.45,"to":35.3,"location":2,"content":"I think, and it was a challenge to do it in eight days."},{"from":35.3,"to":38,"location":2,"content":"So we just really appreciate all the hard work you've put in."},{"from":38,"to":40.91,"location":2,"content":"Um, we also understand the project proposal was,"},{"from":40.91,"to":45.59,"location":2,"content":"uh, sometimes a bit difficult to understand the expectations for some people."},{"from":45.59,"to":47.99,"location":2,"content":"Um, so, yeah, these are both new components of"},{"from":47.99,"to":50.47,"location":2,"content":"the class this year that were not present last year."},{"from":50.47,"to":52.12,"location":2,"content":"Um, so you know,"},{"from":52.12,"to":55.27,"location":2,"content":"we have to go through some learning curves as well as the teaching staff."},{"from":55.27,"to":57.71,"location":2,"content":"So just we really want to say thank you so much, uh,"},{"from":57.71,"to":60.37,"location":2,"content":"for putting everything into this class."},{"from":60.37,"to":63.23,"location":2,"content":"And please do continue to give us your feedback both right"},{"from":63.23,"to":67.08,"location":2,"content":"now and in the end of quarter feedback survey."},{"from":67.08,"to":70.69,"location":2,"content":"Okay, so here's the overview for what we're going to be doing today."},{"from":70.69,"to":73.95,"location":2,"content":"So today we're going to learn about what's happening in the world"},{"from":73.95,"to":76.96,"location":2,"content":"of neural approaches for Natural Language Generation."},{"from":76.96,"to":78.69,"location":2,"content":"Uh, that is a super,"},{"from":78.69,"to":82.06,"location":2,"content":"super broad title, Natural Language Generation."},{"from":82.06,"to":85.69,"location":2,"content":"Um, NLG encompasses a huge variety of research areas"},{"from":85.69,"to":87.8,"location":2,"content":"and pretty much each of those could have had"},{"from":87.8,"to":89.83,"location":2,"content":"their own lectures and we could have taught a whole,"},{"from":89.83,"to":94.03,"location":2,"content":"a whole quarter- quarter's worth of classes on, ah, NLG."},{"from":94.03,"to":97.47,"location":2,"content":"Uh, but we're going to try to cover a selection of things today."},{"from":97.47,"to":101.52,"location":2,"content":"And, um, uh, it's mostly going to be, uh,"},{"from":101.52,"to":102.89,"location":2,"content":"guided by the things which, uh,"},{"from":102.89,"to":106.04,"location":2,"content":"I've seen that I think are cool or interesting or exciting."},{"from":106.04,"to":108.14,"location":2,"content":"So it's by no means going to be comprehensive but"},{"from":108.14,"to":112.07,"location":2,"content":"I hope you're going to enjoy some of the stuff we're going to learn about."},{"from":112.07,"to":116.66,"location":2,"content":"Okay, so in particular we're going to start off by having a recap of what we"},{"from":116.66,"to":121.07,"location":2,"content":"already know about Natural Language Generation to make sure we're on the same page."},{"from":121.07,"to":124.23,"location":2,"content":"And we're also going to learn a little bit extra about decoding algorithms."},{"from":124.23,"to":125.87,"location":2,"content":"So we learned a bit before about, uh,"},{"from":125.87,"to":128.33,"location":2,"content":"greedy decoding and beam search decoding,"},{"from":128.33,"to":130.28,"location":2,"content":"but today we're going to learn some extra information about"},{"from":130.28,"to":133.09,"location":2,"content":"that and some other types of decoding algorithms."},{"from":133.09,"to":135.71,"location":2,"content":"After that we're going to go through, um,"},{"from":135.71,"to":137.57,"location":2,"content":"a pretty quick tour of lots of"},{"from":137.57,"to":141.86,"location":2,"content":"different NLG tasks and a selection of neural approaches to them."},{"from":141.86,"to":145.58,"location":2,"content":"And then after that we're gonna talk about probably the biggest problem in NLG research,"},{"from":145.58,"to":149.94,"location":2,"content":"which is NLG evaluation and why it is such a tricky situation."},{"from":149.94,"to":153.66,"location":2,"content":"And then lastly, we're going to have some concluding thoughts on NLG research."},{"from":153.66,"to":158.26,"location":2,"content":"What are the current trends and where are we going in the future?"},{"from":158.26,"to":167.88,"location":2,"content":"Okay. So, uh, section one, let's do a recap."},{"from":167.88,"to":171.44,"location":2,"content":"Okay, so Natural Language Generation to define it just"},{"from":171.44,"to":175.09,"location":2,"content":"refers to any setting in which we are generating some kind of text."},{"from":175.09,"to":178.38,"location":2,"content":"So for example, NLG is an important sub-component"},{"from":178.38,"to":181.4,"location":2,"content":"of lots of different tasks such as, uh, machine translation,"},{"from":181.4,"to":184.37,"location":2,"content":"we've already met, uh, abstracted summarization,"},{"from":184.37,"to":186.31,"location":2,"content":"we'll learn a bit more about that later, um,"},{"from":186.31,"to":189.76,"location":2,"content":"dialogue both chit-chat and task-based."},{"from":189.76,"to":195.16,"location":2,"content":"Uh, also creative writing tasks such as writing stories and writing poems even."},{"from":195.16,"to":197.42,"location":2,"content":"NLG is also a sub-component of,"},{"from":197.42,"to":199.61,"location":2,"content":"uh, free-form question answering."},{"from":199.61,"to":202.28,"location":2,"content":"So I know a lot of you are doing the SQuAD project right now, uh,"},{"from":202.28,"to":206.03,"location":2,"content":"that is not an NLG task because you're just extracting the answer from the,"},{"from":206.03,"to":207.25,"location":2,"content":"uh, the source document."},{"from":207.25,"to":209.51,"location":2,"content":"But there are other question answering tasks"},{"from":209.51,"to":211.78,"location":2,"content":"that do have a Natural Language Generation component."},{"from":211.78,"to":215.87,"location":2,"content":"Uh, image captioning is another example of,"},{"from":215.87,"to":218.81,"location":2,"content":"uh, a task that has an NLG sub-component."},{"from":218.81,"to":223.63,"location":2,"content":"So NLG is a pretty cool component of a lot of different NLP tasks."},{"from":223.63,"to":225.44,"location":2,"content":"All right, let's go into our recap."},{"from":225.44,"to":227.45,"location":2,"content":"So the first thing I want to recap is,"},{"from":227.45,"to":228.72,"location":2,"content":"uh, what is language modeling?"},{"from":228.72,"to":233.3,"location":2,"content":"Um, I've noticed that some people are a little bit confused about this, I think it, uh,"},{"from":233.3,"to":236.3,"location":2,"content":"might be because the name language modeling sounds like it might mean"},{"from":236.3,"to":240.63,"location":2,"content":"just simply encoding language like representing language using embeddings or something."},{"from":240.63,"to":242.93,"location":2,"content":"So as a reminder language modeling,"},{"from":242.93,"to":244.13,"location":2,"content":"uh, has a more precise meaning."},{"from":244.13,"to":249.25,"location":2,"content":"Language modeling is the task of predicting the next word given the word so far."},{"from":249.25,"to":251.3,"location":2,"content":"So any system which produces"},{"from":251.3,"to":256.42,"location":2,"content":"this conditional probability distribution that does this task is called a Language Model."},{"from":256.42,"to":258.64,"location":2,"content":"And if that language model,"},{"from":258.64,"to":260.02,"location":2,"content":"uh, system is an RNN,"},{"from":260.02,"to":264.35,"location":2,"content":"then we often abbreviate it as RNN-Language Model."},{"from":264.35,"to":267.39,"location":2,"content":"Okay, so I hope, uh, you'll remember that."},{"from":267.39,"to":269.06,"location":2,"content":"Uh, the next thing we're going to recap is do you"},{"from":269.06,"to":271.06,"location":2,"content":"remember what a Conditional Language Model is?"},{"from":271.06,"to":275.04,"location":2,"content":"Uh, the task of Conditional Language Modeling is when you're predicting, uh,"},{"from":275.04,"to":277.91,"location":2,"content":"what word's going to come next but you're also conditioning on"},{"from":277.91,"to":281.58,"location":2,"content":"some other input x as well as all of your words so far."},{"from":281.58,"to":285.64,"location":2,"content":"So to recap some examples of conditional language modeling include, uh,"},{"from":285.64,"to":289.14,"location":2,"content":"machine translation where you're conditioning on the source sentence x,"},{"from":289.14,"to":292.94,"location":2,"content":"uh, summarization you're conditioning on your input text that you're trying to summarize."},{"from":292.94,"to":297.68,"location":2,"content":"Dialogue, you're conditioning on your dialogue history and so on."},{"from":297.68,"to":303.45,"location":2,"content":"Okay, uh, next we're going to quickly recap how do you train an RNN-Language model?"},{"from":303.45,"to":307.76,"location":2,"content":"I guess, it could also be a transformer-based language model or a CNN-based language model,"},{"from":307.76,"to":311.09,"location":2,"content":"now that you know about those, uh, and it could be conditional or it could be not."},{"from":311.09,"to":315.62,"location":2,"content":"So the main thing I want to remind you about is that when you are training the system,"},{"from":315.62,"to":318.41,"location":2,"content":"then you feed in the target sequence that you're trying to"},{"from":318.41,"to":321.62,"location":2,"content":"generate so where it says target sentence from corpus, uh,"},{"from":321.62,"to":324.29,"location":2,"content":"that's saying that you have some sequence that you're trying to"},{"from":324.29,"to":327.79,"location":2,"content":"generate and you feed that into the decoder, the RNN-Language model."},{"from":327.79,"to":331.38,"location":2,"content":"And then it predicts what words are going to come next."},{"from":331.38,"to":335.01,"location":2,"content":"So the super important thing is that during training,"},{"from":335.01,"to":339.43,"location":2,"content":"we're feeding the gold, that is the reference target sentence into the decoder,"},{"from":339.43,"to":341.98,"location":2,"content":"regardless of what the decoder is predicting."},{"from":341.98,"to":346.75,"location":2,"content":"So even if let's say this is a very bad decoder that isn't predicting the correct words,"},{"from":346.75,"to":348.55,"location":2,"content":"uh, it's not, you know, predicting them high at all,"},{"from":348.55,"to":351.79,"location":2,"content":"um, that doesn't matter we still just, um,"},{"from":351.79,"to":355.6,"location":2,"content":"input the targets- the gold target sequence into the decoder."},{"from":355.6,"to":358.66,"location":2,"content":"And, um, I'm emphasizing this because it's going to come up later,"},{"from":358.66,"to":360.89,"location":2,"content":"uh, this training method is called Teacher Forcing."},{"from":360.89,"to":363.17,"location":2,"content":"Which might be a phrase that you have come across elsewhere."},{"from":363.17,"to":365.98,"location":2,"content":"So, yeah, it refers to the fact that the teacher,"},{"from":365.98,"to":369.16,"location":2,"content":"that is kind of like the gold input is- is forcing, uh,"},{"from":369.16,"to":371.26,"location":2,"content":"the language model to use that on every step"},{"from":371.26,"to":374.5,"location":2,"content":"instead of using its own predictions on each step."},{"from":374.5,"to":378.63,"location":2,"content":"So that's how you train a RNN-Language model which might be conditional."},{"from":378.63,"to":380.61,"location":2,"content":"Uh, okay."},{"from":380.61,"to":382.81,"location":2,"content":"So now a recap on decoding algorithms."},{"from":382.81,"to":386.94,"location":2,"content":"So, uh, you've got your trained language model which might be conditional."},{"from":386.94,"to":389.35,"location":2,"content":"The question is how do you use it to generate a text?"},{"from":389.35,"to":392,"location":2,"content":"So the answer is you need a decoding algorithm."},{"from":392,"to":394.64,"location":2,"content":"A decoding algorithm is an algorithm you use to"},{"from":394.64,"to":397.58,"location":2,"content":"generate the text from your trained language model."},{"from":397.58,"to":399.68,"location":2,"content":"So, uh, in the NMT lecture"},{"from":399.68,"to":402.47,"location":2,"content":"a few weeks ago we learned about two different decoding algorithms."},{"from":402.47,"to":405.62,"location":2,"content":"We learned about greedy decoding and beam search."},{"from":405.62,"to":407.8,"location":2,"content":"So let's quickly recap those."},{"from":407.8,"to":411.21,"location":2,"content":"Uh, greedy decoding is a pretty simple algorithm."},{"from":411.21,"to":413.03,"location":2,"content":"On each step you just take what's"},{"from":413.03,"to":415.89,"location":2,"content":"the most probable words according to the language model."},{"from":415.89,"to":419.25,"location":2,"content":"You could deal with the argmax and then use that as the next word,"},{"from":419.25,"to":421.34,"location":2,"content":"you feed it in as the input on the next step."},{"from":421.34,"to":423.86,"location":2,"content":"And you just keep going until you produce some kind of END"},{"from":423.86,"to":426.62,"location":2,"content":"token or maybe when you reach some maximum length."},{"from":426.62,"to":429.81,"location":2,"content":"And I think you're all quite familiar with this because you did it in assignment five."},{"from":429.81,"to":435.22,"location":2,"content":"So uh, yes this diagram shows how greedy decoding would work to generate the sentence."},{"from":435.22,"to":437.42,"location":2,"content":"So as we learned before,"},{"from":437.42,"to":439.61,"location":2,"content":"due to a kind of lack of backtracking and"},{"from":439.61,"to":442.13,"location":2,"content":"inability to go back if you made a wrong choice, uh,"},{"from":442.13,"to":445.05,"location":2,"content":"the output from greedy decoding is generally, uh,"},{"from":445.05,"to":450.28,"location":2,"content":"pretty poor like it can be ungrammatical, or it can be unnatural, kind of nonsensical."},{"from":450.28,"to":453.27,"location":2,"content":"Okay, let's recap beam search decoding."},{"from":453.27,"to":458.66,"location":2,"content":"So beam search is a search algorithm which aims to find a high probability sequence."},{"from":458.66,"to":463.61,"location":2,"content":"So if we're doing translation that sequence is the sequence of translation words,"},{"from":463.61,"to":468.2,"location":2,"content":"um, by tracking multiple possible sequences at once."},{"from":468.2,"to":471.98,"location":2,"content":"So the core idea is that on each step of the decoder,"},{"from":471.98,"to":473.06,"location":2,"content":"you're going to be keeping track of"},{"from":473.06,"to":477.51,"location":2,"content":"the K most probable partial sequences which we call hypotheses."},{"from":477.51,"to":481,"location":2,"content":"And here K is some hyper- hyper parameter called the beam size."},{"from":481,"to":482.57,"location":2,"content":"So the idea is by um,"},{"from":482.57,"to":485.87,"location":2,"content":"considering lots of different hypotheses we're going to try to search effectively for"},{"from":485.87,"to":487.79,"location":2,"content":"a high probability sequence but there is"},{"from":487.79,"to":490.29,"location":2,"content":"no guarantee that this is going to be the optimal,"},{"from":490.29,"to":492.87,"location":2,"content":"most high probability sequence."},{"from":492.87,"to":495.93,"location":2,"content":"So, uh, at the end of beam search, uh,"},{"from":495.93,"to":497.93,"location":2,"content":"you reach some kind of stopping criterion which we talked"},{"from":497.93,"to":500.3,"location":2,"content":"about before but I won't cover in detail again."},{"from":500.3,"to":502.61,"location":2,"content":"Uh, and once you've reached your stopping criterion,"},{"from":502.61,"to":505.18,"location":2,"content":"you choose the sequence with the highest probability,"},{"from":505.18,"to":509.5,"location":2,"content":"um, factoring in some adjustments for length and then that's your output."},{"from":509.5,"to":511.4,"location":2,"content":"So just to do this one more time."},{"from":511.4,"to":515.43,"location":2,"content":"Here's the diagram that we saw in the NMT lecture of beam search decoding um,"},{"from":515.43,"to":520.02,"location":2,"content":"once it's completed and in this scenario we have a beam size of two."},{"from":520.02,"to":523.18,"location":2,"content":"So this is what it looks like after we've done this exploration problem,"},{"from":523.18,"to":525.42,"location":2,"content":"this shows the full tree that we explored,"},{"from":525.42,"to":529.14,"location":2,"content":"and then we've come to some kind of stopping criterion and we identify the top,"},{"from":529.14,"to":530.48,"location":2,"content":"uh, hypothesis and, uh,"},{"from":530.48,"to":532.51,"location":2,"content":"that's highlighted in green."},{"from":532.51,"to":535.82,"location":2,"content":"So on the subject of beam search decoding,"},{"from":535.82,"to":537.81,"location":2,"content":"I was watching TV the other day,"},{"from":537.81,"to":540.71,"location":2,"content":"and I notice something in Westworld."},{"from":540.71,"to":546.02,"location":2,"content":"I think the hosts- [LAUGHTER] the AI hosts in Westworld maybe used beam search."},{"from":546.02,"to":548.84,"location":2,"content":"Which is something I wasn't expecting to see on TV."},{"from":548.84,"to":550.95,"location":2,"content":"[LAUGHTER] So there's this scene,"},{"from":550.95,"to":552.39,"location":2,"content":"uh, Westworld is, by the way,"},{"from":552.39,"to":554.24,"location":2,"content":"a sci-fi series that has these, um,"},{"from":554.24,"to":556.58,"location":2,"content":"very convincing humanoid AI systems."},{"from":556.58,"to":559.04,"location":2,"content":"Um, and there's a scene where one of"},{"from":559.04,"to":562.05,"location":2,"content":"the AI systems is confronted with the reality of the fact that,"},{"from":562.05,"to":563.98,"location":2,"content":"um, she, I suppose is,"},{"from":563.98,"to":569.85,"location":2,"content":"um, not human because she sees the generation system of words as she says them,"},{"from":569.85,"to":571.68,"location":2,"content":"and I was looking at the TV and I thought,"},{"from":571.68,"to":572.77,"location":2,"content":"is that beam search?"},{"from":572.77,"to":575.84,"location":2,"content":"Because that diagram looks a lot like this diagram here,"},{"from":575.84,"to":578.58,"location":2,"content":"um, but maybe with a bigger beam size."},{"from":578.58,"to":580.61,"location":2,"content":"So, I thought, that was pretty cool because, you know,"},{"from":580.61,"to":583.49,"location":2,"content":"AI has hit the mainstream when you see beam search on TV."},{"from":583.49,"to":585.2,"location":2,"content":"And then if you zoom in really hard you can see"},{"from":585.2,"to":589.47,"location":2,"content":"some other exciting words in this screenshot like knowledge base,"},{"from":589.47,"to":591.17,"location":2,"content":"forward chaining and backward chaining,"},{"from":591.17,"to":594.2,"location":2,"content":"identifies the same thing as forward prop and backward prop,"},{"from":594.2,"to":597.18,"location":2,"content":"um, and also fuzzy logic algorithms and neural net."},{"from":597.18,"to":599.39,"location":2,"content":"Um, so yeah, beam search,"},{"from":599.39,"to":600.88,"location":2,"content":"I think, has hit the mainstream now,"},{"from":600.88,"to":604.1,"location":2,"content":"um, so it's good enough for Westworld,"},{"from":604.1,"to":605.06,"location":2,"content":"maybe it's good enough for us."},{"from":605.06,"to":608.5,"location":2,"content":"Uh, so with beam search, right?"},{"from":608.5,"to":612.05,"location":2,"content":"We've talked about how you have this hyperparameter k or the beam size."},{"from":612.05,"to":614.11,"location":2,"content":"And one thing we didn't talk about in the last lecture,"},{"from":614.11,"to":616.66,"location":2,"content":"so now we're leaving the recap portion, um,"},{"from":616.66,"to":620.98,"location":2,"content":"is what's the effect of changing that beam size k. So, uh,"},{"from":620.98,"to":622.48,"location":2,"content":"if you have a really small k,"},{"from":622.48,"to":626.07,"location":2,"content":"then you're gonna have similar problems to greedy decoding."},{"from":626.07,"to":627.37,"location":2,"content":"And in fact, if k equals one,"},{"from":627.37,"to":629.89,"location":2,"content":"then you are actually just doing greedy decoding."},{"from":629.89,"to":632.41,"location":2,"content":"So those same problems are, you know, ungrammatical,"},{"from":632.41,"to":636.8,"location":2,"content":"maybe unnatural, nonsensical, just kind of plain incorrect output."},{"from":636.8,"to":639.41,"location":2,"content":"So once if we get larger k,"},{"from":639.41,"to":641.3,"location":2,"content":"if you have a larger beam size,"},{"from":641.3,"to":646.75,"location":2,"content":"then you're doing your search algorithm but considering more hypotheses, right?"},{"from":646.75,"to":648.61,"location":2,"content":"You're, you're having a larger search space and"},{"from":648.61,"to":651.1,"location":2,"content":"you're considering more different possibilities."},{"from":651.1,"to":655.5,"location":2,"content":"So if you do that, then we often find that this reduces some of the problems above."},{"from":655.5,"to":658.54,"location":2,"content":"So you're much less likely to have this ungrammatical,"},{"from":658.54,"to":661.01,"location":2,"content":"uh, you know, disjointed output."},{"from":661.01,"to":664.93,"location":2,"content":"But there are some downsides to raising k. So of course,"},{"from":664.93,"to":666.97,"location":2,"content":"larger k is more computationally expensive"},{"from":666.97,"to":669.25,"location":2,"content":"and that can get pretty bad if you're trying to, um,"},{"from":669.25,"to":671.53,"location":2,"content":"for example, generate your, uh,"},{"from":671.53,"to":672.85,"location":2,"content":"outputs for a large, you know,"},{"from":672.85,"to":675.48,"location":2,"content":"test set of NMT examples."},{"from":675.48,"to":677.68,"location":2,"content":"Um, but more seriously than that,"},{"from":677.68,"to":679.87,"location":2,"content":"increasing k can introduce some other problems."},{"from":679.87,"to":683.25,"location":2,"content":"So for example, it's been shown that in NMT,"},{"from":683.25,"to":688.03,"location":2,"content":"increasing the beam size too much actually decreases the BLEU score."},{"from":688.03,"to":690.63,"location":2,"content":"And this is kind of counter-intuitive, right?"},{"from":690.63,"to":692.88,"location":2,"content":"Because we were thinking of beam search"},{"from":692.88,"to":695.41,"location":2,"content":"as this algorithm that tries to find the optimal solution."},{"from":695.41,"to":697.01,"location":2,"content":"So surely, if you increase k,"},{"from":697.01,"to":699.97,"location":2,"content":"then you're only going to find a better solution, right?"},{"from":699.97,"to":704.44,"location":2,"content":"Um, so I think maybe the key here is the difference between optimality"},{"from":704.44,"to":706.3,"location":2,"content":"in terms of the search problem that is finding"},{"from":706.3,"to":708.89,"location":2,"content":"a high probability sequence and BLEU score,"},{"from":708.89,"to":710.08,"location":2,"content":"which are two separate things,"},{"from":710.08,"to":714.31,"location":2,"content":"and there's no guarantee that they actually, um, correspond, right?"},{"from":714.31,"to":717.85,"location":2,"content":"And I mean, there's a difference, again, between BLEU score and actual translation,"},{"from":717.85,"to":719.44,"location":2,"content":"uh, quality as we know."},{"from":719.44,"to":721.72,"location":2,"content":"So if you look at the two papers which I've linked to"},{"from":721.72,"to":724.39,"location":2,"content":"here which are the ones that show that,"},{"from":724.39,"to":727.33,"location":2,"content":"uh, increasing beam size too much decreases the BLEU score."},{"from":727.33,"to":730.69,"location":2,"content":"They explain it by saying that the main reason why this"},{"from":730.69,"to":734.37,"location":2,"content":"happens is because when you increase the beam size too much,"},{"from":734.37,"to":738.37,"location":2,"content":"then you end up producing translations that are too short."},{"from":738.37,"to":742.72,"location":2,"content":"So I mean, that kind of explains it to a degree that translations are too short,"},{"from":742.72,"to":744.13,"location":2,"content":"therefore they have low BLEU because they're"},{"from":744.13,"to":746.23,"location":2,"content":"probably missing words that they should contain."},{"from":746.23,"to":749.86,"location":2,"content":"But the question is, why does large beam size gives you short translations?"},{"from":749.86,"to":751.21,"location":2,"content":"I think that's harder to answer."},{"from":751.21,"to":754.98,"location":2,"content":"Wherever, in these two papers, I didn't see an explicit explanation of why."},{"from":754.98,"to":757.92,"location":2,"content":"Um, I think it's possible larger kind of passing,"},{"from":757.92,"to":761.57,"location":2,"content":"we see sometimes with beam search which is when you really increase your, uh,"},{"from":761.57,"to":763.44,"location":2,"content":"search space and make the search much more"},{"from":763.44,"to":766.62,"location":2,"content":"powerful so that it can consider lots of different alternatives."},{"from":766.62,"to":769.62,"location":2,"content":"It can end up finding these high probability,"},{"from":769.62,"to":773.21,"location":2,"content":"um, sequences which aren't actually the thing that you want."},{"from":773.21,"to":775.26,"location":2,"content":"Sure, they're high probabili- probability"},{"from":775.26,"to":777.35,"location":2,"content":"but they're not actually the thing that you wanted."},{"from":777.35,"to":780.55,"location":2,"content":"Um, so another example of that is"},{"from":780.55,"to":783.63,"location":2,"content":"that in open-ended tasks like for example chit-chat dialogue"},{"from":783.63,"to":784.83,"location":2,"content":"where you're trying to just, um,"},{"from":784.83,"to":787.33,"location":2,"content":"say something interesting back to your conversational partner,"},{"from":787.33,"to":790.3,"location":2,"content":"if we use a beam search with a large beam size,"},{"from":790.3,"to":793.5,"location":2,"content":"we find that that can give you some output that is really generic."},{"from":793.5,"to":796.4,"location":2,"content":"Um, and I'll give you an example here to show you what I mean."},{"from":796.4,"to":800.54,"location":2,"content":"So these are examples from a chit-chat,"},{"from":800.54,"to":802.83,"location":2,"content":"uh, dialogue project that I was doing."},{"from":802.83,"to":804.19,"location":2,"content":"So here you've got, uh,"},{"from":804.19,"to":808.33,"location":2,"content":"your human chit-chat partner said something like I mostly eat a fresh and raw diet,"},{"from":808.33,"to":809.78,"location":2,"content":"so I save on groceries."},{"from":809.78,"to":814.03,"location":2,"content":"And then here's what the chat bot said back depending on the beam size."},{"from":814.03,"to":823.59,"location":2,"content":"I will let you read that."},{"from":823.59,"to":827.35,"location":2,"content":"So I would say that this is fairly characteristic of what you see"},{"from":827.35,"to":830.5,"location":2,"content":"happening when you raise and lower the beam size [NOISE]."},{"from":830.5,"to":831.96,"location":2,"content":"When you have a low beam size,"},{"from":831.96,"to":834.7,"location":2,"content":"um, it might be more kind of on topic."},{"from":834.7,"to":837.58,"location":2,"content":"Like here, we can see that eat healthy, eat healthy,"},{"from":837.58,"to":839.71,"location":2,"content":"I am a nurse so I do not eat raw food and so on,"},{"from":839.71,"to":842.34,"location":2,"content":"that kind of relates to what the user said,"},{"from":842.34,"to":844.15,"location":2,"content":"uh, but it's kind of bad English, right?"},{"from":844.15,"to":846.1,"location":2,"content":"There's some repetition and,"},{"from":846.1,"to":848.02,"location":2,"content":"uh, it doesn't always make that much sense, right?"},{"from":848.02,"to":849.58,"location":2,"content":"Um, [NOISE] but then,"},{"from":849.58,"to":850.88,"location":2,"content":"when you raise the beam size,"},{"from":850.88,"to":852.25,"location":2,"content":"then it kind of converges to"},{"from":852.25,"to":857.18,"location":2,"content":"a safe so-called correct response but it's kind of generic and less relevant, right?"},{"from":857.18,"to":859.6,"location":2,"content":"And it's kind of applicable in all scenarios, what do you do for a living."},{"from":859.6,"to":861.97,"location":2,"content":"Um, so the, the,"},{"from":861.97,"to":864.16,"location":2,"content":"the particular dataset I was using here is, uh,"},{"from":864.16,"to":865.24,"location":2,"content":"one called Persona-Chat,"},{"from":865.24,"to":866.44,"location":2,"content":"that I'll tell you more about later."},{"from":866.44,"to":868.24,"location":2,"content":"Um, but it's a,"},{"from":868.24,"to":871.32,"location":2,"content":"it's a chit-chat dialog dataset where each,"},{"from":871.32,"to":875.58,"location":2,"content":"uh, conv- conversational partner has a persona which is a set of traits."},{"from":875.58,"to":877.6,"location":2,"content":"Um, so the reason it keeps talking about being a nurse,"},{"from":877.6,"to":879.16,"location":2,"content":"I think is because it was in the persona."},{"from":879.16,"to":882.34,"location":2,"content":"[NOISE] But the main point here is that, um,"},{"from":882.34,"to":885.68,"location":2,"content":"we kind of have an unfortunate trade off with no,"},{"from":885.68,"to":888.8,"location":2,"content":"with no Goldilocks zone that's very obvious."},{"from":888.8,"to":890.41,"location":2,"content":"I mean, there's, there's a, yeah,"},{"from":890.41,"to":893.29,"location":2,"content":"kind of an unfortunate trade-off between having kind of bad,"},{"from":893.29,"to":896.68,"location":2,"content":"bad output, bad English and just having something very boring."},{"from":896.68,"to":901.32,"location":2,"content":"So this is one of the problems that we get with beam, beam search."},{"from":901.32,"to":903.79,"location":2,"content":"Okay. So we've talked about, uh,"},{"from":903.79,"to":906.45,"location":2,"content":"greedy decoding and beam search. Yes."},{"from":906.45,"to":913,"location":2,"content":"So beam size depending on the [inaudible]"},{"from":913,"to":914.05,"location":2,"content":"The question is, can we have"},{"from":914.05,"to":917.75,"location":2,"content":"an adaptive beam size dependent on the position that you're in?"},{"from":917.75,"to":919.06,"location":2,"content":"You mean like in the sequence?"},{"from":919.06,"to":926.04,"location":2,"content":"Yeah. That is in [inaudible]."},{"from":926.04,"to":929.22,"location":2,"content":"Yeah. I mean, I think I- I might have heard of a research paper that does that?"},{"from":929.22,"to":934.88,"location":2,"content":"That adaptively like raises the capacity of the, the hypothesis space."},{"from":934.88,"to":937.13,"location":2,"content":"I mean, it sounds awkward to implement, uh,"},{"from":937.13,"to":940.99,"location":2,"content":"because, you know, things fitting into a fixed space in your GPU."},{"from":940.99,"to":942.58,"location":2,"content":"Um, but I think that might be possible,"},{"from":942.58,"to":946.23,"location":2,"content":"I suppose you'd would have to learn the criterion on which you increase beam,"},{"from":946.23,"to":949.3,"location":2,"content":"beam size, yeah. Seems possible."},{"from":949.3,"to":951.63,"location":2,"content":"Okay. So we've talked about, uh,"},{"from":951.63,"to":953.37,"location":2,"content":"beam search and greedy decoding."},{"from":953.37,"to":955.99,"location":2,"content":"So here's a new family of decoding"},{"from":955.99,"to":959.1,"location":2,"content":"algorithms which are pretty simple, uh, sampling-based decoding."},{"from":959.1,"to":963.24,"location":2,"content":"So something which I'm calling pure sampling because I didn't know what else to call it."},{"from":963.24,"to":964.86,"location":2,"content":"Um, this is just the,"},{"from":964.86,"to":967.36,"location":2,"content":"the simple sampling method that says that on each, uh,"},{"from":967.36,"to":968.89,"location":2,"content":"timestep of your decoder t,"},{"from":968.89,"to":972.04,"location":2,"content":"you just want to randomly sample from the probability distribution,"},{"from":972.04,"to":973.78,"location":2,"content":"uh, to obtain your next word."},{"from":973.78,"to":975.49,"location":2,"content":"So this is very simple."},{"from":975.49,"to":977.34,"location":2,"content":"It's just like greedy decoding."},{"from":977.34,"to":979.28,"location":2,"content":"But instead of taking the top words,"},{"from":979.28,"to":982.35,"location":2,"content":"instead just sample from that distribution."},{"from":982.35,"to":988.6,"location":2,"content":"So the reason I call this pure sampling was to differentiate it from top-n sampling."},{"from":988.6,"to":990.64,"location":2,"content":"And again, this is actually usually called top-k"},{"from":990.64,"to":993.4,"location":2,"content":"sampling but I already called k the beam size,"},{"from":993.4,"to":996.34,"location":2,"content":"and I didn't want to be confusing, so I'm gonna call it top-n sampling for now."},{"from":996.34,"to":998.93,"location":2,"content":"Um, so the idea here is also pretty simple."},{"from":998.93,"to":1000.59,"location":2,"content":"On each step t,"},{"from":1000.59,"to":1004.03,"location":2,"content":"you want to randomly sample from your probability distribution but"},{"from":1004.03,"to":1008.26,"location":2,"content":"you're gonna restrict to just the top n most probable words."},{"from":1008.26,"to":1010.18,"location":2,"content":"So this is saying that it's,"},{"from":1010.18,"to":1011.43,"location":2,"content":"it's like the simple, you know,"},{"from":1011.43,"to":1016.51,"location":2,"content":"pure sampling method but you want to truncate your probability distribution just to be,"},{"from":1016.51,"to":1019.02,"location":2,"content":"you know, the, the top most probable words."},{"from":1019.02,"to":1023.14,"location":2,"content":"So, uh, the idea here kind of like how beam search, um,"},{"from":1023.14,"to":1026.61,"location":2,"content":"gave you a hyperparameter is kind of go between greedy decoding and,"},{"from":1026.61,"to":1028.93,"location":2,"content":"you know, uh, a very exhaustive search."},{"from":1028.93,"to":1032.03,"location":2,"content":"In the same way here, you've got a hyperparameter n"},{"from":1032.03,"to":1035.34,"location":2,"content":"which can take you between greedy search and pure sampling."},{"from":1035.34,"to":1036.63,"location":2,"content":"If you think about this for a moment,"},{"from":1036.63,"to":1039.15,"location":2,"content":"if n is one, then you would truncate it the top one."},{"from":1039.15,"to":1041.09,"location":2,"content":"So you're just taking arg max which is greedy."},{"from":1041.09,"to":1042.66,"location":2,"content":"And if n is vocab size,"},{"from":1042.66,"to":1044.09,"location":2,"content":"then you don't truncate it at all."},{"from":1044.09,"to":1045.51,"location":2,"content":"You're sampling from everything,"},{"from":1045.51,"to":1047.79,"location":2,"content":"that's just the pure sampling method."},{"from":1047.79,"to":1051,"location":2,"content":"So here, um, it should be clear, I hope,"},{"from":1051,"to":1053.71,"location":2,"content":"if you think about that if you increase n,"},{"from":1053.71,"to":1056.91,"location":2,"content":"then you're gonna get more diverse and risky output, right?"},{"from":1056.91,"to":1059.23,"location":2,"content":"Because you're, uh, giving it more,"},{"from":1059.23,"to":1062.76,"location":2,"content":"more to choose from and you're going lower into the probability distribution,"},{"from":1062.76,"to":1064.77,"location":2,"content":"going lower into less likely things."},{"from":1064.77,"to":1066.27,"location":2,"content":"And then, if you decrease n,"},{"from":1066.27,"to":1068.58,"location":2,"content":"then you're gonna get more kind of generic safe output because you're"},{"from":1068.58,"to":1073.46,"location":2,"content":"restricting more to the most high probability options."},{"from":1073.46,"to":1076.44,"location":2,"content":"So both of these are more efficient than"},{"from":1076.44,"to":1078.63,"location":2,"content":"beam search which I think is something important to note,"},{"from":1078.63,"to":1082.42,"location":2,"content":"uh, because there are no multiple hypotheses to track, right?"},{"from":1082.42,"to":1084.73,"location":2,"content":"Because in beam search, on every step t of the decoder,"},{"from":1084.73,"to":1086.12,"location":2,"content":"you've got k different, you know,"},{"from":1086.12,"to":1088.77,"location":2,"content":"beam size, many hypotheses to track."},{"from":1088.77,"to":1091.56,"location":2,"content":"Uh, whereas here, at least if you're only generating one sample,"},{"from":1091.56,"to":1092.76,"location":2,"content":"there's only one thing to track."},{"from":1092.76,"to":1094.44,"location":2,"content":"So it, it's a very simple algorithm."},{"from":1094.44,"to":1101.2,"location":2,"content":"So that is one advantage of these sampling-based algorithms over beam search."},{"from":1101.2,"to":1105.56,"location":2,"content":"Okay. So, the last thing I want to tell you that's kind of related to decoding is,"},{"from":1105.56,"to":1107.16,"location":2,"content":"uh, softmax [NOISE] temperature."},{"from":1107.16,"to":1110.93,"location":2,"content":"So, if you recall on timestep t of your decoder,"},{"from":1110.93,"to":1114.59,"location":2,"content":"your language model computes some kind of probability distribution P_t, uh,"},{"from":1114.59,"to":1119.03,"location":2,"content":"by applying the softmax function to a vector of scores that you got from somewhere."},{"from":1119.03,"to":1122.73,"location":2,"content":"Like from your transformer or from your RNN or something."},{"from":1122.73,"to":1124.67,"location":2,"content":"So, there's the softmax function again."},{"from":1124.67,"to":1127.58,"location":2,"content":"It's saying that the probability of a word W is this softmax function,"},{"from":1127.58,"to":1130.12,"location":2,"content":"uh, given, given the scores."},{"from":1130.12,"to":1135.08,"location":2,"content":"So, the idea here of a temperature on the softmax is that you have some kind of"},{"from":1135.08,"to":1141.2,"location":2,"content":"temperature hyperparameter tau and you're going to apply that to this, uh, softmax."},{"from":1141.2,"to":1144.92,"location":2,"content":"So, all that we're doing is we're div- dividing all of the scores,"},{"from":1144.92,"to":1146.38,"location":2,"content":"or logits you might call them,"},{"from":1146.38,"to":1148.57,"location":2,"content":"by the temperature hyperparameter."},{"from":1148.57,"to":1150.55,"location":2,"content":"So again, if you just think about this a little bit,"},{"from":1150.55,"to":1152.57,"location":2,"content":"you'll see that raising the temperature,"},{"from":1152.57,"to":1153.8,"location":2,"content":"that is increasing, uh,"},{"from":1153.8,"to":1159.93,"location":2,"content":"the hyperparameter, this is going to make your probability distribution more uniform."},{"from":1159.93,"to":1163.41,"location":2,"content":"And this kind of comes down to the question about when you,"},{"from":1163.41,"to":1165.83,"location":2,"content":"when you multiply all of your scores by a constant,"},{"from":1165.83,"to":1168.98,"location":2,"content":"um, how does that affect the softmax, right?"},{"from":1168.98,"to":1173.88,"location":2,"content":"So, do things get more far apart or less far apart once you take the exponential?"},{"from":1173.88,"to":1176.69,"location":2,"content":"So, this is something you can just work up by yourself on paper,"},{"from":1176.69,"to":1178.52,"location":2,"content":"but as a, uh,"},{"from":1178.52,"to":1180.13,"location":2,"content":"a kind of a memory shortcut,"},{"from":1180.13,"to":1183.5,"location":2,"content":"a good way to think about it is that if you raise the temperature,"},{"from":1183.5,"to":1187.49,"location":2,"content":"then the distribution kind of melts and goes soft and mushy and uniform."},{"from":1187.49,"to":1188.81,"location":2,"content":"And if you, uh,"},{"from":1188.81,"to":1191.15,"location":2,"content":"lower the temperature, like make it cold then,"},{"from":1191.15,"to":1194.69,"location":2,"content":"the probability distribution becomes more spiky, right?"},{"from":1194.69,"to":1199.12,"location":2,"content":"So, like the things which are rated as high probability become like even more,"},{"from":1199.12,"to":1202.67,"location":2,"content":"uh, disproportionately high probability compared to the other things."},{"from":1202.67,"to":1205.54,"location":2,"content":"Um, I think that's a easy way to remember it."},{"from":1205.54,"to":1207.93,"location":2,"content":"Today I had to work it out on paper and then, uh,"},{"from":1207.93,"to":1209.13,"location":2,"content":"I realized that just the, the,"},{"from":1209.13,"to":1212.38,"location":2,"content":"the temperature visualization thing usually gets me there quicker."},{"from":1212.38,"to":1218.48,"location":2,"content":"So, um, one thing I want to note is that softmax temperature is not a decoding algorithm."},{"from":1218.48,"to":1221.12,"location":2,"content":"I know that I put it in the decoding algorithm section,"},{"from":1221.12,"to":1223.71,"location":2,"content":"uh, that was just because it's kind of a thing, a"},{"from":1223.71,"to":1229.88,"location":2,"content":"simple thing that you can do at test time to change how the decoding happens, right?"},{"from":1229.88,"to":1231.32,"location":2,"content":"You don't need to train, uh,"},{"from":1231.32,"to":1233.77,"location":2,"content":"with the, the softmax temperature."},{"from":1233.77,"to":1236.22,"location":2,"content":"So, it's not a decoding algorithm itself."},{"from":1236.22,"to":1238.41,"location":2,"content":"It's a technique that you can apply at test time"},{"from":1238.41,"to":1241.04,"location":2,"content":"in conjunction with a decoding algorithm."},{"from":1241.04,"to":1244.38,"location":2,"content":"So, for example, if you're doing beam search or you're doing some kind of sampling,"},{"from":1244.38,"to":1248.62,"location":2,"content":"then you can also apply a softmax temperature, um, to change,"},{"from":1248.62,"to":1255.22,"location":2,"content":"you know, this kind of risky versus safe, um, trade-off."},{"from":1255.22,"to":1263.06,"location":2,"content":"Any questions on this? Okay. So, here's"},{"from":1263.06,"to":1266.27,"location":2,"content":"a summary of what we just learned about decoding algorithms."},{"from":1266.27,"to":1269.38,"location":2,"content":"Um, Greedy decoding is a simple method."},{"from":1269.38,"to":1274.27,"location":2,"content":"It gives kind of low quality output in comparison to the others, at least beam search."},{"from":1274.27,"to":1277.16,"location":2,"content":"Beam search, especially when you've got a high beam size, uh,"},{"from":1277.16,"to":1280.95,"location":2,"content":"it searches through lots of different hypotheses for high-probability outputs."},{"from":1280.95,"to":1284.12,"location":2,"content":"And this generally is gonna deliver better quality than greedy search, uh,"},{"from":1284.12,"to":1286.73,"location":2,"content":"but if the beam size is too high, then you can have these,"},{"from":1286.73,"to":1289.38,"location":2,"content":"uh, kind of counter-intuitive problems we talked about before."},{"from":1289.38,"to":1292.87,"location":2,"content":"Where you've retrieved some kind of high-probability but unsuitable output."},{"from":1292.87,"to":1295.41,"location":2,"content":"Say, something is too generic or something is too short."},{"from":1295.41,"to":1297.29,"location":2,"content":"And we're gonna talk about that more later."},{"from":1297.29,"to":1301.22,"location":2,"content":"Uh, sampling methods are a way to get more diversity,"},{"from":1301.22,"to":1303.1,"location":2,"content":"uh, via, via randomness."},{"from":1303.1,"to":1306.38,"location":2,"content":"Uh, well, getting randomness might be your goal in itself."},{"from":1306.38,"to":1309.48,"location":2,"content":"Um, so, this is good if you want to have some kind of, for example,"},{"from":1309.48,"to":1311.93,"location":2,"content":"open-ended or creative generation setting like,"},{"from":1311.93,"to":1313.91,"location":2,"content":"uh, generating poetry or stories,"},{"from":1313.91,"to":1316.37,"location":2,"content":"then sampling is probably a better idea than"},{"from":1316.37,"to":1319.7,"location":2,"content":"beam search because you want to have a kind of source of randomness to,"},{"from":1319.7,"to":1322.16,"location":2,"content":"uh, write different things creatively."},{"from":1322.16,"to":1327.17,"location":2,"content":"And top-n sampling allows you to control the diversity by,"},{"from":1327.17,"to":1329.33,"location":2,"content":"uh, changing n. And then lastly,"},{"from":1329.33,"to":1331.61,"location":2,"content":"softmax temperature is another way to control diversity."},{"from":1331.61,"to":1334.52,"location":2,"content":"So there's quite a few different knobs you can turn here."},{"from":1334.52,"to":1336.26,"location":2,"content":"And it's not a decoding algorithm,"},{"from":1336.26,"to":1340.19,"location":2,"content":"it's just a technique that you can apply alongside any decoding algorithm."},{"from":1340.19,"to":1342.83,"location":2,"content":"Although it wouldn't make sense to apply it with"},{"from":1342.83,"to":1346.37,"location":2,"content":"greedy decoding because even if you make it more spiky or more flat,"},{"from":1346.37,"to":1351.4,"location":2,"content":"the argmax is still the argmax, so it doesn't make sense."},{"from":1351.4,"to":1354.35,"location":2,"content":"Okay. Cool. I'm going to move on to section two."},{"from":1354.35,"to":1359.13,"location":2,"content":"So, uh, section two is NLG tasks and neural approaches to them."},{"from":1359.13,"to":1362.33,"location":2,"content":"Uh, as mentioned before, this is not going to be an overview of all of NLG."},{"from":1362.33,"to":1363.62,"location":2,"content":"That will be quite impossible."},{"from":1363.62,"to":1365.19,"location":2,"content":"This is gonna be some selected highlights."},{"from":1365.19,"to":1367.49,"location":2,"content":"So, in particular, I'm gonna start off with"},{"from":1367.49,"to":1371.27,"location":2,"content":"a fairly deep dive into a particular NLG task that I'm a bit more familiar with,"},{"from":1371.27,"to":1373.25,"location":2,"content":"and that is, uh, summarization."},{"from":1373.25,"to":1377.66,"location":2,"content":"So, let's start off with a task definition for summarization."},{"from":1377.66,"to":1382.33,"location":2,"content":"Um, one sensible definition would be: Given some kind of input text x,"},{"from":1382.33,"to":1384.89,"location":2,"content":"you want to write a summary y which is shorter than"},{"from":1384.89,"to":1387.83,"location":2,"content":"x and contains the main information of x."},{"from":1387.83,"to":1391.36,"location":2,"content":"So, summarization can be single-document or multi-document."},{"from":1391.36,"to":1396.51,"location":2,"content":"Uh, single-document means that you just have a summary y of a single document x."},{"from":1396.51,"to":1400.04,"location":2,"content":"In multi-document summarization, you're saying that you want to write"},{"from":1400.04,"to":1404.39,"location":2,"content":"a single summary y of multiple documents x_1 up to x_n."},{"from":1404.39,"to":1408.98,"location":2,"content":"And here typically x_1 up to x_n will have some kind of overlapping content."},{"from":1408.98,"to":1412.04,"location":2,"content":"So, for example, they might all be different news articles"},{"from":1412.04,"to":1415.22,"location":2,"content":"from different newspapers about the same event, right?"},{"from":1415.22,"to":1419.03,"location":2,"content":"Because it kind of makes sense to write a single summary that draws from all of those."},{"from":1419.03,"to":1425.92,"location":2,"content":"Um, makes less sense to summarize things that are about different topics."},{"from":1425.92,"to":1428.02,"location":2,"content":"There is further, uh,"},{"from":1428.02,"to":1431.27,"location":2,"content":"subdivision of, uh, task definitions in, in summarization."},{"from":1431.27,"to":1433.84,"location":2,"content":"So, I'm gonna describe it via some datasets."},{"from":1433.84,"to":1438.45,"location":2,"content":"Uh, here are some different really common datasets especially in, uh,"},{"from":1438.45,"to":1441.8,"location":2,"content":"neural summarization, um, and they kind of correspond to different,"},{"from":1441.8,"to":1444.04,"location":2,"content":"like, lengths and different styles of text."},{"from":1444.04,"to":1445.43,"location":2,"content":"So, a common one is,"},{"from":1445.43,"to":1447.05,"location":2,"content":"uh, the Gigaword dataset."},{"from":1447.05,"to":1449.36,"location":2,"content":"And the task here is that you want to map from"},{"from":1449.36,"to":1453.71,"location":2,"content":"the first one or two sentences of a news article to write the headline."},{"from":1453.71,"to":1456.29,"location":2,"content":"[NOISE] And you could think of this as sentence compression,"},{"from":1456.29,"to":1459.14,"location":2,"content":"especially if it's kind of one sentence to headline because you're going from"},{"from":1459.14,"to":1462.71,"location":2,"content":"a longish sentence to a shortish headline style sentence."},{"from":1462.71,"to":1466.95,"location":2,"content":"Uh, next one that I, um,"},{"from":1466.95,"to":1469.13,"location":2,"content":"wanted to tell you about is this, uh,"},{"from":1469.13,"to":1471.32,"location":2,"content":"it's a Chinese summarization dataset but I,"},{"from":1471.32,"to":1473.69,"location":2,"content":"I see people using it a lot."},{"from":1473.69,"to":1476.48,"location":2,"content":"And it's, uh, from a micro-blogging,"},{"from":1476.48,"to":1479.94,"location":2,"content":"um, website where people write summaries of their posts."},{"from":1479.94,"to":1482.27,"location":2,"content":"So, the actual summarization task is"},{"from":1482.27,"to":1484.79,"location":2,"content":"you've got some paragraph of text and then you want to,"},{"from":1484.79,"to":1486.23,"location":2,"content":"uh, summarize that into,"},{"from":1486.23,"to":1488.18,"location":2,"content":"I think, a single sentence summary."},{"from":1488.18,"to":1491.12,"location":2,"content":"Uh, another one, uh, two actually,"},{"from":1491.12,"to":1495.65,"location":2,"content":"are the New York Times and CNN/Daily Mail, uh, datasets."},{"from":1495.65,"to":1497.18,"location":2,"content":"So, these ones are both of the form,"},{"from":1497.18,"to":1499.94,"location":2,"content":"you've got a whole news article which is actually pretty long like"},{"from":1499.94,"to":1503.69,"location":2,"content":"hun-hundreds of words and then you want to summarize that into,"},{"from":1503.69,"to":1506.84,"location":2,"content":"uh, like, maybe a single-sentence or multi-sentence summary."},{"from":1506.84,"to":1510.56,"location":2,"content":"Uh, The New York Times ones are written by, I think, uh,"},{"from":1510.56,"to":1513.13,"location":2,"content":"librarians or people who, who,"},{"from":1513.13,"to":1516.44,"location":2,"content":"um, write summaries for, for library purposes."},{"from":1516.44,"to":1518.88,"location":2,"content":"Uh, and then, uh,"},{"from":1518.88,"to":1522.37,"location":2,"content":"one I just spotted today when I was writing this list is there's a new,"},{"from":1522.37,"to":1525.85,"location":2,"content":"fairly new like last six months dataset from wikiHow."},{"from":1525.85,"to":1527.84,"location":2,"content":"So, from what I can tell this seems to be,"},{"from":1527.84,"to":1531.95,"location":2,"content":"you've got a full how-to-article from wikiHow and then you want to boil this down to"},{"from":1531.95,"to":1534.2,"location":2,"content":"the summary sentences which are kind of cleverly"},{"from":1534.2,"to":1537.18,"location":2,"content":"extracted from throughout the wikiHow article."},{"from":1537.18,"to":1538.79,"location":2,"content":"They are kind of like headings."},{"from":1538.79,"to":1542.39,"location":2,"content":"So, um, I looked at this paper and it seems that, um,"},{"from":1542.39,"to":1545.84,"location":2,"content":"this is kind of interesting because it's a different type of text."},{"from":1545.84,"to":1548.99,"location":2,"content":"As you might have noticed most of the other ones are news-based and this is,"},{"from":1548.99,"to":1551.83,"location":2,"content":"uh, not, so that kind of poses different challenges."},{"from":1551.83,"to":1557.36,"location":2,"content":"Uh, another kind of division of summarization is sentence simplification."},{"from":1557.36,"to":1560.69,"location":2,"content":"So, this is a related but actually different task."},{"from":1560.69,"to":1564.41,"location":2,"content":"In summarization, you want to write something which is shorter and contains"},{"from":1564.41,"to":1568.22,"location":2,"content":"main information but is still maybe written in just as complex language,"},{"from":1568.22,"to":1573.79,"location":2,"content":"whereas in sentence simplification you want to rewrite the source text using simpler,"},{"from":1573.79,"to":1575.42,"location":2,"content":"uh, simpler language, right?"},{"from":1575.42,"to":1578.77,"location":2,"content":"So, like simpler word choices and simpler sentence structure."},{"from":1578.77,"to":1581.24,"location":2,"content":"That might mean it's shorter but not necessarily."},{"from":1581.24,"to":1582.89,"location":2,"content":"So, for example, uh,"},{"from":1582.89,"to":1585.95,"location":2,"content":"simple Wiki- Wikipedia is a standard dataset for this."},{"from":1585.95,"to":1588.47,"location":2,"content":"And the idea is you've got, um, you know,"},{"from":1588.47,"to":1591.44,"location":2,"content":"standard Wikipedia and you've got a simple Wikipedia version."},{"from":1591.44,"to":1592.55,"location":2,"content":"And they mostly align up,"},{"from":1592.55,"to":1593.96,"location":2,"content":"so you want to map from"},{"from":1593.96,"to":1597.37,"location":2,"content":"some sentence in one to the equivalent sentence in the [NOISE] other."},{"from":1597.37,"to":1601.88,"location":2,"content":"Another source of data for this is Newsela which is a website that,"},{"from":1601.88,"to":1604.09,"location":2,"content":"uh, rewrites news for children."},{"from":1604.09,"to":1606.32,"location":2,"content":"Actually, at different learning levels I think."},{"from":1606.32,"to":1610.18,"location":2,"content":"So, you have multiple options for how much it's simplified."},{"from":1610.18,"to":1614.69,"location":2,"content":"Okay. So, um, so"},{"from":1614.69,"to":1619.09,"location":2,"content":"that's the definition or the many definitions of summarization as different tasks."},{"from":1619.09,"to":1620.81,"location":2,"content":"So, now I'm gonna give an overview of, like,"},{"from":1620.81,"to":1622.19,"location":2,"content":"what are the main, uh,"},{"from":1622.19,"to":1624.1,"location":2,"content":"techniques for doing summarization."},{"from":1624.1,"to":1626.39,"location":2,"content":"So, there's two main strategies for summarization."},{"from":1626.39,"to":1630.56,"location":2,"content":"Uh, you can call them extractive summarization and abstractive summarization."},{"from":1630.56,"to":1632.73,"location":2,"content":"And the main idea as I had hinted out earlier,"},{"from":1632.73,"to":1635.72,"location":2,"content":"is that in extractive summarization you're just selecting"},{"from":1635.72,"to":1639.05,"location":2,"content":"parts of the original texts to form a summary."},{"from":1639.05,"to":1642.77,"location":2,"content":"And often this will be whole sentences but maybe it'll be more granular than that;"},{"from":1642.77,"to":1644.83,"location":2,"content":"maybe, uh, phrases or words."},{"from":1644.83,"to":1647.36,"location":2,"content":"Whereas abstractive summarization, you're going to be"},{"from":1647.36,"to":1651.28,"location":2,"content":"generating some new text using NLG techniques."},{"from":1651.28,"to":1653.84,"location":2,"content":"So the idea is that it's, you know, generation from scratch."},{"from":1653.84,"to":1657.59,"location":2,"content":"And my visual metaphor for this is this kind of like the difference between highlighting"},{"from":1657.59,"to":1663.1,"location":2,"content":"the parts with a highlighter or writing the summary yourself with a pen."},{"from":1663.1,"to":1667.16,"location":2,"content":"I think the high level things to know about these two techniques are that"},{"from":1667.16,"to":1670.61,"location":2,"content":"extractive summarization is basically easier,"},{"from":1670.61,"to":1672.72,"location":2,"content":"at least to make a decent system to start,"},{"from":1672.72,"to":1677.12,"location":2,"content":"because selecting things is probably easier than writing text from scratch."},{"from":1677.12,"to":1680.94,"location":2,"content":"Um, but extractive summarization is pretty restrictive, right?"},{"from":1680.94,"to":1682.76,"location":2,"content":"Because you can't really paraphrase anything,"},{"from":1682.76,"to":1685.43,"location":2,"content":"you can't really do any powerful sentence compression"},{"from":1685.43,"to":1688.47,"location":2,"content":"if you can only just select sentences."},{"from":1688.47,"to":1692.19,"location":2,"content":"Um, and, of course, abstractive summarization as a paradigm"},{"from":1692.19,"to":1695.64,"location":2,"content":"is more flexible and it's more how humans might summarize,"},{"from":1695.64,"to":1698.15,"location":2,"content":"uh, but as noted it's pretty difficult."},{"from":1698.15,"to":1703.84,"location":2,"content":"So, I'm gonna give you a very quick view of what pre-neural summarization looks like."},{"from":1703.84,"to":1704.94,"location":2,"content":"And here we've got, uh,"},{"from":1704.94,"to":1706.7,"location":2,"content":"this is a diagram from the, uh,"},{"from":1706.7,"to":1708.8,"location":2,"content":"Speech and Language Processing book."},{"from":1708.8,"to":1713.12,"location":2,"content":"So, uh, pre-neural summarization systems were mostly extractive."},{"from":1713.12,"to":1715.37,"location":2,"content":"And like pre-neural NMT,"},{"from":1715.37,"to":1717.13,"location":2,"content":"which we learnt about in the NMT lecture,"},{"from":1717.13,"to":1720.39,"location":2,"content":"it typically had a pipeline which is what this picture is showing."},{"from":1720.39,"to":1723.07,"location":2,"content":"So, a typical pipeline might have three parts."},{"from":1723.07,"to":1726.17,"location":2,"content":"First, you have content selection which is, uh,"},{"from":1726.17,"to":1729.79,"location":2,"content":"essentially choosing some of the sentences from the source document to include."},{"from":1729.79,"to":1732.15,"location":2,"content":"And then secondly, you're going to do some kind of information"},{"from":1732.15,"to":1736.05,"location":2,"content":"ordering which means choosing what order should I put these sentences in."},{"from":1736.05,"to":1739.75,"location":2,"content":"And this is particularly a more nontrivial question if you were"},{"from":1739.75,"to":1741.58,"location":2,"content":"doing multiple document summarization"},{"from":1741.58,"to":1743.56,"location":2,"content":"because your sentences might come from different documents."},{"from":1743.56,"to":1745.06,"location":2,"content":"Uh, and then lastly,"},{"from":1745.06,"to":1748.26,"location":2,"content":"you're going to do a sentence realization that is actually, um,"},{"from":1748.26,"to":1752.13,"location":2,"content":"turning your selected sentences into your actual summary."},{"from":1752.13,"to":1753.68,"location":2,"content":"So, although we're not doing, kind of,"},{"from":1753.68,"to":1755.83,"location":2,"content":"free-form text generation, uh,"},{"from":1755.83,"to":1759.29,"location":2,"content":"there might be some kind of editing for example like, uh, simplifying, editing,"},{"from":1759.29,"to":1761.88,"location":2,"content":"or removing parts that are redundant,"},{"from":1761.88,"to":1763.87,"location":2,"content":"or fixing continuity issues."},{"from":1763.87,"to":1766.22,"location":2,"content":"So for example, you can't refer to"},{"from":1766.22,"to":1768.92,"location":2,"content":"a person as she if you never introduced them in the first place."},{"from":1768.92,"to":1773.18,"location":2,"content":"So maybe you need to change that she to the name of the person."},{"from":1773.18,"to":1775.89,"location":2,"content":"So in particular [NOISE] uh,"},{"from":1775.89,"to":1777.94,"location":2,"content":"these pre-neural summarization systems, uh,"},{"from":1777.94,"to":1781.23,"location":2,"content":"have some pretty sophisticated algorithms of content selection."},{"from":1781.23,"to":1783.45,"location":2,"content":"Um, so, for example,"},{"from":1783.45,"to":1786.24,"location":2,"content":"uh, you would have some sentence scoring functions."},{"from":1786.24,"to":1788.14,"location":2,"content":"This is the most simple, uh, way you might do it,"},{"from":1788.14,"to":1790.77,"location":2,"content":"is you might score all of the sentences individually"},{"from":1790.77,"to":1793.62,"location":2,"content":"and you could score them based on features such as,"},{"from":1793.62,"to":1796.65,"location":2,"content":"um, are there, you know, topic keywords in the sentence?"},{"from":1796.65,"to":1799.38,"location":2,"content":"If so, maybe it's an important sentence that we should include."},{"from":1799.38,"to":1802.72,"location":2,"content":"Um, and you could compute those, uh,"},{"from":1802.72,"to":1806.76,"location":2,"content":"keywords using, uh, statistics such as tf-idf for example."},{"from":1806.76,"to":1810.96,"location":2,"content":"[NOISE] You can also use pretty basic but powerful features such as,"},{"from":1810.96,"to":1812.92,"location":2,"content":"uh, where does the sentence appear in the document?"},{"from":1812.92,"to":1814.26,"location":2,"content":"If it's near the top of the document,"},{"from":1814.26,"to":1816.51,"location":2,"content":"then it's more likely to be important."},{"from":1816.51,"to":1818.1,"location":2,"content":"Uh, there are also"},{"from":1818.1,"to":1821.91,"location":2,"content":"some more complex content selection algorithms such as for example, uh,"},{"from":1821.91,"to":1825.42,"location":2,"content":"there are these graph-based algorithms which kind of view the document as"},{"from":1825.42,"to":1829.01,"location":2,"content":"a set of sentences and those sentences are the nodes of the graph,"},{"from":1829.01,"to":1830.76,"location":2,"content":"and you imagine that all sentences, er,"},{"from":1830.76,"to":1833.19,"location":2,"content":"sentence pairs have an edge between them,"},{"from":1833.19,"to":1836.76,"location":2,"content":"and the weight of the edge is kind of how similar the sentences are."},{"from":1836.76,"to":1839.92,"location":2,"content":"So, then, if you think about the graph in that sense,"},{"from":1839.92,"to":1843.6,"location":2,"content":"then now you can try to identify which sentences are"},{"from":1843.6,"to":1847.5,"location":2,"content":"important by finding which sentences are central in the graph."},{"from":1847.5,"to":1849.54,"location":2,"content":"So you can apply some kind of general purpose"},{"from":1849.54,"to":1852.93,"location":2,"content":"gla- graph algorithms to figure out which [NOISE] nodes are central,"},{"from":1852.93,"to":1856.34,"location":2,"content":"and this is a way to find central sentences."},{"from":1856.34,"to":1863.36,"location":2,"content":"Okay. So um, [NOISE] back to summarization as a task."},{"from":1863.36,"to":1866.94,"location":2,"content":"Um, we've, I can't remember if we've talked about ROUGE already."},{"from":1866.94,"to":1868.14,"location":2,"content":"We've certainly talked about BLEU."},{"from":1868.14,"to":1869.82,"location":2,"content":"But I'm gonna tell you about ROUGE now which is"},{"from":1869.82,"to":1872.4,"location":2,"content":"the main automatic metric for summarization."},{"from":1872.4,"to":1877.69,"location":2,"content":"So ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation."},{"from":1877.69,"to":1879.48,"location":2,"content":"I'm not sure if that was the first thing they came up with"},{"from":1879.48,"to":1881.79,"location":2,"content":"or if they made it like that to match BLEU."},{"from":1881.79,"to":1884.61,"location":2,"content":"Um, and here's the,"},{"from":1884.61,"to":1886.05,"location":2,"content":"here's the equation, uh,"},{"from":1886.05,"to":1888.86,"location":2,"content":"for, well, I suppose one of the ROUGE metrics."},{"from":1888.86,"to":1891.21,"location":2,"content":"I'll tell you more about what that means later and you can"},{"from":1891.21,"to":1894.11,"location":2,"content":"read more in the original paper which is linked at the bottom."},{"from":1894.11,"to":1898.1,"location":2,"content":"So, uh, the overall idea is that ROUGE is actually pretty similar to BLEU."},{"from":1898.1,"to":1900.02,"location":2,"content":"It's based on n-gram overlap."},{"from":1900.02,"to":1905.65,"location":2,"content":"So, some main differences with BLEU are ROUGE doesn't have a brevity penalty."},{"from":1905.65,"to":1907.23,"location":2,"content":"Um, I'll talk more about that in a minute."},{"from":1907.23,"to":1912.19,"location":2,"content":"Uh, the other big one is that ROUGE is based on recall while BLEU is based on precision."},{"from":1912.19,"to":1913.44,"location":2,"content":"So you can see it's there in the title."},{"from":1913.44,"to":1917.12,"location":2,"content":"[NOISE] Um, so, if you think about this a little bit,"},{"from":1917.12,"to":1922.24,"location":2,"content":"I think you can say arguably precision is more important for machine translation."},{"from":1922.24,"to":1929.13,"location":2,"content":"That is, you only want to generate text that appears in one of your reference, uh,"},{"from":1929.13,"to":1932.52,"location":2,"content":"translations, and then to avoid taking"},{"from":1932.52,"to":1934.77,"location":2,"content":"a really conservative strategy where you only generate"},{"from":1934.77,"to":1937.55,"location":2,"content":"really safe things in a really short translation."},{"from":1937.55,"to":1940.04,"location":2,"content":"That's why you add the brevity penalty to make sure"},{"from":1940.04,"to":1943.04,"location":2,"content":"that [NOISE] it tries to write something long enough."},{"from":1943.04,"to":1944.64,"location":2,"content":"And then by contrast,"},{"from":1944.64,"to":1946.29,"location":2,"content":"recall is more important for"},{"from":1946.29,"to":1950.27,"location":2,"content":"summarization because you want to include all the information,"},{"from":1950.27,"to":1953.19,"location":2,"content":"the info- the important information in your summary, right?"},{"from":1953.19,"to":1956.49,"location":2,"content":"So the information that's in the reference summary is,"},{"from":1956.49,"to":1958.08,"location":2,"content":"uh, assumed to be the important information."},{"from":1958.08,"to":1960.24,"location":2,"content":"So recall means that you captured all of that."},{"from":1960.24,"to":1962.46,"location":2,"content":"Um, and I suppose i- if you assume that you have"},{"from":1962.46,"to":1965.04,"location":2,"content":"a maximum length constraint for your summarization system,"},{"from":1965.04,"to":1967.95,"location":2,"content":"then those two kind of give a trade-off, right?"},{"from":1967.95,"to":1972.72,"location":2,"content":"Where you want to include all the information but you can't be too long as a summary."},{"from":1972.72,"to":1975.43,"location":2,"content":"So I think that's the kind of justification why you have"},{"from":1975.43,"to":1978.15,"location":2,"content":"recall and precision for these two different tasks."},{"from":1978.15,"to":1981.48,"location":2,"content":"However, confusingly, often an F1,"},{"from":1981.48,"to":1983.91,"location":2,"content":"that is combination of precision and recall version of"},{"from":1983.91,"to":1986.94,"location":2,"content":"ROUGE is reported anyway in the summarization literature."},{"from":1986.94,"to":1989.49,"location":2,"content":"And to be honest, I'm not entirely sure why this is, uh,"},{"from":1989.49,"to":1991.14,"location":2,"content":"maybe it's because of the lack of,"},{"from":1991.14,"to":1993.49,"location":2,"content":"uh, explicit max length constraint."},{"from":1993.49,"to":1997.82,"location":2,"content":"Um, anyway, I, I tried to search that but I couldn't find an answer."},{"from":1997.82,"to":2001.1,"location":2,"content":"So here's some more information on ROUGE."},{"from":2001.1,"to":2002.84,"location":2,"content":"Um, if you remember,"},{"from":2002.84,"to":2004.94,"location":2,"content":"BLEU is reported as a single number, right?"},{"from":2004.94,"to":2006.98,"location":2,"content":"BLEU is just a single number and it is"},{"from":2006.98,"to":2010.64,"location":2,"content":"a combination of the precisions for the different n-grams"},{"from":2010.64,"to":2012.95,"location":2,"content":"which is usually 1-4 whereas"},{"from":2012.95,"to":2016.91,"location":2,"content":"ROUGE scores are usually reported separately for each n-gram."},{"from":2016.91,"to":2022.25,"location":2,"content":"So, the most commonly reported ROUGE scores are ROUGE-1, ROUGE-2 and ROUGE-L."},{"from":2022.25,"to":2027.37,"location":2,"content":"So, ROUGE one, not to be confused with Rogue One: A Star Wars Story."},{"from":2027.37,"to":2029.06,"location":2,"content":"Um, I feel like since that film came out,"},{"from":2029.06,"to":2031.61,"location":2,"content":"I see so many people mistyping this, and I think it's related."},{"from":2031.61,"to":2034.73,"location":2,"content":"Um, so, ROUGE-1 is, uh,"},{"from":2034.73,"to":2037.3,"location":2,"content":"based on unigram overlap,"},{"from":2037.3,"to":2041.02,"location":2,"content":"um, [NOISE] and ROUGE-2 based on bigram overlap."},{"from":2041.02,"to":2043.31,"location":2,"content":"It's kind of an analogy to BLEU really except,"},{"from":2043.31,"to":2045.24,"location":2,"content":"uh, recall-based, not precision-based."},{"from":2045.24,"to":2050.45,"location":2,"content":"The more interesting one is ROUGE-L which is longest common subsequence overlap."},{"from":2050.45,"to":2054.59,"location":2,"content":"Um, so, the idea here is that you are interested not only in, uh,"},{"from":2054.59,"to":2056.86,"location":2,"content":"particular n-grams matching up but in,"},{"from":2056.86,"to":2058.31,"location":2,"content":"you know, how many, uh, how,"},{"from":2058.31,"to":2063.52,"location":2,"content":"how long a sequence of words can you find that appear in both."},{"from":2063.52,"to":2066.64,"location":2,"content":"So you can, uh, read more about these metrics"},{"from":2066.64,"to":2069.07,"location":2,"content":"in the paper that was linked on the previous page."},{"from":2069.07,"to":2071.49,"location":2,"content":"And another really important thing to note is there's [NOISE] now"},{"from":2071.49,"to":2075.2,"location":2,"content":"a convenient Python implementation of ROUGE, and um,"},{"from":2075.2,"to":2078.16,"location":2,"content":"maybe it is not apparent why that's exciting,"},{"from":2078.16,"to":2080.42,"location":2,"content":"but it's actually pretty exciting because for a long time,"},{"from":2080.42,"to":2082.48,"location":2,"content":"there was just this Perl script, um,"},{"from":2082.48,"to":2086.36,"location":2,"content":"that was quite hard to run and quite hard to set up and understand."},{"from":2086.36,"to":2089.44,"location":2,"content":"So um, someone out there has been a hero and has, uh,"},{"from":2089.44,"to":2092.29,"location":2,"content":"implemented a pure Python version of ROUGE and checked that it"},{"from":2092.29,"to":2095.32,"location":2,"content":"really does match up to the Perl script that people were using before."},{"from":2095.32,"to":2098.89,"location":2,"content":"So if any of you are using ROUGE or doing summarization for your projects, uh,"},{"from":2098.89,"to":2100.07,"location":2,"content":"make sure that you, uh,"},{"from":2100.07,"to":2102.53,"location":2,"content":"go use that because it will probably save you some time."},{"from":2102.53,"to":2106.09,"location":2,"content":"[NOISE] Okay."},{"from":2106.09,"to":2108.02,"location":2,"content":"So we're gonna re- return to ROUGE a little bit later."},{"from":2108.02,"to":2110.21,"location":2,"content":"Um, I know that in assignment 4 you thought about"},{"from":2110.21,"to":2112.79,"location":2,"content":"the shortcomings of BLEU as a metric and um,"},{"from":2112.79,"to":2116.55,"location":2,"content":"for sure ROUGE has some short- shortcomings as well as a metric for summarization."},{"from":2116.55,"to":2119.08,"location":2,"content":"Um, we're gonna come back to that later."},{"from":2119.08,"to":2123.23,"location":2,"content":"Okay. So, we're gonna move on to neural approaches for summarization."},{"from":2123.23,"to":2127.97,"location":2,"content":"[NOISE] So uh, going back to 2015,"},{"from":2127.97,"to":2130.31,"location":2,"content":"I don't have another dramatic reenactment, I'm afraid."},{"from":2130.31,"to":2132.71,"location":2,"content":"[NOISE] Um, Rush et al."},{"from":2132.71,"to":2135.59,"location":2,"content":"published the first seq2seq summarization paper."},{"from":2135.59,"to":2139.07,"location":2,"content":"[NOISE] So uh, they were viewing this as,"},{"from":2139.07,"to":2141.39,"location":2,"content":"you know, NMT has recently been super successful,"},{"from":2141.39,"to":2144.5,"location":2,"content":"why don't we view abstractive summarization as a translation task and"},{"from":2144.5,"to":2148.57,"location":2,"content":"therefore apply standard translation seq2seq methods to it."},{"from":2148.57,"to":2151.91,"location":2,"content":"So that's exactly what they did and they applied,"},{"from":2151.91,"to":2153.5,"location":2,"content":"uh, a standard attention model,"},{"from":2153.5,"to":2158,"location":2,"content":"and then they did a pretty good job at, uh, Gigaword summarization."},{"from":2158,"to":2159.62,"location":2,"content":"That's the one where you're, um,"},{"from":2159.62,"to":2163.13,"location":2,"content":"converting from the first sentence of the news article to the headline."},{"from":2163.13,"to":2165.57,"location":2,"content":"So it's kind of like, uh, sentence compression."},{"from":2165.57,"to":2170.57,"location":2,"content":"So crucially, this is kind of the same order of magnitude of length as NMT, right?"},{"from":2170.57,"to":2173.81,"location":2,"content":"Because NMT is sentence to sentence and this is kind of sentence to sentence,"},{"from":2173.81,"to":2175.8,"location":2,"content":"maybe at most two sentence two sentence."},{"from":2175.8,"to":2178.31,"location":2,"content":"So this works pretty well and you can get pretty decent, um,"},{"from":2178.31,"to":2180.92,"location":2,"content":"headline generation or sentence compression using this kind of method."},{"from":2180.92,"to":2183.51,"location":2,"content":"[NOISE] Okay."},{"from":2183.51,"to":2185.51,"location":2,"content":"So after that, since 2015,"},{"from":2185.51,"to":2189.38,"location":2,"content":"there have been lots more developments in neural abstractive summarization."},{"from":2189.38,"to":2191.43,"location":2,"content":"And you can kind of um,"},{"from":2191.43,"to":2193.86,"location":2,"content":"group together these developments in,"},{"from":2193.86,"to":2195.44,"location":2,"content":"uh, a collection of themes."},{"from":2195.44,"to":2198.02,"location":2,"content":"So one theme is make it easier to copy."},{"from":2198.02,"to":2201.05,"location":2,"content":"Uh, this seems pretty obvious because in summarization, you know,"},{"from":2201.05,"to":2204.03,"location":2,"content":"you're gonna want to copy every, quite a few words and even phrases,"},{"from":2204.03,"to":2206.61,"location":2,"content":"but don't copy too much."},{"from":2206.61,"to":2208.13,"location":2,"content":"Uh, that's the other thing is that if you make it"},{"from":2208.13,"to":2209.63,"location":2,"content":"too easy to copy, then you copy too much."},{"from":2209.63,"to":2212.6,"location":2,"content":"So, then there's other research showing how to prevent too much copying."},{"from":2212.6,"to":2218.14,"location":2,"content":"[NOISE] Uh, the next thing is some kind of hierarchical or multi-level attention."},{"from":2218.14,"to":2219.47,"location":2,"content":"So as I just showed,"},{"from":2219.47,"to":2221.69,"location":2,"content":"the attention has been pretty key to, um,"},{"from":2221.69,"to":2224,"location":2,"content":"abstractive neural summarization so far."},{"from":2224,"to":2225.61,"location":2,"content":"So there's been some work looking at, you know,"},{"from":2225.61,"to":2228.89,"location":2,"content":"can we kind of make this attention work at a more kind of high-level,"},{"from":2228.89,"to":2232.1,"location":2,"content":"low-level cost fine version so"},{"from":2232.1,"to":2236.03,"location":2,"content":"that we can kind of maybe do our selection at the high-level and at low-level."},{"from":2236.03,"to":2238.99,"location":2,"content":"Another thing which is kind of related is having"},{"from":2238.99,"to":2241.7,"location":2,"content":"some more kind of global content selection."},{"from":2241.7,"to":2243.51,"location":2,"content":"So if you remember when we were talking about the,"},{"from":2243.51,"to":2246.02,"location":2,"content":"the pipelines pre-neural summarization,"},{"from":2246.02,"to":2248.43,"location":2,"content":"they had these different content selection algorithms."},{"from":2248.43,"to":2250.25,"location":2,"content":"And I think you can say that,"},{"from":2250.25,"to":2252.11,"location":2,"content":"um, kind of naive attention,"},{"from":2252.11,"to":2254.63,"location":2,"content":"attention seq2seq is not necessarily"},{"from":2254.63,"to":2257.49,"location":2,"content":"the best way to do content selection for summarization,"},{"from":2257.49,"to":2260.89,"location":2,"content":"maybe you want a more kind of global strategy where you choose what's important."},{"from":2260.89,"to":2264.05,"location":2,"content":"It's not so apparent here when you're doing this small-scale summarization,"},{"from":2264.05,"to":2265.43,"location":2,"content":"but if you imagine that you're summarizing"},{"from":2265.43,"to":2268.29,"location":2,"content":"a whole news article and you're choosing which information,"},{"from":2268.29,"to":2270.45,"location":2,"content":"kind of deciding on each decoder step,"},{"from":2270.45,"to":2273.17,"location":2,"content":"what to choose doesn't seem like the most global strategy."},{"from":2273.17,"to":2276.01,"location":2,"content":"Er, what else have we got?"},{"from":2276.01,"to":2279.41,"location":2,"content":"Uh, there's using, uh, Reinforcement Learning to directly maximize"},{"from":2279.41,"to":2281.3,"location":2,"content":"ROUGE or other discrete goals you might"},{"from":2281.3,"to":2283.82,"location":2,"content":"care about such as maybe the length of the summary."},{"from":2283.82,"to":2287.49,"location":2,"content":"Um, and I say discrete here because ROUGE is a non-differentiable,"},{"from":2287.49,"to":2289.64,"location":2,"content":"uh, function of your generated outputs."},{"from":2289.64,"to":2292.16,"location":2,"content":"There's no, you know, easy way to differentiably"},{"from":2292.16,"to":2294.2,"location":2,"content":"learn that during training in the usual way."},{"from":2294.2,"to":2300.17,"location":2,"content":"Uh, my last point on this list is the kind of theme of"},{"from":2300.17,"to":2304.04,"location":2,"content":"resurrecting pre-neural ideas such as those graph algorithms that I mentioned"},{"from":2304.04,"to":2305.96,"location":2,"content":"earlier and working them into"},{"from":2305.96,"to":2312.01,"location":2,"content":"these new seq2seq abstractive neural systems and I'm sure there is more as well."},{"from":2312.01,"to":2314.15,"location":2,"content":"So, I'm gonna show you a few of these, um,"},{"from":2314.15,"to":2317.66,"location":2,"content":"especially because even if you're not particularly interested in summarization,"},{"from":2317.66,"to":2320.93,"location":2,"content":"a lot of the ideas that we're gonna explore here are actually kind of applicable"},{"from":2320.93,"to":2325.3,"location":2,"content":"to other areas of NLG or just other areas of NLP deep learning."},{"from":2325.3,"to":2328.7,"location":2,"content":"So, the first thing on the list is making it easier to copy,"},{"from":2328.7,"to":2330.88,"location":2,"content":"which seems like probably the first thing you want to fix,"},{"from":2330.88,"to":2333.34,"location":2,"content":"if you've just got basic seq2seq with attention."},{"from":2333.34,"to":2335.8,"location":2,"content":"So, um, a copy mechanism,"},{"from":2335.8,"to":2338.9,"location":2,"content":"which can exist outside of summarization."},{"from":2338.9,"to":2343.16,"location":2,"content":"The reason, why you want this is that basic seq2seq with attention,"},{"from":2343.16,"to":2345.59,"location":2,"content":"they're good at writing fluent output, as we know,"},{"from":2345.59,"to":2349.84,"location":2,"content":"but they are pretty bad at copying over details like rare words correctly."},{"from":2349.84,"to":2353.21,"location":2,"content":"So a copy mechanism is just the kind of sensible idea of saying,"},{"from":2353.21,"to":2357.95,"location":2,"content":"um, let's have an explicit mechanism to just copy over words."},{"from":2357.95,"to":2360.14,"location":2,"content":"So for example, you could use"},{"from":2360.14,"to":2365.38,"location":2,"content":"the attention distribution to- to kind of select what you're going to copy."},{"from":2365.38,"to":2368.89,"location":2,"content":"Um, so, if you are allowing both copying"},{"from":2368.89,"to":2372.24,"location":2,"content":"over words and generating words in the usual way with your language model,"},{"from":2372.24,"to":2377.22,"location":2,"content":"then now you've got a kind of hybrid extractive/abstractive approach to summarization."},{"from":2377.22,"to":2380.36,"location":2,"content":"So, there are several papers, which are- which propose"},{"from":2380.36,"to":2383.33,"location":2,"content":"some kind of copy mechanism variants and I think,"},{"from":2383.33,"to":2385.04,"location":2,"content":"the reason why there is multiple is because there's"},{"from":2385.04,"to":2388.73,"location":2,"content":"kind of a few different choices you can make about how to implement this,"},{"from":2388.73,"to":2393.38,"location":2,"content":"and that means that there's a few different versions of how to implement copy mechanism."},{"from":2393.38,"to":2396.16,"location":2,"content":"So, uh, yeah, there are several papers here which you can look at."},{"from":2396.16,"to":2398.69,"location":2,"content":"I'm going to show you a diagram from a paper that um,"},{"from":2398.69,"to":2401.15,"location":2,"content":"I did a few years ago with Chris."},{"from":2401.15,"to":2404.91,"location":2,"content":"So, this is just one example of how you can do a copying mechanism."},{"from":2404.91,"to":2406.51,"location":2,"content":"So, the - the way we did it,"},{"from":2406.51,"to":2408.49,"location":2,"content":"is we said that on each decoder step,"},{"from":2408.49,"to":2411.59,"location":2,"content":"you're going to calculate this probability Pgen and that's"},{"from":2411.59,"to":2415.37,"location":2,"content":"the probability of generating the next word rather than copying it,"},{"from":2415.37,"to":2419.09,"location":2,"content":"and the idea is that this is computed based on your current kind of context,"},{"from":2419.09,"to":2420.93,"location":2,"content":"your current decoder hidden state."},{"from":2420.93,"to":2422.59,"location":2,"content":"So, then once you've done that,"},{"from":2422.59,"to":2424.79,"location":2,"content":"then the idea is you've got your attention distribution as"},{"from":2424.79,"to":2427.28,"location":2,"content":"normal and you've got your kind of output,"},{"from":2427.28,"to":2431.36,"location":2,"content":"you know, generation distribution as normal and you're going to use this Pgen,"},{"from":2431.36,"to":2432.55,"location":2,"content":"which is just a scalar."},{"from":2432.55,"to":2435.05,"location":2,"content":"You can use that to kind of, uh, combine,"},{"from":2435.05,"to":2438.01,"location":2,"content":"mix together these two probability distributions."},{"from":2438.01,"to":2440.12,"location":2,"content":"So, what this equation is telling you,"},{"from":2440.12,"to":2441.41,"location":2,"content":"is that saying that the uh,"},{"from":2441.41,"to":2444.23,"location":2,"content":"final output distribution for uh,"},{"from":2444.23,"to":2445.59,"location":2,"content":"what word is gonna come next,"},{"from":2445.59,"to":2447.08,"location":2,"content":"it's kind of saying, you know,"},{"from":2447.08,"to":2448.68,"location":2,"content":"it is the probability of generating"},{"from":2448.68,"to":2451.89,"location":2,"content":"times your probability distribution of what you would generate"},{"from":2451.89,"to":2454.28,"location":2,"content":"but then also the probability of copying"},{"from":2454.28,"to":2457.22,"location":2,"content":"and then also what you're attending to at that time."},{"from":2457.22,"to":2461.55,"location":2,"content":"So, the, the main thing is, you're using attention as your copying mechanism."},{"from":2461.55,"to":2463.61,"location":2,"content":"So, attention is kind of doing double-duty here."},{"from":2463.61,"to":2467.89,"location":2,"content":"It's both uh, being useful for the generator to,"},{"from":2467.89,"to":2470,"location":2,"content":"you know, uh, maybe choose to rephrase things"},{"from":2470,"to":2472.46,"location":2,"content":"but it is also being useful as a copying mechanism."},{"from":2472.46,"to":2475.43,"location":2,"content":"And I think that's one of the several things that these different papers do differently."},{"from":2475.43,"to":2478.94,"location":2,"content":"I think, I've seen a paper that maybe has like two separate uh,"},{"from":2478.94,"to":2481.68,"location":2,"content":"attention distributions, one for the copying and one for the attending."},{"from":2481.68,"to":2484.46,"location":2,"content":"Um, other choices you can make differently are for example,"},{"from":2484.46,"to":2487.43,"location":2,"content":"D1 Pgen to be this kind of soft thing that's between zero and"},{"from":2487.43,"to":2490.73,"location":2,"content":"one or do you want it to be a hard thing that has to be either zero or one."},{"from":2490.73,"to":2493.97,"location":2,"content":"Um, you can also make decisions about like"},{"from":2493.97,"to":2497,"location":2,"content":"do you want the Pgen to have supervision during training?"},{"from":2497,"to":2500.16,"location":2,"content":"Do you want to kind of annotate your data set saying these things are copied, things,"},{"from":2500.16,"to":2503.54,"location":2,"content":"these things are not, or do you want to just like learn it end-to-end?"},{"from":2503.54,"to":2506.07,"location":2,"content":"So there's multiple ways you can do this and um,"},{"from":2506.07,"to":2510.1,"location":2,"content":"this has now become pretty, pretty standard."},{"from":2510.1,"to":2512.99,"location":2,"content":"Okay, so copy mechanism seems like,"},{"from":2512.99,"to":2515.96,"location":2,"content":"seems like a sensible idea but there's a big problem with them,"},{"from":2515.96,"to":2518.33,"location":2,"content":"which is what I mentioned earlier and that problem is,"},{"from":2518.33,"to":2519.66,"location":2,"content":"that they copy too much."},{"from":2519.66,"to":2523.31,"location":2,"content":"Um, so, when you- when you run these kind of systems on summarization,"},{"from":2523.31,"to":2525.53,"location":2,"content":"you find that they end up copying a lot of"},{"from":2525.53,"to":2528.86,"location":2,"content":"long phrases and sometimes even whole sentences and uh,"},{"from":2528.86,"to":2531.86,"location":2,"content":"unfortunately your dream of having an abstractive summarization system,"},{"from":2531.86,"to":2533.8,"location":2,"content":"isn't going to work out because your, um,"},{"from":2533.8,"to":2536.51,"location":2,"content":"you know, copy augmented seq2seq system has just"},{"from":2536.51,"to":2540.03,"location":2,"content":"collapsed into a mostly extractive system, which is unfortunate."},{"from":2540.03,"to":2542.06,"location":2,"content":"Another problem with these uh,"},{"from":2542.06,"to":2545.16,"location":2,"content":"copy mechanism models is that they are bad at"},{"from":2545.16,"to":2548.6,"location":2,"content":"overall content selection especially if the input document is long,"},{"from":2548.6,"to":2550.25,"location":2,"content":"and this is what I was hinting at earlier."},{"from":2550.25,"to":2553.58,"location":2,"content":"Um, let's suppose, that you are summarizing something that's quite"},{"from":2553.58,"to":2557.09,"location":2,"content":"long like a news article that's hundreds of words long and you,"},{"from":2557.09,"to":2558.99,"location":2,"content":"you want to write a several sentence summary."},{"from":2558.99,"to":2561.57,"location":2,"content":"It doesn't seem like the kind of smartest choice to"},{"from":2561.57,"to":2564.35,"location":2,"content":"on every step of writing your several sentence summary,"},{"from":2564.35,"to":2566.39,"location":2,"content":"but you're choosing again what to attend to,"},{"from":2566.39,"to":2568.32,"location":2,"content":"what to select, what to summarize."},{"from":2568.32,"to":2572.8,"location":2,"content":"It seems better to kind of make a global decision at the beginning and then summarize."},{"from":2572.8,"to":2576.56,"location":2,"content":"So, yeah, the problem is, there's no overall strategy for selecting the contents."},{"from":2576.56,"to":2583.82,"location":2,"content":"So, uh, here's a paper that I found. Nope, not yet."},{"from":2583.82,"to":2588.45,"location":2,"content":"Okay. So, how might you do better content selection for neural summarization?"},{"from":2588.45,"to":2592.01,"location":2,"content":"So, if you remember in this pre-neural summarization we looked at,"},{"from":2592.01,"to":2594.89,"location":2,"content":"you had completely separate stages in the pipeline, right?"},{"from":2594.89,"to":2596.87,"location":2,"content":"You had the content selection stage and you had"},{"from":2596.87,"to":2600.26,"location":2,"content":"a surface realization that is the text generation stage."},{"from":2600.26,"to":2602.75,"location":2,"content":"But in our seq2seq attention systems,"},{"from":2602.75,"to":2605.24,"location":2,"content":"these two stages are just completely mixed together, right?"},{"from":2605.24,"to":2608.78,"location":2,"content":"You're doing your step-by-step surface realization that is text generation,"},{"from":2608.78,"to":2611.74,"location":2,"content":"and then on each of those, you're also doing content selection."},{"from":2611.74,"to":2615.3,"location":2,"content":"So, yeah, this doesn't make sense."},{"from":2615.3,"to":2617.51,"location":2,"content":"So, I found a paper, which is,"},{"from":2617.51,"to":2619.74,"location":2,"content":"uh, published I think last year,"},{"from":2619.74,"to":2622.16,"location":2,"content":"which gives a quite nice kind of"},{"from":2622.16,"to":2627.05,"location":2,"content":"simple solution to this problem and it's called bottom-up summarization."},{"from":2627.05,"to":2631.72,"location":2,"content":"So, in this paper if you look at the- if you look at the figure,"},{"from":2631.72,"to":2633.26,"location":2,"content":"uh, the main idea is pretty simple."},{"from":2633.26,"to":2637.37,"location":2,"content":"It says that, first you're going to have a content selection stage and this is"},{"from":2637.37,"to":2641.99,"location":2,"content":"just uh, thought of as a neural sequence tagging model problem, right?"},{"from":2641.99,"to":2644.66,"location":2,"content":"You run through your source documents and"},{"from":2644.66,"to":2647.61,"location":2,"content":"you kind of tag every word as include or don't include."},{"from":2647.61,"to":2649.79,"location":2,"content":"So, you're just kinda deciding like what seems important,"},{"from":2649.79,"to":2651.68,"location":2,"content":"what seems like it should make it into the summary and what"},{"from":2651.68,"to":2655.63,"location":2,"content":"doesn't and then the bottom-up attention stage says that,"},{"from":2655.63,"to":2658.01,"location":2,"content":"now you'll seq2seq with an attention system,"},{"from":2658.01,"to":2659.95,"location":2,"content":"which is gonna generate the summary."},{"from":2659.95,"to":2661.61,"location":2,"content":"Are you're gonna kind of apply a mask?"},{"from":2661.61,"to":2663.13,"location":2,"content":"You know, apply a hard constraint that says,"},{"from":2663.13,"to":2666.91,"location":2,"content":"that you can't attend to words that were tagged don't-include."},{"from":2666.91,"to":2670.59,"location":2,"content":"So, this turns out to be pretty simple but effective um,"},{"from":2670.59,"to":2674.09,"location":2,"content":"because it's a better overall content selection strategy because by doing"},{"from":2674.09,"to":2678.8,"location":2,"content":"this first content selection stage by sequence-tagging you're kind of just,"},{"from":2678.8,"to":2682.73,"location":2,"content":"just doing the selection thing without also at the same time doing the generation thing,"},{"from":2682.73,"to":2684.8,"location":2,"content":"which I think turns out to be a better way to make"},{"from":2684.8,"to":2687.82,"location":2,"content":"better decisions about what to include and then separately,"},{"from":2687.82,"to":2689.9,"location":2,"content":"this also means as a great side effect,"},{"from":2689.9,"to":2693.5,"location":2,"content":"you have less copying of long sequences in the generation model."},{"from":2693.5,"to":2696.83,"location":2,"content":"Um, because if you are not allowed to attend to things,"},{"from":2696.83,"to":2698.22,"location":2,"content":"which you shouldn't be including,"},{"from":2698.22,"to":2701.96,"location":2,"content":"then it's kind of hard to copy a really long sequence, right?"},{"from":2701.96,"to":2705.32,"location":2,"content":"Like if you want to copy a whole sentence but the sentence has"},{"from":2705.32,"to":2708.98,"location":2,"content":"plenty of don't include words in it,"},{"from":2708.98,"to":2711.64,"location":2,"content":"then you can't really copy a long sequence, you have to break it up."},{"from":2711.64,"to":2712.97,"location":2,"content":"So, what the model ends up doing,"},{"from":2712.97,"to":2714.32,"location":2,"content":"is it kind of has to skip,"},{"from":2714.32,"to":2717.11,"location":2,"content":"skip around the parts that is meant to include and then it's forced to"},{"from":2717.11,"to":2720.65,"location":2,"content":"be more abstractive to put the parts together. Yep."},{"from":2720.65,"to":2725.51,"location":2,"content":"How did they backpropagate the masking decision because it seems like-"},{"from":2725.51,"to":2728.72,"location":2,"content":"Because during training [inaudible] masking decision."},{"from":2728.72,"to":2732.03,"location":2,"content":"Yeah, I think it might be trained separately."},{"from":2732.03,"to":2733.61,"location":2,"content":"I mean, you can go and check the paper."},{"from":2733.61,"to":2735.89,"location":2,"content":"I've, I've read a lot of papers in the last days, I can't quite remember."},{"from":2735.89,"to":2737.99,"location":2,"content":"I think, it might be trained separately but they might"},{"from":2737.99,"to":2740.66,"location":2,"content":"have tried training it together but it didn't work as well."},{"from":2740.66,"to":2743.2,"location":2,"content":"I am not sure. You can check it out."},{"from":2743.2,"to":2748.74,"location":2,"content":"Okay. So, another paper I want to tell you about is a paper which uh,"},{"from":2748.74,"to":2753.97,"location":2,"content":"used reinforcement learning to directly maximize ROUGE for neural summarization."},{"from":2753.97,"to":2756.28,"location":2,"content":"So this was a paper from two years ago."},{"from":2756.28,"to":2758.36,"location":2,"content":"And the main idea is that they can use RL to"},{"from":2758.36,"to":2761.87,"location":2,"content":"directly optimize in this case ROUGE-L, the metric."},{"from":2761.87,"to":2766.01,"location":2,"content":"So by contrast, the standard maximum likelihood of training that is"},{"from":2766.01,"to":2767.84,"location":2,"content":"the training objective we've been talking about for"},{"from":2767.84,"to":2770.39,"location":2,"content":"the whole class so far for language models uh,"},{"from":2770.39,"to":2773.84,"location":2,"content":"that can't directly optimize ROUGE-L because it's a non-differentiable function."},{"from":2773.84,"to":2776.87,"location":2,"content":"So they uh, they use this RL technique"},{"from":2776.87,"to":2781.82,"location":2,"content":"to compute the ROUGE score during training and then uh,"},{"from":2781.82,"to":2786.11,"location":2,"content":"use a reinforcement learning to backprop to the model."},{"from":2786.11,"to":2793.22,"location":2,"content":"So, the interesting finding from this paper is that if they just used the RL objective,"},{"from":2793.22,"to":2796.04,"location":2,"content":"then they do indeed get higher ROUGE scores."},{"from":2796.04,"to":2798.47,"location":2,"content":"So they can successfully optimize"},{"from":2798.47,"to":2800.24,"location":2,"content":"this ROUGE-L metric that they were aiming to"},{"from":2800.24,"to":2802.76,"location":2,"content":"optimize but the problem is that when you do that,"},{"from":2802.76,"to":2804.72,"location":2,"content":"you get lower human judgment scores."},{"from":2804.72,"to":2807.05,"location":2,"content":"So, on the right we're seeing that the RL only model has"},{"from":2807.05,"to":2811.78,"location":2,"content":"actually pretty pretty bad readability relevance human judgment scores."},{"from":2811.78,"to":2817.24,"location":2,"content":"It's worse than just the maximum likelihood supervised training system."},{"from":2817.24,"to":2820.68,"location":2,"content":"So, this is a quote from their blog post that says,"},{"from":2820.68,"to":2822.95,"location":2,"content":"\"We have observed that our models with the highest ROUGE scores"},{"from":2822.95,"to":2825.34,"location":2,"content":"also generated barely readable summaries.\""},{"from":2825.34,"to":2826.76,"location":2,"content":"So, this is- this is,"},{"from":2826.76,"to":2828.14,"location":2,"content":"um, I suppose a problem, right?"},{"from":2828.14,"to":2831.17,"location":2,"content":"If you try to directly optimize for the metric,"},{"from":2831.17,"to":2833.45,"location":2,"content":"then you might start finding that you're kind of gaming"},{"from":2833.45,"to":2836.68,"location":2,"content":"the metric and not optimizing for the true task, right,"},{"from":2836.68,"to":2840.55,"location":2,"content":"because we know, just as we know that BLEU was not really a perfect analogy to"},{"from":2840.55,"to":2842.53,"location":2,"content":"actual translation quality so is ROUGE"},{"from":2842.53,"to":2846.26,"location":2,"content":"not a perfect analogy to uh, summarization quality."},{"from":2846.26,"to":2848.66,"location":2,"content":"But they did do something cool, which is that they found that if"},{"from":2848.66,"to":2851.42,"location":2,"content":"you combine the two objectives,"},{"from":2851.42,"to":2853.03,"location":2,"content":"so they kind of, uh, you know,"},{"from":2853.03,"to":2856.91,"location":2,"content":"predict the language model sequence objective and then they also like produce"},{"from":2856.91,"to":2861.3,"location":2,"content":"an overall summary that gets a high ROUGE score objective and you combine them together,"},{"from":2861.3,"to":2865.37,"location":2,"content":"then you can get a better human uh, judgment score,"},{"from":2865.37,"to":2868.22,"location":2,"content":"which in the end is the closest thing we have to uh,"},{"from":2868.22,"to":2869.93,"location":2,"content":"a measure of actual summarization quality."},{"from":2869.93,"to":2874.34,"location":2,"content":"[NOISE] Okay."},{"from":2874.34,"to":2877.28,"location":2,"content":"So, I'm gonna move on to uh, dialogue,"},{"from":2877.28,"to":2881.75,"location":2,"content":"which is um, a different NLG, kind of family of tasks."},{"from":2881.75,"to":2885.59,"location":2,"content":"Uh, so, really dialogue encompasses a really large variety of settings."},{"from":2885.59,"to":2886.7,"location":2,"content":"And we are not going to cover them all,"},{"from":2886.7,"to":2888.8,"location":2,"content":"but here is a kind of overview of all the different kinds"},{"from":2888.8,"to":2891.18,"location":2,"content":"of tasks that people might mean, when they say dialogue."},{"from":2891.18,"to":2895.49,"location":2,"content":"Um, so, there's task-oriented dialogue and this kind of refers to any setting,"},{"from":2895.49,"to":2898.2,"location":2,"content":"where you're trying to kind of get something done in the conversation."},{"from":2898.2,"to":2899.69,"location":2,"content":"So, if for example, you've got kind of"},{"from":2899.69,"to":2903.59,"location":2,"content":"assistive tasks where it's assumed that you have, you know, maybe the uh,"},{"from":2903.59,"to":2907.04,"location":2,"content":"the dialogue agent is trying to help a human user to do"},{"from":2907.04,"to":2910.7,"location":2,"content":"something like maybe giving customer service or recommendations,"},{"from":2910.7,"to":2912.89,"location":2,"content":"answering questions, helping a user,"},{"from":2912.89,"to":2915.95,"location":2,"content":"you know, accomplish a task like buying or booking something."},{"from":2915.95,"to":2918.35,"location":2,"content":"Uh, these are the kinds of tasks, which the virtual systems on"},{"from":2918.35,"to":2921.74,"location":2,"content":"your phone can do or can kind of do."},{"from":2921.74,"to":2926.59,"location":2,"content":"Um, another family of task-oriented dialogue tasks are cooperative tasks."},{"from":2926.59,"to":2929.15,"location":2,"content":"So, this is kind of anything where you've  got two agents who are"},{"from":2929.15,"to":2932.12,"location":2,"content":"trying to solve a task together via dialogue."},{"from":2932.12,"to":2934.72,"location":2,"content":"Um, and the opposite of that would be adversarial."},{"from":2934.72,"to":2938.6,"location":2,"content":"So anything where you have two agents who are trying to compete in a task and that uh,"},{"from":2938.6,"to":2942.34,"location":2,"content":"competition is conducted through dialogue."},{"from":2942.34,"to":2948.95,"location":2,"content":"[NOISE] So uh, the opposite to task-oriented dialogue is, uh, social dialogue."},{"from":2948.95,"to":2953.6,"location":2,"content":"So that's something where there is no explicit task other than to, I suppose socialize."},{"from":2953.6,"to":2956.11,"location":2,"content":"So chit-chat dialogue, um,"},{"from":2956.11,"to":2960.2,"location":2,"content":"is just dialogue where you're just doing it for social fun or for company."},{"from":2960.2,"to":2964.91,"location":2,"content":"Um, I've also seen some work on kind of like therapy or mental well-being dialogue,"},{"from":2964.91,"to":2966.74,"location":2,"content":"I'm not sure if this should go in task or social,"},{"from":2966.74,"to":2968.11,"location":2,"content":"it's kind of a mix, uh,"},{"from":2968.11,"to":2970.58,"location":2,"content":"but I suppose these are the ones where the goal is to"},{"from":2970.58,"to":2974.28,"location":2,"content":"maybe offer kind of emotional support to the human user."},{"from":2974.28,"to":2980.03,"location":2,"content":"Um, so as a very kind of brief overview of how,"},{"from":2980.03,"to":2982.07,"location":2,"content":"uh, the deep learning, uh,"},{"from":2982.07,"to":2985.07,"location":2,"content":"renaissance has kind of changed dialog research, um,"},{"from":2985.07,"to":2988.59,"location":2,"content":"I think you can say that in kind of pre-deep learning,"},{"from":2988.59,"to":2990.62,"location":2,"content":"uh, the difficulty of open-ended,"},{"from":2990.62,"to":2993.83,"location":2,"content":"free-form natural language generation, meant that, uh,"},{"from":2993.83,"to":2995.41,"location":2,"content":"dialogue systems were often,"},{"from":2995.41,"to":2998.36,"location":2,"content":"uh, not doing free-form NLG."},{"from":2998.36,"to":3000.73,"location":2,"content":"They might use predefined templates meaning that you have"},{"from":3000.73,"to":3003.78,"location":2,"content":"a template where you just fill in some slots with the content, uh,"},{"from":3003.78,"to":3006.7,"location":2,"content":"or maybe you retrieve an appropriate response from"},{"from":3006.7,"to":3009.63,"location":2,"content":"a corpus of responses that you have in order to find,"},{"from":3009.63,"to":3011.38,"location":2,"content":"you know, an appropriate response for the user."},{"from":3011.38,"to":3013.33,"location":2,"content":"And these are by no means simple systems,"},{"from":3013.33,"to":3016.18,"location":2,"content":"they had some very complex things going on like deciding, you know,"},{"from":3016.18,"to":3019.57,"location":2,"content":"what their dialogue state is and what template you should use and so on and the-"},{"from":3019.57,"to":3023.91,"location":2,"content":"all the natural language understanding components of understanding the context so far."},{"from":3023.91,"to":3026.45,"location":2,"content":"But, uh, one effect that,"},{"from":3026.45,"to":3028.82,"location":2,"content":"that deep learning had is that, uh,"},{"from":3028.82,"to":3031.91,"location":2,"content":"since again kind of 2015 which is when NMT, uh,"},{"from":3031.91,"to":3034.61,"location":2,"content":"became standard, there's been, uh,"},{"from":3034.61,"to":3038.44,"location":2,"content":"just like summarization, lots of papers applying seq2seq methods to dialogue."},{"from":3038.44,"to":3043.43,"location":2,"content":"And this has kind of led to a renewed interest in open-ended, free-form dialogue systems."},{"from":3043.43,"to":3045.76,"location":2,"content":"So uh, if you wanna have a look at what did"},{"from":3045.76,"to":3048.13,"location":2,"content":"those early seq2seq dialogue papers look like,"},{"from":3048.13,"to":3055.53,"location":2,"content":"um, here's two kind of early ones like maybe the first ones to apply seq2seq."},{"from":3055.53,"to":3060.4,"location":2,"content":"Okay. So uh, people quickly applied seq2seq, uh,"},{"from":3060.4,"to":3063.16,"location":2,"content":"NMT methods to dialogue but it quickly became"},{"from":3063.16,"to":3066.13,"location":2,"content":"very apparent that this kind of naive application of"},{"from":3066.13,"to":3068.56,"location":2,"content":"standard NMT methods has"},{"from":3068.56,"to":3073.91,"location":2,"content":"some serious pervasive deficiencies when applied to a task like chitchat dialogue."},{"from":3073.91,"to":3076.96,"location":2,"content":"And this is even more true than it was for summarization."},{"from":3076.96,"to":3081.14,"location":2,"content":"So what are some examples of these serious pervas- pervasive deficiencies?"},{"from":3081.14,"to":3084.43,"location":2,"content":"Uh, one would be genericness or boring responses,"},{"from":3084.43,"to":3086.71,"location":2,"content":"and I'll go into more detail about these in a moment."},{"from":3086.71,"to":3089.01,"location":2,"content":"Another one is irrelevant responses."},{"from":3089.01,"to":3090.18,"location":2,"content":"So that's when, uh,"},{"from":3090.18,"to":3092.2,"location":2,"content":"the dialogue agent kind of says something back"},{"from":3092.2,"to":3095.07,"location":2,"content":"that's just kind of unrelated to what the user says."},{"from":3095.07,"to":3096.7,"location":2,"content":"Um, another one is repetition,"},{"from":3096.7,"to":3098.08,"location":2,"content":"this is pretty basic but it,"},{"from":3098.08,"to":3099.64,"location":2,"content":"uh, it happens a lot."},{"from":3099.64,"to":3104.28,"location":2,"content":"Um, so that's also repetition within the utterance and maybe repetition across utterances."},{"from":3104.28,"to":3107.49,"location":2,"content":"Ah, another difficulty is,"},{"from":3107.49,"to":3108.91,"location":2,"content":"uh, kind of lack of context,"},{"from":3108.91,"to":3110.8,"location":2,"content":"like not remembering the conversation history."},{"from":3110.8,"to":3113.71,"location":2,"content":"Obviously, if you do not condition on the whole conversation history,"},{"from":3113.71,"to":3117.19,"location":2,"content":"there's no way your dialogue agent can use it but it is a challenge especially if you"},{"from":3117.19,"to":3121.32,"location":2,"content":"have a very long dialogue history to figure out how to condition on it effectively."},{"from":3121.32,"to":3124.06,"location":2,"content":"Another problem is the lack of consistent persona."},{"from":3124.06,"to":3125.38,"location":2,"content":"So if you kind of, uh,"},{"from":3125.38,"to":3129.97,"location":2,"content":"naively as in maybe those two papers that I referenced on the previous slide,"},{"from":3129.97,"to":3134.39,"location":2,"content":"if you naively train a kind of standard seq2seq model to maybe take the, uh,"},{"from":3134.39,"to":3136.48,"location":2,"content":"you know the user's last utterance and then say something back,"},{"from":3136.48,"to":3138.95,"location":2,"content":"or maybe even the whole dialogue history and say something back."},{"from":3138.95,"to":3142.68,"location":2,"content":"Often your dialogue agent will have this completely inconsistent persona,"},{"from":3142.68,"to":3146.8,"location":2,"content":"like one moment they will say that it lives in Europe and then it'll say it lives in,"},{"from":3146.8,"to":3149.77,"location":2,"content":"I don't know, China or something and it just doesn't make sense."},{"from":3149.77,"to":3151.91,"location":2,"content":"So I'm gonna go through, uh,"},{"from":3151.91,"to":3154.81,"location":2,"content":"some of these problems and give you a bit more detail on them."},{"from":3154.81,"to":3157.87,"location":2,"content":"So first, this irrelevant response problem."},{"from":3157.87,"to":3160.96,"location":2,"content":"So in a bit more detail, your problem is that seq2seq often"},{"from":3160.96,"to":3164.08,"location":2,"content":"generates some response that's kind of unrelated to the user's utterance."},{"from":3164.08,"to":3167.16,"location":2,"content":"So it can be unrelated because it's simply generic,"},{"from":3167.16,"to":3169.15,"location":2,"content":"which means that this is kind of like an overlap with"},{"from":3169.15,"to":3171.61,"location":2,"content":"a generic response problem or it can be"},{"from":3171.61,"to":3174.16,"location":2,"content":"kind of unrelated because the model's choosing to kind of change,"},{"from":3174.16,"to":3176.84,"location":2,"content":"to change the subject to something unrelated."},{"from":3176.84,"to":3179.02,"location":2,"content":"So one solution of many, there,"},{"from":3179.02,"to":3180.88,"location":2,"content":"there are a lot of different papers which, uh,"},{"from":3180.88,"to":3184.74,"location":2,"content":"kind of attack this irrelevant response problem, uh, but just one,"},{"from":3184.74,"to":3187.01,"location":2,"content":"one for example is, uh,"},{"from":3187.01,"to":3189.84,"location":2,"content":"that you should tr- change the training objective."},{"from":3189.84,"to":3192.76,"location":2,"content":"So instead of trying to optimize, um,"},{"from":3192.76,"to":3195.52,"location":2,"content":"mapping from input S to response T such that"},{"from":3195.52,"to":3198.63,"location":2,"content":"you're maximizing the conditional probability of T given S,"},{"from":3198.63,"to":3202.43,"location":2,"content":"instead you should maximize the maximum mutual information."},{"from":3202.43,"to":3204.24,"location":2,"content":"So that's why this is here."},{"from":3204.24,"to":3206.53,"location":2,"content":"So maximum mutual information, uh,"},{"from":3206.53,"to":3209.14,"location":2,"content":"you can kind of rewrite the objective like this,"},{"from":3209.14,"to":3211.91,"location":2,"content":"and if you want to see some more detail you can go look at this paper here."},{"from":3211.91,"to":3215.83,"location":2,"content":"But the idea is that you're trying to find your response T that kind of, uh,"},{"from":3215.83,"to":3218.68,"location":2,"content":"maximizes this thing which is kind of like saying,"},{"from":3218.68,"to":3221.68,"location":2,"content":"it needs to be probable given the inputs but"},{"from":3221.68,"to":3224.92,"location":2,"content":"kind of like as a ratio of its probability in itself."},{"from":3224.92,"to":3229.51,"location":2,"content":"So if T is very high likelihood,"},{"from":3229.51,"to":3232.6,"location":2,"content":"then it gets penalized and it's kind of like about the ratio"},{"from":3232.6,"to":3236.44,"location":2,"content":"of the probability given the input and it's just the stand-alone probability."},{"from":3236.44,"to":3239.65,"location":2,"content":"So the idea is that this is meant to discourage, um,"},{"from":3239.65,"to":3244.24,"location":2,"content":"just saying generic things that just have a high PT by themselves."},{"from":3244.24,"to":3248.99,"location":2,"content":"Um, so that's the irrelevant response problem."},{"from":3248.99,"to":3250.78,"location":2,"content":"And as I just hinted at, there's, uh,"},{"from":3250.78,"to":3252.04,"location":2,"content":"definitely a strong link between"},{"from":3252.04,"to":3256.87,"location":2,"content":"the irrelevant response problem and the kind of generic or boring response problem."},{"from":3256.87,"to":3261.49,"location":2,"content":"So to look at the genericness or boring response problem."},{"from":3261.49,"to":3267.73,"location":2,"content":"[NOISE] So I think"},{"from":3267.73,"to":3272.23,"location":2,"content":"there are some pretty easy fixes that you can make to,"},{"from":3272.23,"to":3275.47,"location":2,"content":"to a degree ameliorate the boring response problem."},{"from":3275.47,"to":3278.41,"location":2,"content":"Whether you're really getting to the heart of the issue is a different question."},{"from":3278.41,"to":3282.31,"location":2,"content":"But some kind of easy test-time fixes that you can certainly do are for example,"},{"from":3282.31,"to":3286.89,"location":2,"content":"you can just directly up-rate, up-weight rare words during beam search."},{"from":3286.89,"to":3289.68,"location":2,"content":"So you can say, all rare words kind of get a boost to their, uh,"},{"from":3289.68,"to":3291.88,"location":2,"content":"log probabilities and then now we're"},{"from":3291.88,"to":3294.22,"location":2,"content":"more likely to produce them during beam search."},{"from":3294.22,"to":3296.41,"location":2,"content":"Another thing you could do is you could use for example,"},{"from":3296.41,"to":3300.53,"location":2,"content":"a sampling decoding algorithm rather than beam search and we talked about that earlier,"},{"from":3300.53,"to":3302.35,"location":2,"content":"um, or you could use, oh yeah,"},{"from":3302.35,"to":3304.2,"location":2,"content":"you could use softmax temperature as well."},{"from":3304.2,"to":3307.11,"location":2,"content":"That's another thing. So those are"},{"from":3307.11,"to":3312.04,"location":2,"content":"kind of test-time fixes and you could regard those as a kind of late intervention, right?"},{"from":3312.04,"to":3316,"location":2,"content":"So an earlier intervention would be maybe training your model differently."},{"from":3316,"to":3320.01,"location":2,"content":"So I'm calling these kind of conditioning fixes because these fixes kind of relate to,"},{"from":3320.01,"to":3323.97,"location":2,"content":"uh, conditioning your model on something that's gonna help it be less boring."},{"from":3323.97,"to":3326.32,"location":2,"content":"So one example is maybe you should condition"},{"from":3326.32,"to":3328.93,"location":2,"content":"the decoder on some kind of additional context."},{"from":3328.93,"to":3331.15,"location":2,"content":"Uh, so for example, there's some work showing that, you know,"},{"from":3331.15,"to":3333.63,"location":2,"content":"if you're doing chitchat dialogue, then maybe you should, uh,"},{"from":3333.63,"to":3336.28,"location":2,"content":"go and sample some related words that are related to"},{"from":3336.28,"to":3338.71,"location":2,"content":"what the user said and then just kind of attend to them when you"},{"from":3338.71,"to":3341.35,"location":2,"content":"generate and then you're more likely to say something that's kind of content"},{"from":3341.35,"to":3344.08,"location":2,"content":"full and interesting compared to the boring things you were saying before."},{"from":3344.08,"to":3346.87,"location":2,"content":"Ah, another option is you could train"},{"from":3346.87,"to":3350.77,"location":2,"content":"a retrieve-and-refine model rather than a generate-from-scratch model."},{"from":3350.77,"to":3353.44,"location":2,"content":"So by retrieve-and-refine, I mean, uh,"},{"from":3353.44,"to":3355.82,"location":2,"content":"you've- supposing you have some kind of corpus of,"},{"from":3355.82,"to":3357.4,"location":2,"content":"of just general kind of utterances,"},{"from":3357.4,"to":3360.46,"location":2,"content":"things that you could say and then maybe you sample one, uh,"},{"from":3360.46,"to":3361.86,"location":2,"content":"from that test set,"},{"from":3361.86,"to":3363.78,"location":2,"content":"th- the training set,"},{"from":3363.78,"to":3366.89,"location":2,"content":"and then you edit it to fit the current situation."},{"from":3366.89,"to":3370.63,"location":2,"content":"So it turns out that this is a pretty strong method to produce"},{"from":3370.63,"to":3374.8,"location":2,"content":"much more kind of diverse and human-like and interesting utterances, um,"},{"from":3374.8,"to":3379.55,"location":2,"content":"because you can get all of that kind of fine grain detail from the sampled,"},{"from":3379.55,"to":3383.66,"location":2,"content":"ah, utterance and then you edit it as necessary to fit your current situation."},{"from":3383.66,"to":3386.74,"location":2,"content":"So I mean, there are downsides to these kinds of methods like maybe it can be"},{"from":3386.74,"to":3390.14,"location":2,"content":"hard to edit it to actually appropriately fit the situation,"},{"from":3390.14,"to":3392.41,"location":2,"content":"um, but it's certainly a way to effectively get like"},{"from":3392.41,"to":3397.17,"location":2,"content":"some more diversity and, um, interest in that."},{"from":3397.17,"to":3400.81,"location":2,"content":"So on the subject of the repetition problem,"},{"from":3400.81,"to":3403.11,"location":2,"content":"that was another kind of major problem we noticed for,"},{"from":3403.11,"to":3405.79,"location":2,"content":"um, applying seq2seq to, uh, chitchat."},{"from":3405.79,"to":3408.97,"location":2,"content":"Um, again, there are kind of simple solutions and more complex solutions."},{"from":3408.97,"to":3412.15,"location":2,"content":"Um, so a simple solution is you could just block repeating"},{"from":3412.15,"to":3415.91,"location":2,"content":"n-grams during beam search and this is usually really quite effective."},{"from":3415.91,"to":3417.59,"location":2,"content":"And what I mean by that is, uh,"},{"from":3417.59,"to":3419.82,"location":2,"content":"during beam search when you're kind of considering,"},{"from":3419.82,"to":3421.51,"location":2,"content":"you know, what are my K hypotheses?"},{"from":3421.51,"to":3425.11,"location":2,"content":"Which is just kind of the top K in the probability distribution, you say,"},{"from":3425.11,"to":3429.53,"location":2,"content":"well, anything that would constitute a repeating n-gram just gets thrown out."},{"from":3429.53,"to":3431.59,"location":2,"content":"So when I say constitutes a repeating n-gram,"},{"from":3431.59,"to":3434.47,"location":2,"content":"I mean if you did take that word,"},{"from":3434.47,"to":3439.63,"location":2,"content":"would you now be creating a repeating let's say two-gram, bigram and, um,"},{"from":3439.63,"to":3443.5,"location":2,"content":"if we're deciding that we're banning all repeating bigrams or trigrams or whatever,"},{"from":3443.5,"to":3446.62,"location":2,"content":"then you essentially just have to check for every possible word that you might"},{"from":3446.62,"to":3450.7,"location":2,"content":"be looking at in beam search and whether that would create a repeating n-gram."},{"from":3450.7,"to":3452.44,"location":2,"content":"So this works pretty well, I mean,"},{"from":3452.44,"to":3454.78,"location":2,"content":"it's by no means a kind of principled solution, right?"},{"from":3454.78,"to":3457.49,"location":2,"content":"If feels like we should kind of have a better way to learn not to repeat, um,"},{"from":3457.49,"to":3459.79,"location":2,"content":"but as a kind of, uh,"},{"from":3459.79,"to":3462.53,"location":2,"content":"effective hack, I think that works, that works pretty well."},{"from":3462.53,"to":3464.83,"location":2,"content":"So the more complex solutions are,"},{"from":3464.83,"to":3467.92,"location":2,"content":"for example, you can train something called coverage mechanism."},{"from":3467.92,"to":3470.53,"location":2,"content":"Um, so in seq2seq, and this is mostly, uh,"},{"from":3470.53,"to":3473.8,"location":2,"content":"inspired by the machine translation setting, uh,"},{"from":3473.8,"to":3476.44,"location":2,"content":"a coverage mechanism is a kind of objective that prevents"},{"from":3476.44,"to":3478.63,"location":2,"content":"the attention mechanism from attending to"},{"from":3478.63,"to":3481.81,"location":2,"content":"the same words multiple times or too many times."},{"from":3481.81,"to":3483.66,"location":2,"content":"And the intuition here is that, uh,"},{"from":3483.66,"to":3486.59,"location":2,"content":"maybe repetition is caused by repeated attention."},{"from":3486.59,"to":3488.62,"location":2,"content":"So if you attend to the same things many times,"},{"from":3488.62,"to":3489.97,"location":2,"content":"then maybe you're gonna repeat,"},{"from":3489.97,"to":3491.61,"location":2,"content":"you know, the same output many times."},{"from":3491.61,"to":3493.69,"location":2,"content":"So if you prevent the repeated attention,"},{"from":3493.69,"to":3495.28,"location":2,"content":"you prevent the repeated output."},{"from":3495.28,"to":3498.19,"location":2,"content":"So this does work pretty well but it's definitely,"},{"from":3498.19,"to":3501.49,"location":2,"content":"um, more of a complex thing to implement,"},{"from":3501.49,"to":3503.64,"location":2,"content":"it's less convenient and,"},{"from":3503.64,"to":3505.12,"location":2,"content":"um, I don't know,"},{"from":3505.12,"to":3508.07,"location":2,"content":"in some settings, it does seem like the simple solution is,"},{"from":3508.07,"to":3509.53,"location":2,"content":"uh, easier and works just as well."},{"from":3509.53,"to":3512.74,"location":2,"content":"Uh, so other complex solutions"},{"from":3512.74,"to":3515.8,"location":2,"content":"might be you could define a training objective to discourage repetition."},{"from":3515.8,"to":3518.32,"location":2,"content":"Uh, this cou- you could try to, um,"},{"from":3518.32,"to":3521.13,"location":2,"content":"define something differentiable but one of the,"},{"from":3521.13,"to":3525.6,"location":2,"content":"the difficulties there is that because you're training with a teacher forcing, right?"},{"from":3525.6,"to":3527.05,"location":2,"content":"Where you're always like looking at the,"},{"from":3527.05,"to":3528.43,"location":2,"content":"the gold inputs so far,"},{"from":3528.43,"to":3530.7,"location":2,"content":"then you never really do the thing where"},{"from":3530.7,"to":3533.01,"location":2,"content":"you generate your own output and start repeating yourself."},{"from":3533.01,"to":3535.84,"location":2,"content":"So it's kind of hard to define the penalty in that situation."},{"from":3535.84,"to":3538.35,"location":2,"content":"So maybe this needs to be a kind of non-differentiable function."},{"from":3538.35,"to":3540.26,"location":2,"content":"So kind of like how,"},{"from":3540.26,"to":3543.74,"location":2,"content":"um, the Paul et al paper was, uh,"},{"from":3543.74,"to":3546.25,"location":2,"content":"optimizing for ROUGE, maybe we kind of, uh,"},{"from":3546.25,"to":3551.45,"location":2,"content":"optimize for not repeating which is a discrete function of the input."},{"from":3551.45,"to":3554.43,"location":2,"content":"Uh, I'm going to skip ahead to storytelling."},{"from":3554.43,"to":3556.2,"location":2,"content":"So in storytelling, uh,"},{"from":3556.2,"to":3559.01,"location":2,"content":"there's a lot of interesting neural storytelling work going on right now."},{"from":3559.01,"to":3562.28,"location":2,"content":"And most of it uses some kind of prompt to write a story."},{"from":3562.28,"to":3564.61,"location":2,"content":"So for example, uh,"},{"from":3564.61,"to":3568.11,"location":2,"content":"writing a story given an image or given a writing prompt"},{"from":3568.11,"to":3572.72,"location":2,"content":"or writing the next sentence of the story given the story so far."},{"from":3572.72,"to":3577.64,"location":2,"content":"So, uh, here's an example of generating a story from an image."},{"from":3577.64,"to":3580.36,"location":2,"content":"And what's interesting here is that we have this image which"},{"from":3580.36,"to":3582.94,"location":2,"content":"is a picture of what appears to be an explosion and"},{"from":3582.94,"to":3584.74,"location":2,"content":"then here you have"},{"from":3584.74,"to":3588.47,"location":2,"content":"a story about the image but written in the style of Taylor Swift lyrics."},{"from":3588.47,"to":3592.01,"location":2,"content":"So it says, you have to be the only light bulb in the night sky I thought,"},{"from":3592.01,"to":3595.22,"location":2,"content":"oh god, it's so dark out of me that I missed you, I promise."},{"from":3595.22,"to":3598.69,"location":2,"content":"And what's interesting here is that there wasn't any straightforward, supervised,"},{"from":3598.69,"to":3602.62,"location":2,"content":"you know, image-captioning data set of explosions and Taylor Swift lyrics."},{"from":3602.62,"to":3605.89,"location":2,"content":"Um, they kind of learned this, uh, separately."},{"from":3605.89,"to":3612.22,"location":2,"content":"So how they did this is that they used a kind of common sentence encoding space."},{"from":3612.22,"to":3615.16,"location":2,"content":"So they used this particular kind of sentence encoding called"},{"from":3615.16,"to":3618.2,"location":2,"content":"skip-thought vectors and then they trained,"},{"from":3618.2,"to":3621.88,"location":2,"content":"um, this COCO image-captioning, uh,"},{"from":3621.88,"to":3624.79,"location":2,"content":"system to go from the image to the encoding of"},{"from":3624.79,"to":3628.11,"location":2,"content":"the sentence and then separately they also trained,"},{"from":3628.11,"to":3630.37,"location":2,"content":"uh, a language model, a conditional language model to go from"},{"from":3630.37,"to":3633.01,"location":2,"content":"the sentence-encoding to the Taylor Swift lyrics."},{"from":3633.01,"to":3635.23,"location":2,"content":"And then because you had this shared encoding space,"},{"from":3635.23,"to":3638.3,"location":2,"content":"you can now put the two together and then go from the picture,"},{"from":3638.3,"to":3641.05,"location":2,"content":"to the embedding, to the Taylor Swift style output,"},{"from":3641.05,"to":3644.22,"location":2,"content":"which I think is pretty, pretty amazing."},{"from":3644.22,"to":3646.6,"location":2,"content":"Wow, I've really lost, lost track of the time."},{"from":3646.6,"to":3648.25,"location":2,"content":"So I, I think I have to hurry up quite a lot."},{"from":3648.25,"to":3655.14,"location":2,"content":"So, um, we've got some really impressive story,"},{"from":3655.14,"to":3657.9,"location":2,"content":"generation systems, recently, um,"},{"from":3657.9,"to":3660.8,"location":2,"content":"and this is an example of,"},{"from":3660.8,"to":3662.58,"location":2,"content":"uh, a system which,"},{"from":3662.58,"to":3663.84,"location":2,"content":"uh, prepares a new data set,"},{"from":3663.84,"to":3665.74,"location":2,"content":"where you write a story given a prompt,"},{"from":3665.74,"to":3668.16,"location":2,"content":"and they made this very impressive,"},{"from":3668.16,"to":3670.9,"location":2,"content":"very beefed-up, uh, convolutional language model,"},{"from":3670.9,"to":3673.97,"location":2,"content":"seq-to-seq system that generates the story given the input."},{"from":3673.97,"to":3675.64,"location":2,"content":"I'm not gonna go through all these details,"},{"from":3675.64,"to":3678.07,"location":2,"content":"but I encourage you if you want to check out, uh,"},{"from":3678.07,"to":3680.95,"location":2,"content":"what's the state of the art in story generation, you should check this out."},{"from":3680.95,"to":3683.11,"location":2,"content":"There's a lot of different interesting things going on with"},{"from":3683.11,"to":3685.78,"location":2,"content":"very fancy attention and convolutions and so on,"},{"from":3685.78,"to":3689.7,"location":2,"content":"and they managed to generate some really interesting, um, impressive stories."},{"from":3689.7,"to":3691.75,"location":2,"content":"So here, if you look at this example,"},{"from":3691.75,"to":3696.28,"location":2,"content":"we've got some really interesting, um, kind of,"},{"from":3696.28,"to":3699.32,"location":2,"content":"uh, story generation that's kind of diverse, it's non-generic,"},{"from":3699.32,"to":3701.32,"location":2,"content":"it's stylistically dramatic which is good,"},{"from":3701.32,"to":3702.93,"location":2,"content":"and is related to the prompts."},{"from":3702.93,"to":3706.33,"location":2,"content":"Um, but I think you can see here kind of the limits of what"},{"from":3706.33,"to":3709.86,"location":2,"content":"the state of the art story generation system can do which is that- um,"},{"from":3709.86,"to":3711.72,"location":2,"content":"although it's kind of in style,"},{"from":3711.72,"to":3714.63,"location":2,"content":"it's mostly kind of atmospheric and descriptive."},{"from":3714.63,"to":3716.14,"location":2,"content":"It's not really moving the plot forward."},{"from":3716.14,"to":3717.94,"location":2,"content":"There's no kind of events here, right?"},{"from":3717.94,"to":3722.3,"location":2,"content":"Um, so the problem is it gets even worse when you generate for longer."},{"from":3722.3,"to":3724.18,"location":2,"content":"When you generate a long, a long text,"},{"from":3724.18,"to":3728.76,"location":2,"content":"then it will mostly just stay on the same idea without moving forward with new ideas."},{"from":3728.76,"to":3731.5,"location":2,"content":"Okay. So I'm gonna skip forward a lot and,"},{"from":3731.5,"to":3733.51,"location":2,"content":"uh, sorry, ought to have planned better."},{"from":3733.51,"to":3735.16,"location":2,"content":"There's a lot of information here which you wanna check"},{"from":3735.16,"to":3737.34,"location":2,"content":"out about poetry generation and other things."},{"from":3737.34,"to":3739.69,"location":2,"content":"I'm going to skip ahead because I want to get to"},{"from":3739.69,"to":3743.32,"location":2,"content":"the NLG evaluation section because that's pretty important."},{"from":3743.32,"to":3747.66,"location":2,"content":"So, um, we've talked about Automatic Evaluation Metrics fr NLG,"},{"from":3747.66,"to":3750.76,"location":2,"content":"and we know that these words overlap based metrics, such as BLEU,"},{"from":3750.76,"to":3752.16,"location":2,"content":"and ROUGE, and METEOR, uh,"},{"from":3752.16,"to":3754.36,"location":2,"content":"we know they're not ideal for machine translation."},{"from":3754.36,"to":3757.78,"location":2,"content":"Ah, they're kind of even worse for summarization mostly"},{"from":3757.78,"to":3761.77,"location":2,"content":"because summarization is even more open-ended than machine translation."},{"from":3761.77,"to":3764.17,"location":2,"content":"And that means that having this kind of rigid notion,"},{"from":3764.17,"to":3765.84,"location":2,"content":"if you've got to match the N-grams,"},{"from":3765.84,"to":3767.38,"location":2,"content":"is even less useful."},{"from":3767.38,"to":3769.93,"location":2,"content":"And then for something even more open-ended like dialogue,"},{"from":3769.93,"to":3771.58,"location":2,"content":"then it's just kind of a disaster."},{"from":3771.58,"to":3774.22,"location":2,"content":"It's not even a metric that gives you a good signal at all,"},{"from":3774.22,"to":3778.05,"location":2,"content":"and this also applies to anything else open-ended, like story generation."},{"from":3778.05,"to":3781.22,"location":2,"content":"So it's been shown, and you can check out the paper at the bottom,"},{"from":3781.22,"to":3785.13,"location":2,"content":"that word overlap metrics are just not a good fit for dialogue."},{"from":3785.13,"to":3787.48,"location":2,"content":"So the orange box is showing you, uh,"},{"from":3787.48,"to":3793.86,"location":2,"content":"some plots of the correlation between human score on a dialog class and BLEU-2,"},{"from":3793.86,"to":3795.41,"location":2,"content":"some variation of BLEU."},{"from":3795.41,"to":3798.49,"location":2,"content":"And the prob- the problem here is you're not seeing much of a correlation at all, right?"},{"from":3798.49,"to":3801.42,"location":2,"content":"It seems that particularly on this dialogue setting, ah,"},{"from":3801.42,"to":3803.37,"location":2,"content":"the correlation between the BLEU metric and"},{"from":3803.37,"to":3806.57,"location":2,"content":"the human judgment of whether it's a good dialogue response is,"},{"from":3806.57,"to":3808.02,"location":2,"content":"uh, the correlation is- I mean,"},{"from":3808.02,"to":3809.04,"location":2,"content":"it looks kind of non-existent."},{"from":3809.04,"to":3810.72,"location":2,"content":"It's at least very weak."},{"from":3810.72,"to":3815.12,"location":2,"content":"So that's pretty unfortunate and there's some other papers that show much the same thing."},{"from":3815.12,"to":3816.64,"location":2,"content":"So you might think, \"Well,"},{"from":3816.64,"to":3818.92,"location":2,"content":"what other automatic metrics can we use?"},{"from":3818.92,"to":3820.6,"location":2,"content":"\"What about perplexity?"},{"from":3820.6,"to":3825.82,"location":2,"content":"Um, so perplexity certainly captures how powerful your language model is,"},{"from":3825.82,"to":3828.09,"location":2,"content":"but it doesn't tell you anything about generation."},{"from":3828.09,"to":3831.97,"location":2,"content":"So for example, if your deca- decoding algorithm is bad in some way,"},{"from":3831.97,"to":3834.7,"location":2,"content":"then perplexity is not gonna tell you anything about that, right?"},{"from":3834.7,"to":3837.64,"location":2,"content":"Because decoding is something you apply to your trained language model."},{"from":3837.64,"to":3840.13,"location":2,"content":"Perplexity can tell if you've got a strong language model or not,"},{"from":3840.13,"to":3841.84,"location":2,"content":"but it's not gonna tell you, um,"},{"from":3841.84,"to":3844.16,"location":2,"content":"necessarily how good your generation is."},{"from":3844.16,"to":3847.33,"location":2,"content":"So some other thoughts you might have about automatic evaluation are,"},{"from":3847.33,"to":3849.46,"location":2,"content":"well, what about word embedding based metrics?"},{"from":3849.46,"to":3852.14,"location":2,"content":"Uh, so the main idea with word embedding based metrics,"},{"from":3852.14,"to":3854.51,"location":2,"content":"uh, you want to compute the similarity of the,"},{"from":3854.51,"to":3858.22,"location":2,"content":"the word embeddings or maybe the average of the word embeddings across a sentence,"},{"from":3858.22,"to":3860.53,"location":2,"content":"not just the overlap of the words themselves."},{"from":3860.53,"to":3862.78,"location":2,"content":"Um, so the idea is that rather than just being"},{"from":3862.78,"to":3865.51,"location":2,"content":"very strict and saying only the exact same word counts,"},{"from":3865.51,"to":3868.89,"location":2,"content":"you say, \"Well, if the words are similar and in word embedding space, then they count.\""},{"from":3868.89,"to":3871.9,"location":2,"content":"So this is certainly more flexible, but unfortunately, uh,"},{"from":3871.9,"to":3874.36,"location":2,"content":"the same paper I showed before shows that this doesn't"},{"from":3874.36,"to":3877.32,"location":2,"content":"correlate well either with human judgments of quality,"},{"from":3877.32,"to":3879.95,"location":2,"content":"at least for the- the dialogue task they are looking at."},{"from":3879.95,"to":3883.28,"location":2,"content":"So here, the middle column is showing the correlation between human,"},{"from":3883.28,"to":3887.51,"location":2,"content":"judgments, and some kind of average of word embedding based metric."},{"from":3887.51,"to":3889.54,"location":2,"content":"So, um, yeah, that doesn't look great either,"},{"from":3889.54,"to":3891.4,"location":2,"content":"not a great correlation."},{"from":3891.4,"to":3894.97,"location":2,"content":"So if we have no automatic metrics to adequately"},{"from":3894.97,"to":3898.43,"location":2,"content":"capture overall quality for natural language generation,"},{"from":3898.43,"to":3900.46,"location":2,"content":"um, what, what can we do instead?"},{"from":3900.46,"to":3902.59,"location":2,"content":"So I think often the strategy is,"},{"from":3902.59,"to":3906.28,"location":2,"content":"you end up defining some more kind of focused automatic metrics to"},{"from":3906.28,"to":3910.7,"location":2,"content":"capture the particular aspects of the generated text that you might be interested in."},{"from":3910.7,"to":3913.64,"location":2,"content":"Um, so for example, you might be interested in, uh, fluency,"},{"from":3913.64,"to":3915.54,"location":2,"content":"and you can compute that by just kind of running"},{"from":3915.54,"to":3918.73,"location":2,"content":"a well-trained language model over your text and generating the probability,"},{"from":3918.73,"to":3923.51,"location":2,"content":"and that's kind of a proxy for how well it's written, you know, good,  fluent, grammatical text."},{"from":3923.51,"to":3928,"location":2,"content":"Um, if you're particularly interested in maybe generating text in a particular style,"},{"from":3928,"to":3929.86,"location":2,"content":"then you could ta- take a language model that's"},{"from":3929.86,"to":3932.18,"location":2,"content":"trained on the corpus representing that style,"},{"from":3932.18,"to":3935.11,"location":2,"content":"and now the probability tells you not only is it a good text,"},{"from":3935.11,"to":3936.68,"location":2,"content":"but is it in the right style."},{"from":3936.68,"to":3938.88,"location":2,"content":"Um, there are some other things as well that are like,"},{"from":3938.88,"to":3941.18,"location":2,"content":"you know, diversity, um,"},{"from":3941.18,"to":3943.9,"location":2,"content":"and you can can that pretty easily by just having some statistics about,"},{"from":3943.9,"to":3945.94,"location":2,"content":"you know, how much you're using rare words."},{"from":3945.94,"to":3948.25,"location":2,"content":"Um, relevance to input,"},{"from":3948.25,"to":3950.71,"location":2,"content":"you can kind of compute a similarity score with the input,"},{"from":3950.71,"to":3952.55,"location":2,"content":"and there are just some simple things like, you know,"},{"from":3952.55,"to":3955.48,"location":2,"content":"length and repetition that you surely can count, and yes,"},{"from":3955.48,"to":3958.07,"location":2,"content":"it doesn't tell you overall the overall quality,"},{"from":3958.07,"to":3960.53,"location":2,"content":"but these things are worth measuring."},{"from":3960.53,"to":3962.41,"location":2,"content":"So I think my main point is that yes,"},{"from":3962.41,"to":3964.96,"location":2,"content":"we have a really difficult situation with NLG evaluation."},{"from":3964.96,"to":3966.4,"location":2,"content":"There's no kind of overall metric."},{"from":3966.4,"to":3968.86,"location":2,"content":"Often, they capture this overall quality."},{"from":3968.86,"to":3971.36,"location":2,"content":"Um, but if you measure lots of these things,"},{"from":3971.36,"to":3976.07,"location":2,"content":"then they certainly can help you track some important things that you should know."},{"from":3976.07,"to":3981.89,"location":2,"content":"So we talked about how automatic evaluation metrics for NLG are really tough."},{"from":3981.89,"to":3983.71,"location":2,"content":"So let's talk about human evaluation."},{"from":3983.71,"to":3987.4,"location":2,"content":"Uh, human judgments are regarded as the gold standard, right?"},{"from":3987.4,"to":3990.86,"location":2,"content":"But we already know that human evaluation is slow and expensive,"},{"from":3990.86,"to":3994.16,"location":2,"content":"uh, but are those the only problems with human eval?"},{"from":3994.16,"to":3996.91,"location":2,"content":"Let's suppose that you do have access, uh, to,"},{"from":3996.91,"to":4000.06,"location":2,"content":"let's say, the time or money you need to do human evaluations."},{"from":4000.06,"to":4002.11,"location":2,"content":"Um, does that solve all your problems?"},{"from":4002.11,"to":4003.48,"location":2,"content":"Suppose you have unlimited human eval,"},{"from":4003.48,"to":4004.98,"location":2,"content":"does that actually solve your problems?"},{"from":4004.98,"to":4007.59,"location":2,"content":"And my answer is, uh, no."},{"from":4007.59,"to":4009.63,"location":2,"content":"And this is kinda from personal experience."},{"from":4009.63,"to":4013.59,"location":2,"content":"Um, conducting human evaluation in itself is very difficult to get right."},{"from":4013.59,"to":4017.28,"location":2,"content":"It's not easy at all, and this is partially because humans do a lot of weird things."},{"from":4017.28,"to":4019.91,"location":2,"content":"Humans, uh, unlike a metric, uh,"},{"from":4019.91,"to":4022.13,"location":2,"content":"an automatic metric, they're inconsistent,"},{"from":4022.13,"to":4023.67,"location":2,"content":"they could be illogical."},{"from":4023.67,"to":4025.29,"location":2,"content":"Sometimes, they just get bored of your task,"},{"from":4025.29,"to":4026.76,"location":2,"content":"and they don't really pay attention anymore."},{"from":4026.76,"to":4029.58,"location":2,"content":"Uh, they can misinterpret the question you asked,"},{"from":4029.58,"to":4032.4,"location":2,"content":"and sometimes they do things they can't really explain why they did it."},{"from":4032.4,"to":4034.44,"location":2,"content":"So, um, as a kind of case study of"},{"from":4034.44,"to":4036.54,"location":2,"content":"this I'm going to tell you about, um,"},{"from":4036.54,"to":4038.01,"location":2,"content":"a project I did where I was,"},{"from":4038.01,"to":4039.54,"location":2,"content":"uh, building some chatbots,"},{"from":4039.54,"to":4043.49,"location":2,"content":"and it turned out that the human evaluation was kind of the hardest part of the project."},{"from":4043.49,"to":4046.23,"location":2,"content":"So I was trying to build these chatbots for the Persona-Chat data"},{"from":4046.23,"to":4049.64,"location":2,"content":"set and in particular investigating controllability."},{"from":4049.64,"to":4052.88,"location":2,"content":"So we're trying to control aspects of the generated texts such as, you know,"},{"from":4052.88,"to":4054.05,"location":2,"content":"whether you repeat itself,"},{"from":4054.05,"to":4055.39,"location":2,"content":"how generic you are,"},{"from":4055.39,"to":4057.61,"location":2,"content":"kind of these same problems that we noted before."},{"from":4057.61,"to":4060.18,"location":2,"content":"So we built these models that control, you know,"},{"from":4060.18,"to":4062.09,"location":2,"content":"specificity of what we're saying and"},{"from":4062.09,"to":4064.74,"location":2,"content":"how related what we're saying is to what the user said."},{"from":4064.74,"to":4066.09,"location":2,"content":"So here you can see that,"},{"from":4066.09,"to":4068.88,"location":2,"content":"you know, uh, our partner said something like, \"Yes,"},{"from":4068.88,"to":4071.74,"location":2,"content":"I'm studying law at the moment,\" and we can kind of control-"},{"from":4071.74,"to":4074.72,"location":2,"content":"turn this control knob that makes us say something very generic like,"},{"from":4074.72,"to":4077.01,"location":2,"content":"\"Oh,\" and then like 20 dots or something"},{"from":4077.01,"to":4079.47,"location":2,"content":"just completely bonkers that's just all the rare words you know."},{"from":4079.47,"to":4081.51,"location":2,"content":"And there's like a sweet- a sweet spot between what you say,"},{"from":4081.51,"to":4083.95,"location":2,"content":"\"That sounds like a lot of fun. How long have you been studying?\""},{"from":4083.95,"to":4086.95,"location":2,"content":"And then similarly, we have a knob we can turn to,"},{"from":4086.95,"to":4091.26,"location":2,"content":"uh, determine how semantically related what we say is to what, what they said."},{"from":4091.26,"to":4093.54,"location":2,"content":"So, um, you know, that's kind of interesting."},{"from":4093.54,"to":4096.61,"location":2,"content":"It's, it's a way to control the output of the, uh, NLG system."},{"from":4096.61,"to":4099.52,"location":2,"content":"But actually, I want to tell you about how the human evaluation was so difficult,"},{"from":4099.52,"to":4102.89,"location":2,"content":"so we have these systems that we wanted to generate using human eval."},{"from":4102.89,"to":4106.23,"location":2,"content":"So the question is, how do you ask for the human quality judgments here?"},{"from":4106.23,"to":4109.8,"location":2,"content":"Uh, you can ask kind of simple overall quality questions,"},{"from":4109.8,"to":4111.98,"location":2,"content":"like, you know, how well does the conversation go?"},{"from":4111.98,"to":4113.67,"location":2,"content":"Was- was the user engaging?"},{"from":4113.67,"to":4114.99,"location":2,"content":"Um, or maybe comparative,"},{"from":4114.99,"to":4118.6,"location":2,"content":"Which of these users gave the best response? Uh, questions like this."},{"from":4118.6,"to":4120.33,"location":2,"content":"And, you know, we tried a lot of them,"},{"from":4120.33,"to":4123,"location":2,"content":"but there were just major problems with all of them."},{"from":4123,"to":4126.96,"location":2,"content":"Like, these questions are necessarily very subjective and also,"},{"from":4126.96,"to":4129.15,"location":2,"content":"the different respondents have different expectations,"},{"from":4129.15,"to":4130.62,"location":2,"content":"and this affects their judgments."},{"from":4130.62,"to":4133.69,"location":2,"content":"So for example, if you ask, do you think this user is a human or a bot?"},{"from":4133.69,"to":4135.65,"location":2,"content":"Then, well, that depends entirely on"},{"from":4135.65,"to":4140.22,"location":2,"content":"this respondents' knowledge of bots or opinion of bots and what they think they can do."},{"from":4140.22,"to":4143.94,"location":2,"content":"Another example is, you'd have kind of catastrophic misunderstanding of the question."},{"from":4143.94,"to":4145.14,"location":2,"content":"So for example, if we ask,"},{"from":4145.14,"to":4147.68,"location":2,"content":"was this user- was this chatbot engaging?"},{"from":4147.68,"to":4149.48,"location":2,"content":"Then someone responded saying, \"Yup,"},{"from":4149.48,"to":4151.02,"location":2,"content":"it was engaging because it always wrote back\","},{"from":4151.02,"to":4152.31,"location":2,"content":"which clearly isn't what we meant."},{"from":4152.31,"to":4154.6,"location":2,"content":"We meant like are they an engaging conversation partner,"},{"from":4154.6,"to":4156.77,"location":2,"content":"but they took a very literal assumption,"},{"from":4156.77,"to":4159.07,"location":2,"content":"uh, of, of what engaging means."},{"from":4159.07,"to":4162.98,"location":2,"content":"So the problem here is that overall quality depends on many underlying factors,"},{"from":4162.98,"to":4165.39,"location":2,"content":"and it's pretty hard to kind of find a single,"},{"from":4165.39,"to":4168.72,"location":2,"content":"overall question that captures just overall quality."},{"from":4168.72,"to":4171.03,"location":2,"content":"So we ended up doing this, we ended up breaking this down"},{"from":4171.03,"to":4174.27,"location":2,"content":"into lots more kind of factors of quality."},{"from":4174.27,"to":4176.2,"location":2,"content":"So, uh, the way we saw it is that,"},{"from":4176.2,"to":4179.82,"location":2,"content":"you have maybe these kind of overall measures of quality of the chatbot,"},{"from":4179.82,"to":4181.38,"location":2,"content":"such as how engaging was it,"},{"from":4181.38,"to":4183.14,"location":2,"content":"how enjoyable was it to talk to,"},{"from":4183.14,"to":4185.69,"location":2,"content":"and kind of maybe how convincing was it that it was human."},{"from":4185.69,"to":4187.23,"location":2,"content":"And then below those,"},{"from":4187.23,"to":4189.81,"location":2,"content":"we kind of broke down as these more low level, uh,"},{"from":4189.81,"to":4191.69,"location":2,"content":"components of quality such as,"},{"from":4191.69,"to":4193.29,"location":2,"content":"you know, uh, were you interesting?"},{"from":4193.29,"to":4195.15,"location":2,"content":"Were you li- showing that you were listening?"},{"from":4195.15,"to":4196.94,"location":2,"content":"Were you asking enough questions and so on?"},{"from":4196.94,"to":4199.62,"location":2,"content":"And then below that, we had these kind of controllable attributes which"},{"from":4199.62,"to":4202.47,"location":2,"content":"were the knobs that we were turning and then the goal was to figure out,"},{"from":4202.47,"to":4204.52,"location":2,"content":"um, how these things affected the output."},{"from":4204.52,"to":4209.74,"location":2,"content":"Um, so let's see."},{"from":4209.74,"to":4213.12,"location":2,"content":"Um, so we had a bunch of findings here, and I think,"},{"from":4213.12,"to":4216.44,"location":2,"content":"maybe the ones which I will highlight were,"},{"from":4216.44,"to":4218.06,"location":2,"content":"uh, these two kind of in the middle."},{"from":4218.06,"to":4219.98,"location":2,"content":"So the overall metric engagingness,"},{"from":4219.98,"to":4223.09,"location":2,"content":"which means enjoyment, that was really easy to maximize."},{"from":4223.09,"to":4224.42,"location":2,"content":"It turned out, uh,"},{"from":4224.42,"to":4228.3,"location":2,"content":"our bots managed to get near human performance in terms of engagingness."},{"from":4228.3,"to":4230.73,"location":2,"content":"Um, but the overall metric humanness,"},{"from":4230.73,"to":4232.44,"location":2,"content":"that is the kind of Turing test metric,"},{"from":4232.44,"to":4234.4,"location":2,"content":"that was not at all easy to maximize."},{"from":4234.4,"to":4235.92,"location":2,"content":"All of our bots were way,"},{"from":4235.92,"to":4238.02,"location":2,"content":"way below humans in terms of humanness, right?"},{"from":4238.02,"to":4240.3,"location":2,"content":"So we were not at all convincing of being human,"},{"from":4240.3,"to":4241.78,"location":2,"content":"and this is kind of interesting, right?"},{"from":4241.78,"to":4244.03,"location":2,"content":"Like, we were as enjoyable as talk to as humans,"},{"from":4244.03,"to":4246.63,"location":2,"content":"but we were clearly not human, right?"},{"from":4246.63,"to":4250.24,"location":2,"content":"So like, humanness is not the same thing as conversational quality."},{"from":4250.24,"to":4252.61,"location":2,"content":"And one of the interesting things we found in this,"},{"from":4252.61,"to":4255.39,"location":2,"content":"um, study, where we not only evaluated our chatbots,"},{"from":4255.39,"to":4257.65,"location":2,"content":"we also actually got humans to evaluate each other,"},{"from":4257.65,"to":4260.93,"location":2,"content":"was that, um, humans are sub-optimal conversationalists."},{"from":4260.93,"to":4265.17,"location":2,"content":"Uh, they scored pretty poorly on interestingness, fluency, listening."},{"from":4265.17,"to":4266.9,"location":2,"content":"They didn't ask each other enough questions,"},{"from":4266.9,"to":4269.46,"location":2,"content":"and this is kind of the reason why we managed to like approach"},{"from":4269.46,"to":4273.15,"location":2,"content":"human performance in kind of enjoyableness to talk to you because we just,"},{"from":4273.15,"to":4276.5,"location":2,"content":"for example, turned up the question asking knob, asked more questions,"},{"from":4276.5,"to":4279.88,"location":2,"content":"and people responded really well to that because people like talking about themselves."},{"from":4279.88,"to":4282.29,"location":2,"content":"So, um, yeah."},{"from":4282.29,"to":4283.61,"location":2,"content":"I think this is kind of interesting, right?"},{"from":4283.61,"to":4286.82,"location":2,"content":"Because it shows that there is no obvious just one question to ask, right?"},{"from":4286.82,"to":4287.99,"location":2,"content":"Because if you just seemed, \"Oh,"},{"from":4287.99,"to":4291.87,"location":2,"content":"the one question to ask is clearly engagingness or it's clearly humanness,"},{"from":4291.87,"to":4295.36,"location":2,"content":"then we would have gotten completely different reads on how well we were doing, right?"},{"from":4295.36,"to":4301.73,"location":2,"content":"Whereas asking these multiple questions kind of gives you more of an overview."},{"from":4301.73,"to":4306.78,"location":2,"content":"I am going to skip this just because there's not a lot of time."},{"from":4306.78,"to":4308.6,"location":2,"content":"Okay. So, here's the final section."},{"from":4308.6,"to":4311.67,"location":2,"content":"Uh, this is my kind of wrap-up thoughts on NLG research,"},{"from":4311.67,"to":4314.34,"location":2,"content":"the current trends and where we're going in the future."},{"from":4314.34,"to":4319.02,"location":2,"content":"So, here's kind of three exciting current trends to identify in NLG."},{"from":4319.02,"to":4320.98,"location":2,"content":"And of course your mileage may vary,"},{"from":4320.98,"to":4322.86,"location":2,"content":"you might think that other things are more interesting."},{"from":4322.86,"to":4325.11,"location":2,"content":"So, uh, the ones which I was thinking about, are"},{"from":4325.11,"to":4328.64,"location":2,"content":"firstly incorporating discrete latent variables into NLG."},{"from":4328.64,"to":4331.14,"location":2,"content":"Um, so, you should go check out"},{"from":4331.14,"to":4333.59,"location":2,"content":"the slides I skipped over because there were some examples of this."},{"from":4333.59,"to":4336.96,"location":2,"content":"But the idea is that with some tasks such as for example"},{"from":4336.96,"to":4339.36,"location":2,"content":"storytelling or task oriented dialogue"},{"from":4339.36,"to":4341.1,"location":2,"content":"where you're trying to actually get something done."},{"from":4341.1,"to":4342.81,"location":2,"content":"Um, you probably want a more kind of"},{"from":4342.81,"to":4345.54,"location":2,"content":"concrete hard notion of the things that you're talking about"},{"from":4345.54,"to":4350.16,"location":2,"content":"like you know, entities and people and events and negotiation and so on."},{"from":4350.16,"to":4353.81,"location":2,"content":"So, uh, there's, there's mentioning what kind of modeling"},{"from":4353.81,"to":4358.86,"location":2,"content":"these discrete latent variables inside these continuous, uh, NLG methods."},{"from":4358.86,"to":4362.52,"location":2,"content":"The second one is alternatives to strict left to right generation."},{"from":4362.52,"to":4364.44,"location":2,"content":"And I'm really sorry [LAUGHTER] I skipped over so many things."},{"from":4364.44,"to":4367.02,"location":2,"content":"Um, so, there's some interesting work recently in trying"},{"from":4367.02,"to":4369.81,"location":2,"content":"to generate text in ways other than left to right."},{"from":4369.81,"to":4371.16,"location":2,"content":"So, for example there's some kind of"},{"from":4371.16,"to":4375.73,"location":2,"content":"parallel generation stuff or maybe writing something and iteratively refining it, uh,"},{"from":4375.73,"to":4379.81,"location":2,"content":"there's also the idea of kind of top-down generation, um, for"},{"from":4379.81,"to":4382.8,"location":2,"content":"especially longer pieces of text like maybe tried to decide the contents"},{"from":4382.8,"to":4386.39,"location":2,"content":"of each of the sentences separately before uh, writing the words."},{"from":4386.39,"to":4388.62,"location":2,"content":"And then a third one is like"},{"from":4388.62,"to":4391.53,"location":2,"content":"alternatives to maximum likelihood training with teacher forcing."},{"from":4391.53,"to":4394.32,"location":2,"content":"So, to remind you, a maximum likelihood training with teacher forcing is"},{"from":4394.32,"to":4396.42,"location":2,"content":"just the standard method of training"},{"from":4396.42,"to":4399.21,"location":2,"content":"a language model that we've been telling you about in the class so far."},{"from":4399.21,"to":4400.76,"location":2,"content":"Um, so, you know,"},{"from":4400.76,"to":4403.2,"location":2,"content":"there's some interesting work on looking at more kind of holistic,"},{"from":4403.2,"to":4405.73,"location":2,"content":"um, sentence level rather than word level objectives."},{"from":4405.73,"to":4407.55,"location":2,"content":"Uh, so, unfortunately I ran out of time with"},{"from":4407.55,"to":4409.86,"location":2,"content":"this slide, and I didn't have time to put the references in but I will"},{"from":4409.86,"to":4411.99,"location":2,"content":"put the references in later and it"},{"from":4411.99,"to":4414.94,"location":2,"content":"will be on the course website so you can go check them out later."},{"from":4414.94,"to":4419.82,"location":2,"content":"Okay. So, as a kind of overview, NLG research, where are we and where are we going?"},{"from":4419.82,"to":4421.95,"location":2,"content":"Um, so my metaphor is I think that"},{"from":4421.95,"to":4426.21,"location":2,"content":"about five years ago NLP and deep learning research was a kind of a Wild West."},{"from":4426.21,"to":4430.77,"location":2,"content":"Right? Like everything was new and um, we were unsure,"},{"from":4430.77,"to":4432.3,"location":2,"content":"NLP research weren't sure what kind of what"},{"from":4432.3,"to":4435.66,"location":2,"content":"the new research landscape was because uh, you know,"},{"from":4435.66,"to":4438.65,"location":2,"content":"uh, neural methods kind of changed machine translation a lot,"},{"from":4438.65,"to":4441.68,"location":2,"content":"looked like they might change other areas but it was uncertain how much."},{"from":4441.68,"to":4444.69,"location":2,"content":"Um, but these days you know five years later,"},{"from":4444.69,"to":4446.48,"location":2,"content":"um, it's a lot less wild."},{"from":4446.48,"to":4449.13,"location":2,"content":"I'd say, you know things are settled down a lot kind of"},{"from":4449.13,"to":4453.14,"location":2,"content":"standard practices have emerged and sure there's still a lot of things changing."},{"from":4453.14,"to":4455.24,"location":2,"content":"Um, but you know there's more people in the community,"},{"from":4455.24,"to":4456.5,"location":2,"content":"there's more standard practices,"},{"from":4456.5,"to":4458.24,"location":2,"content":"we have things like TensorFlow and PyTorch."},{"from":4458.24,"to":4460.09,"location":2,"content":"So, you don't have to take up gradients anymore."},{"from":4460.09,"to":4462.66,"location":2,"content":"So, I'd say things are a lot less wild now"},{"from":4462.66,"to":4466.37,"location":2,"content":"but I would say NLG does seem to be one of the wildest parts"},{"from":4466.37,"to":4469.88,"location":2,"content":"remaining and part of the reasons for that is because of"},{"from":4469.88,"to":4473.91,"location":2,"content":"the lack of evaluation metrics that makes it so difficult to tell what we're doing."},{"from":4473.91,"to":4477.26,"location":2,"content":"It's, uh, quite difficult to identify like what are the main methods that are"},{"from":4477.26,"to":4481.88,"location":2,"content":"working when we don't have any metrics that can clearly tell us what's going on."},{"from":4481.88,"to":4484.71,"location":2,"content":"So, another thing that I'm really glad to see is that"},{"from":4484.71,"to":4487.83,"location":2,"content":"the neural NLG community is rapidly expanding."},{"from":4487.83,"to":4491.04,"location":2,"content":"Um, so, in the early years, uh,"},{"from":4491.04,"to":4495.39,"location":2,"content":"people were mostly transferring successful NMT methods to various NLG tasks."},{"from":4495.39,"to":4498.87,"location":2,"content":"Uh, but now I'm seeing you know, increasingly more inventive NLG techniques"},{"from":4498.87,"to":4502.73,"location":2,"content":"merging which is specific to the non-NMT generation settings."},{"from":4502.73,"to":4505.85,"location":2,"content":"Um, and again I urge you to go back into the slides that I skipped."},{"from":4505.85,"to":4508.65,"location":2,"content":"Um, so, I'm also saying there's increasingly more kind of"},{"from":4508.65,"to":4511.59,"location":2,"content":"neural NLG workshops and competitions especially"},{"from":4511.59,"to":4514.47,"location":2,"content":"focusing on open-ended NLG like those tasks that we"},{"from":4514.47,"to":4518.06,"location":2,"content":"know are not well suited by the automatic metrics that work for NMT."},{"from":4518.06,"to":4522.72,"location":2,"content":"So, there's a neural generation workshop, a storytelling workshop uh,"},{"from":4522.72,"to":4526.47,"location":2,"content":"and various challenges as well where people enter their for example, um,"},{"from":4526.47,"to":4528.87,"location":2,"content":"conversational dialogue agents to be,"},{"from":4528.87,"to":4531.49,"location":2,"content":"um, evaluated against each other."},{"from":4531.49,"to":4533.52,"location":2,"content":"So, I think that these different, um,"},{"from":4533.52,"to":4535.35,"location":2,"content":"kind of community organizing workshops and"},{"from":4535.35,"to":4538.71,"location":2,"content":"competitions are really doing a great job to kind of organize a community,"},{"from":4538.71,"to":4544.08,"location":2,"content":"increase reproducibility and standard evaluate, standardized evaluation."},{"from":4544.08,"to":4546.3,"location":2,"content":"Um, so, this is great but I'd say"},{"from":4546.3,"to":4550.23,"location":2,"content":"the biggest roadblock to progress is definitely still evaluation."},{"from":4550.23,"to":4553.31,"location":2,"content":"Okay. So, the last thing that I want to share with you"},{"from":4553.31,"to":4556.26,"location":2,"content":"is eight things that I've learned from working in NLG."},{"from":4556.26,"to":4558.93,"location":2,"content":"So, the first one is the more open-ended the task,"},{"from":4558.93,"to":4560.53,"location":2,"content":"the harder everything becomes."},{"from":4560.53,"to":4563.65,"location":2,"content":"Evaluation becomes harder, defining what you're doing becomes harder,"},{"from":4563.65,"to":4565.9,"location":2,"content":"telling when you're doing a good job becomes harder."},{"from":4565.9,"to":4569.13,"location":2,"content":"So, for this reason constraints can sometimes make things more welcome."},{"from":4569.13,"to":4575.37,"location":2,"content":"So, if you decide to constrain your task then sometimes it's easier to, to complete it."},{"from":4575.37,"to":4579.12,"location":2,"content":"Uh, the next one is aiming for a specific improvement can"},{"from":4579.12,"to":4582.68,"location":2,"content":"often be more manageable than aiming to improve overall generation quality."},{"from":4582.68,"to":4585.28,"location":2,"content":"So, for example, if you decide that you want to"},{"from":4585.28,"to":4587.86,"location":2,"content":"well for example increase diversity for your model, like say"},{"from":4587.86,"to":4591.27,"location":2,"content":"more interesting things that's an easier thing to achieve and measure than just"},{"from":4591.27,"to":4595.88,"location":2,"content":"saying we want to do overall generation quality because of the evaluation problem."},{"from":4595.88,"to":4600.28,"location":2,"content":"The next one is if you're using your language model to do NLG,"},{"from":4600.28,"to":4604.86,"location":2,"content":"then improving the language model that is getting better with perplexity will give you"},{"from":4604.86,"to":4606.96,"location":2,"content":"probably better generation quality because you've got"},{"from":4606.96,"to":4611.22,"location":2,"content":"a stronger language model but it's not the only way to improve generation quality,"},{"from":4611.22,"to":4613.06,"location":2,"content":"as we talked about before, uh,"},{"from":4613.06,"to":4616.99,"location":2,"content":"there's also other components that can affect generation apart from just language model,"},{"from":4616.99,"to":4620.34,"location":2,"content":"and that's part of the problem is that that's not in the training objective."},{"from":4620.34,"to":4623.65,"location":2,"content":"Um, my next tip is that you should look at your output a lot,"},{"from":4623.65,"to":4627.15,"location":2,"content":"partially because you don't have any single metric that can tell you what's going on."},{"from":4627.15,"to":4630.27,"location":2,"content":"It's pretty important to look at your output a lot to form your own opinions."},{"from":4630.27,"to":4632.74,"location":2,"content":"It can be time consuming but it's probably worth doing."},{"from":4632.74,"to":4634.57,"location":2,"content":"I ended up talking to these chatbots"},{"from":4634.57,"to":4637.44,"location":2,"content":"a huge amount during the time that I was working on the project."},{"from":4637.44,"to":4641.64,"location":2,"content":"Okay. Almost done, so, five you need an automatic metric, even if it's imperfect."},{"from":4641.64,"to":4643.05,"location":2,"content":"So, I know you that already know this because we"},{"from":4643.05,"to":4645.14,"location":2,"content":"wrote it all over the project instructions."},{"from":4645.14,"to":4649.2,"location":2,"content":"Uh, but I'd probably amend that to like maybe you need several automatic metrics."},{"from":4649.2,"to":4650.76,"location":2,"content":"I talked earlier about how you might track"},{"from":4650.76,"to":4653.44,"location":2,"content":"multiple things to get an overall picture of what's going on,"},{"from":4653.44,"to":4656.1,"location":2,"content":"I'd say the more open-ended your NLG task is,"},{"from":4656.1,"to":4659.18,"location":2,"content":"the more likely you need probably several metrics."},{"from":4659.18,"to":4663.19,"location":2,"content":"If you do human eval, you want to make the questions as focused as possible."},{"from":4663.19,"to":4665.1,"location":2,"content":"So, as I found out the hard way if you"},{"from":4665.1,"to":4667.78,"location":2,"content":"define the question as a very kind of overall vague thing,"},{"from":4667.78,"to":4669.89,"location":2,"content":"then you're just opening yourself up to, um,"},{"from":4669.89,"to":4672.98,"location":2,"content":"the respondents kind of misunderstanding you and, uh,"},{"from":4672.98,"to":4674.67,"location":2,"content":"if they are doing that then it's actually not their fault,"},{"from":4674.67,"to":4677.48,"location":2,"content":"it's your fault and you need to take your questions and that's what I learned."},{"from":4677.48,"to":4679.86,"location":2,"content":"Uh, next thing is reproducibility is"},{"from":4679.86,"to":4683.58,"location":2,"content":"a huge problem in today's NLP and deep learning in general,"},{"from":4683.58,"to":4686.13,"location":2,"content":"and the problem is only bigger in NLG,"},{"from":4686.13,"to":4688.38,"location":2,"content":"I guess it's another way that it's still a wild west."},{"from":4688.38,"to":4690.93,"location":2,"content":"So, I'd say that, uh, it would be really great,"},{"from":4690.93,"to":4693.3,"location":2,"content":"if everybody could publicly release all of"},{"from":4693.3,"to":4695.98,"location":2,"content":"their generated output when they write NLG papers."},{"from":4695.98,"to":4700.15,"location":2,"content":"I think this is a great practice because if you released your generated outputs,"},{"from":4700.15,"to":4703.88,"location":2,"content":"then if someone later let's say comes up with a great automatic metric,"},{"from":4703.88,"to":4707.98,"location":2,"content":"then they can just grab your generated output and then compute the metric on that."},{"from":4707.98,"to":4710.04,"location":2,"content":"Whereas if he never released your output or you"},{"from":4710.04,"to":4712.47,"location":2,"content":"released with some kind of imperfect metric number,"},{"from":4712.47,"to":4715.02,"location":2,"content":"then future researchers have nothing to compare it against."},{"from":4715.02,"to":4718.57,"location":2,"content":"Uh, so lastly, my last thought"},{"from":4718.57,"to":4722.79,"location":2,"content":"about working in NLG is that it can be very frustrating sometimes,"},{"from":4722.79,"to":4725.74,"location":2,"content":"uh, because things can be difficult and it's hard to know when you're making progress."},{"from":4725.74,"to":4728.81,"location":2,"content":"But the upside is it can also be very funny."},{"from":4728.81,"to":4732.74,"location":2,"content":"So this my last slide, here are some bizarre conversations that I've had with my chatbot."},{"from":4732.74,"to":4734,"location":2,"content":"[LAUGHTER] Thanks."},{"from":4734,"to":4777,"location":2,"content":"[NOISE] [LAUGHTER] All right, thanks."}]}