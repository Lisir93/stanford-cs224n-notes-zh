{"font_size":0.4,"font_color":"#FFFFFF","background_alpha":0.5,"background_color":"#9C27B0","Stroke":"none","body":[{"from":5.06,"to":8.97,"location":2,"content":"So welcome to, uh, the Machine Translation lecture"},{"from":8.97,"to":11.04,"location":2,"content":"which is kind of like a culmination"},{"from":11.04,"to":15.87,"location":2,"content":"of the sequence of three lectures on RNNs and related topics."},{"from":15.87,"to":18.69,"location":2,"content":"So let's have a few announcements first."},{"from":18.69,"to":20.91,"location":2,"content":"The first thing is that you probably noticed when he came in,"},{"from":20.91,"to":22.77,"location":2,"content":"we're taking attendance today."},{"from":22.77,"to":27.09,"location":2,"content":"Uh, so you need to sign in with the TAs who are outside the auditorium."},{"from":27.09,"to":28.96,"location":2,"content":"Uh, if you missed it,"},{"from":28.96,"to":30.54,"location":2,"content":"don't get up now, it's fine,"},{"from":30.54,"to":32.7,"location":2,"content":"there'll be time to sign in after the lecture."},{"from":32.7,"to":34.64,"location":2,"content":"Uh, and then if you have any kind of questions"},{"from":34.64,"to":36.65,"location":2,"content":"about special cases with the attendance policy,"},{"from":36.65,"to":38.87,"location":2,"content":"uh, you should check out the Piazza post that we put up"},{"from":38.87,"to":41.54,"location":2,"content":"last night with some clarifications."},{"from":41.54,"to":45.23,"location":2,"content":"Uh, the other reminder is assignment four content is going to be covered today."},{"from":45.23,"to":47.85,"location":2,"content":"So you're going to have everything you need to do assignment four at the end of today,"},{"from":47.85,"to":51.83,"location":2,"content":"and do get started early because the model takes four hours to train."},{"from":51.83,"to":54.71,"location":2,"content":"The other announcement is that we are going to be sending out"},{"from":54.71,"to":58.55,"location":2,"content":"a mid-quarter feedback survey sometime in the next few days probably."},{"from":58.55,"to":61.74,"location":2,"content":"Uh, so please do fill it out you'll get 0.5 percent credit,"},{"from":61.74,"to":66.92,"location":2,"content":"and you're also going to help us to make the course better, for the rest of the quarter."},{"from":66.92,"to":70.34,"location":2,"content":"Okay, so here's the overview of what we're going to do today."},{"from":70.34,"to":75.58,"location":2,"content":"Uh, today first we are going to introduce a new task in NLP which is machine translation."},{"from":75.58,"to":79.99,"location":2,"content":"And then we're going to introduce a new neural architecture called sequence-to-sequence."},{"from":79.99,"to":81.65,"location":2,"content":"And the connection here is that"},{"from":81.65,"to":85.53,"location":2,"content":"machine translation is a major use case of sequence-to-sequence."},{"from":85.53,"to":89.53,"location":2,"content":"After that, we're going to introduce a new neural technique called attention."},{"from":89.53,"to":93.39,"location":2,"content":"And this is something that improves sequence-to- sequence a lot."},{"from":93.39,"to":97.22,"location":2,"content":"Okay, so section one of this is gonna be about, uh,"},{"from":97.22,"to":102.34,"location":2,"content":"a bit of machine translation history, Pre-Neural Machine Translation."},{"from":102.34,"to":108.14,"location":2,"content":"So machine translation or MT is the task of translating a sentence X, uh,"},{"from":108.14,"to":110.06,"location":2,"content":"which we call the source language,"},{"from":110.06,"to":112.43,"location":2,"content":"whatever language you're translating from,"},{"from":112.43,"to":116.39,"location":2,"content":"into a sentence Y which is in another language which we call, the target language."},{"from":116.39,"to":117.65,"location":2,"content":"Uh, so here's an example,"},{"from":117.65,"to":119.77,"location":2,"content":"let's suppose X is this French sentence."},{"from":119.77,"to":122.06,"location":2,"content":"Um, could anyone in the audience,"},{"from":122.06,"to":124.22,"location":2,"content":"a French speaker translate to English for us."},{"from":124.22,"to":133.73,"location":2,"content":"[NOISE] Yeah."},{"from":133.73,"to":139.28,"location":2,"content":"The man is born free, and  everywhere he is in irons."},{"from":139.28,"to":140.76,"location":2,"content":"Great. So that was something like,"},{"from":140.76,"to":142.77,"location":2,"content":"the man is born free, but everywhere he is in irons."},{"from":142.77,"to":144.38,"location":2,"content":"That was a fairly literal translation."},{"from":144.38,"to":147.77,"location":2,"content":"It's usually translated this quote by Rousseau is usually translated as."},{"from":147.77,"to":149.85,"location":2,"content":"Man is born free, but everywhere he is in chains."},{"from":149.85,"to":152.15,"location":2,"content":"But you know there's an ambiguity should fers be,"},{"from":152.15,"to":154.19,"location":2,"content":"um, literally irons or chains."},{"from":154.19,"to":156.41,"location":2,"content":"Also you could choose to, uh, translate"},{"from":156.41,"to":158.66,"location":2,"content":"L'homme as man or maybe humankind."},{"from":158.66,"to":162.31,"location":2,"content":"Uh, so this is an example of machine translation and there's already,"},{"from":162.31,"to":164.31,"location":2,"content":"you know, quite a few choices you can make."},{"from":164.31,"to":170.5,"location":2,"content":"So the beginning of machine translation as an AI task began in the early 1950's."},{"from":170.5,"to":172.34,"location":2,"content":"So, um, in particular,"},{"from":172.34,"to":175.01,"location":2,"content":"there was lots of work translating Russian to English, uh,"},{"from":175.01,"to":177.17,"location":2,"content":"because the West was very interested in listening"},{"from":177.17,"to":179.79,"location":2,"content":"to what the Russians were saying during the Cold War."},{"from":179.79,"to":185.3,"location":2,"content":"And we've got a fun video here which shows the state of machine translation in 1954."},{"from":185.3,"to":190.52,"location":2,"content":"[MUSIC] They hadn't reckoned with ambiguity when they set out"},{"from":190.52,"to":193.17,"location":2,"content":"to use computers to translate languages."},{"from":193.17,"to":200.84,"location":2,"content":"A $500,000 simple calculator, the most versatile electronic brain known, translates Russian into English."},{"from":200.84,"to":202.81,"location":2,"content":"Instead of mathematical wizardry,"},{"from":202.81,"to":204.62,"location":2,"content":"a sentence in Russian is to be fed in- [OVERLAPPING]"},{"from":204.62,"to":207.98,"location":2,"content":"One of the first non-numerical applications of computers,"},{"from":207.98,"to":209.48,"location":2,"content":"it was hyped as the solution to"},{"from":209.48,"to":213.37,"location":2,"content":"the Cold War obsession of keeping tabs on what the Russians were doing."},{"from":213.37,"to":217.25,"location":2,"content":"Claims were made that the computer would replace most human translators."},{"from":217.25,"to":220.24,"location":2,"content":"Professor, you're just in the experimental stage."},{"from":220.24,"to":223.24,"location":2,"content":"When you go in for full-scale production what will the capacity be?"},{"from":223.24,"to":228.24,"location":2,"content":"We should be able to do about, a little more than a conventional computer."},{"from":228.24,"to":231.24,"location":2,"content":"Uh, about 1 to 2 million words an hour."},{"from":231.24,"to":233.24,"location":2,"content":"And this will be quite an adequate speed to cope with the"},{"from":233.24,"to":236.24,"location":2,"content":"whole output of the Soviet Union in just a few hours"},{"from":236.24,"to":238.24,"location":2,"content":"of computer time a week."},{"from":238.24,"to":239.24,"location":2,"content":"When do you hope to be able to achieve this speed?"},{"from":239.24,"to":246.24,"location":2,"content":"If our experiments go well, then perhaps within five years or so."},{"from":246.24,"to":248.59,"location":2,"content":"So in this video I think there's a number of interesting things."},{"from":248.59,"to":253.53,"location":2,"content":"Um, firstly we can see an example of about how AI hype is nothing new."},{"from":253.53,"to":255.85,"location":2,"content":"Even in 1954 they were talking"},{"from":255.85,"to":260.13,"location":2,"content":"this machine translation system as if it was an electronic brain which I think,"},{"from":260.13,"to":262.24,"location":2,"content":"uh, overstates maybe how general it is."},{"from":262.24,"to":265.42,"location":2,"content":"Um, they were also at least some of them fairly optimistic that"},{"from":265.42,"to":270.52,"location":2,"content":"this machine translation system was going to be replacing humans, uh, anytime soon."},{"from":270.52,"to":273.67,"location":2,"content":"Um, so yeah that's- that's pretty interesting."},{"from":273.67,"to":278.13,"location":2,"content":"And, um, the thing is that these systems actually were mostly rule-based, uh,"},{"from":278.13,"to":280.74,"location":2,"content":"by which I mean that they were mostly using"},{"from":280.74,"to":283.36,"location":2,"content":"a bilingual dictionary between Russian and English,"},{"from":283.36,"to":286.44,"location":2,"content":"and they were essentially mostly just looking up the Russian words, uh,"},{"from":286.44,"to":287.82,"location":2,"content":"looking at the English counterparts,"},{"from":287.82,"to":291.35,"location":2,"content":"and they were storing these big bilingual dictionaries on these large magnetic tapes."},{"from":291.35,"to":295.36,"location":2,"content":"Um, so certainly it was a huge technical feat at the time, uh,"},{"from":295.36,"to":297.11,"location":2,"content":"but they, uh, some people were probably too"},{"from":297.11,"to":300.06,"location":2,"content":"optimistic about how quickly it would replace humans."},{"from":300.06,"to":303.21,"location":2,"content":"So jumping forward several decades in time,"},{"from":303.21,"to":306.33,"location":2,"content":"uh, and I want to tell you about statistical machine translation."},{"from":306.33,"to":310.04,"location":2,"content":"So the core idea of statistical machine translation is that you're going to"},{"from":310.04,"to":314.3,"location":2,"content":"learn a probabilistic model from the data in order to do the translation."},{"from":314.3,"to":319.28,"location":2,"content":"So as an example as before suppose that we're translating from French to English."},{"from":319.28,"to":323.21,"location":2,"content":"The idea here is that you want to find the best English sentence Y,"},{"from":323.21,"to":325.06,"location":2,"content":"given the French sentence X."},{"from":325.06,"to":328.13,"location":2,"content":"And mathematically you can formulate this as finding argmax"},{"from":328.13,"to":331.81,"location":2,"content":"Y of this conditional probability of Y given X."},{"from":331.81,"to":334.94,"location":2,"content":"And the model that you're learning is this probability distribution"},{"from":334.94,"to":340.81,"location":2,"content":"P. So what we usually do is we break down this probability into,"},{"from":340.81,"to":343.49,"location":2,"content":"uh, its two components using Bayes' Rule."},{"from":343.49,"to":346.97,"location":2,"content":"So this means that finding the Y that maximizes,"},{"from":346.97,"to":348.57,"location":2,"content":"uh, probability of Y given X,"},{"from":348.57,"to":351.41,"location":2,"content":"is equivalent to finding the Y that maximizes"},{"from":351.41,"to":355.24,"location":2,"content":"the probability of X given Y times the probability of Y."},{"from":355.24,"to":357.26,"location":2,"content":"So the two components here,"},{"from":357.26,"to":359.81,"location":2,"content":"on the left we have a translation model."},{"from":359.81,"to":363.32,"location":2,"content":"And this is keeping track of how words and phrases should be translated."},{"from":363.32,"to":366.53,"location":2,"content":"Uh, so the idea is that it knows, uh, how, uh,"},{"from":366.53,"to":369.77,"location":2,"content":"French words and the English words might be translated to each other"},{"from":369.77,"to":373.6,"location":2,"content":"or maybe small- small phrases and chunks of words should be translated."},{"from":373.6,"to":375.98,"location":2,"content":"And this is learned from a lot of parallel data,"},{"from":375.98,"to":377.76,"location":2,"content":"and I'll be telling you later how we do that."},{"from":377.76,"to":380.72,"location":2,"content":"The second component P of Y."},{"from":380.72,"to":382.63,"location":2,"content":"This is just a language model."},{"from":382.63,"to":383.98,"location":2,"content":"We learned about this last week."},{"from":383.98,"to":386.93,"location":2,"content":"A language model is a system that can predict the next word,"},{"from":386.93,"to":389.36,"location":2,"content":"but it can also be thought of as a system that tells"},{"from":389.36,"to":391.99,"location":2,"content":"you the probability of a sequence of words."},{"from":391.99,"to":394.61,"location":2,"content":"So here, if we're translating from French to English,"},{"from":394.61,"to":396.94,"location":2,"content":"P of Y is an English language model."},{"from":396.94,"to":400.34,"location":2,"content":"So the idea here is that the reason why we want to break down"},{"from":400.34,"to":404.36,"location":2,"content":"this single condition- conditional probability distribution into"},{"from":404.36,"to":409.49,"location":2,"content":"the product of two different ones is that this is a kind of division of labor."},{"from":409.49,"to":411.56,"location":2,"content":"The idea here is that instead of, uh,"},{"from":411.56,"to":414.41,"location":2,"content":"a single conditional probability distribution need to"},{"from":414.41,"to":417.53,"location":2,"content":"understand how to translate and how to write good English text,"},{"from":417.53,"to":420.05,"location":2,"content":"and understand sentence structure and everything at once."},{"from":420.05,"to":424.28,"location":2,"content":"The idea is that you separate it, so that the translation model on the left in blue"},{"from":424.28,"to":429.17,"location":2,"content":"mostly just knows about local translation of small chunks of words and phrases,"},{"from":429.17,"to":430.85,"location":2,"content":"whereas the language model on the right,"},{"from":430.85,"to":432.94,"location":2,"content":"more takes care of writing good English,"},{"from":432.94,"to":435.73,"location":2,"content":"good sentence structure, word order, and so on."},{"from":435.73,"to":438.05,"location":2,"content":"So you already know how to learn"},{"from":438.05,"to":440.3,"location":2,"content":"a language model because we learned about that last time,"},{"from":440.3,"to":441.98,"location":2,"content":"you just need lots of monolingual data,"},{"from":441.98,"to":443.57,"location":2,"content":"in this case, English data."},{"from":443.57,"to":445.49,"location":2,"content":"So I'm going to tell you more about how we would learn"},{"from":445.49,"to":450.42,"location":2,"content":"this translation model that needs to be learned from parallel data."},{"from":450.42,"to":453.83,"location":2,"content":"So, we need a large amount of parallel data"},{"from":453.83,"to":456.54,"location":2,"content":"in order to learn this, uh, translation model."},{"from":456.54,"to":459.7,"location":2,"content":"And an early example of a parallel corpus,"},{"from":459.7,"to":461.26,"location":2,"content":"is the Rosetta Stone."},{"from":461.26,"to":466.03,"location":2,"content":"So, this is a stone that has the same text written in three different languages."},{"from":466.03,"to":469.84,"location":2,"content":"And, uh, this is a hugely important artifact for, um,"},{"from":469.84,"to":473.59,"location":2,"content":"the- the people who were trying to understand ancient Egyptian."},{"from":473.59,"to":475.06,"location":2,"content":"So, in the 19th century,"},{"from":475.06,"to":476.67,"location":2,"content":"uh, scholars discovered this stone,"},{"from":476.67,"to":478.86,"location":2,"content":"and it helped them to figure out ancient Egyptian"},{"from":478.86,"to":481.44,"location":2,"content":"because there was this parallel text that had,"},{"from":481.44,"to":484.31,"location":2,"content":"uh, the same- the same text in other languages that they did know."},{"from":484.31,"to":487.58,"location":2,"content":"So, this is a- this is a really important parallel corpus."},{"from":487.58,"to":489.64,"location":2,"content":"And, uh, if you're ever in London you can go to"},{"from":489.64,"to":492.31,"location":2,"content":"the British Museum and see this in person."},{"from":492.31,"to":495.22,"location":2,"content":"So, the idea is that you get your parallel data."},{"from":495.22,"to":497.17,"location":2,"content":"Obviously you need a larger amount that's on the stone"},{"from":497.17,"to":499.57,"location":2,"content":"and hopefully it shouldn't be written on a stone either."},{"from":499.57,"to":505.11,"location":2,"content":"Uh, but you can use this to learn your statistical machine translation model."},{"from":505.11,"to":507.43,"location":2,"content":"So, the idea is that you are trying to learn"},{"from":507.43,"to":510.52,"location":2,"content":"this conditional probability distribution of X given Y."},{"from":510.52,"to":513.52,"location":2,"content":"So, what we do is we actually break this down even further."},{"from":513.52,"to":519.65,"location":2,"content":"We actually want to consider the probability of X and A given Y where A is the alignment."},{"from":519.65,"to":521.98,"location":2,"content":"So, the idea of alignment is this is, uh,"},{"from":521.98,"to":527.11,"location":2,"content":"how the words in the English sentence and the French sentence correspond to each other."},{"from":527.11,"to":530.68,"location":2,"content":"So, I'm gonna, uh, demonstrate this by an example."},{"from":530.68,"to":534.13,"location":2,"content":"So, in this example where we're translating"},{"from":534.13,"to":537.31,"location":2,"content":"the sentence \"Japan shaken by two new quakes\" to French."},{"from":537.31,"to":541.06,"location":2,"content":"Then you can see there's a pretty simple one-to-one alignment here,"},{"from":541.06,"to":545.14,"location":2,"content":"uh, of English words to French words and also they appear in the exact same order."},{"from":545.14,"to":549.85,"location":2,"content":"The only thing, uh, that doesn't conform to that is the word 'Le' in, uh,"},{"from":549.85,"to":552.46,"location":2,"content":"French which we call a spurious word because it doesn't"},{"from":552.46,"to":555.19,"location":2,"content":"have a direct counterpart in the English sentence."},{"from":555.19,"to":559.68,"location":2,"content":"And that's because in English we just say Japan but in French we say 'Le Japon'."},{"from":559.68,"to":562.91,"location":2,"content":"So, alignment can be a bit more complicated than that,"},{"from":562.91,"to":565.47,"location":2,"content":"for example alignment can be many-to-one."},{"from":565.47,"to":568.15,"location":2,"content":"In this example, you have, uh,"},{"from":568.15,"to":572.73,"location":2,"content":"several French words that have multiple English words that correspond to them."},{"from":572.73,"to":576.15,"location":2,"content":"So, this is what we call many-to-one alignments."},{"from":576.15,"to":578.46,"location":2,"content":"It can go in the other direction too,"},{"from":578.46,"to":580.06,"location":2,"content":"alignment can be one-to-many."},{"from":580.06,"to":583.54,"location":2,"content":"So, here we have a single English word implemented which has"},{"from":583.54,"to":584.92,"location":2,"content":"a one-to-many alignment because there's"},{"from":584.92,"to":588.33,"location":2,"content":"a three-word French phase- phrase that corresponds to it."},{"from":588.33,"to":589.9,"location":2,"content":"So, on the left and the right,"},{"from":589.9,"to":592.39,"location":2,"content":"we have two ways of depicting the same alignment."},{"from":592.39,"to":594.05,"location":2,"content":"It's either, uh, kind of,"},{"from":594.05,"to":597.99,"location":2,"content":"uh, charts or it can be a- a graph."},{"from":597.99,"to":601.33,"location":2,"content":"So, here's another example, um,"},{"from":601.33,"to":604.4,"location":2,"content":"of a one-to-many, uh, sorry,"},{"from":604.4,"to":608.2,"location":2,"content":"right so we call this word implemented, that is one-to-many."},{"from":608.2,"to":609.85,"location":2,"content":"We call it a fertile word,"},{"from":609.85,"to":614.08,"location":2,"content":"because the idea is that it has many children in the- in the target sentence."},{"from":614.08,"to":616.75,"location":2,"content":"So, in fact, there are some words which are very fertile."},{"from":616.75,"to":621.01,"location":2,"content":"Uh, here's an example where the source sentence 'il a m' entarte',"},{"from":621.01,"to":623.46,"location":2,"content":"uh, means he hit me with a pie."},{"from":623.46,"to":625.24,"location":2,"content":"And here in French,uh,"},{"from":625.24,"to":628.81,"location":2,"content":"this verb m' entarte means, uh, to hit someone with a pie."},{"from":628.81,"to":635.82,"location":2,"content":"And [LAUGHTER] this word has no single word equivalent in English."},{"from":635.82,"to":638.53,"location":2,"content":"We don't have a single verb that means to hit someone with a pie."},{"from":638.53,"to":641.7,"location":2,"content":"[NOISE] Um, which I think that's really fun that French has a word,"},{"from":641.7,"to":643.24,"location":2,"content":"you wonder maybe they do it so"},{"from":643.24,"to":645.01,"location":2,"content":"often that they need a single word for that, I don't know, [LAUGHTER]."},{"from":645.01,"to":648.76,"location":2,"content":"Um, so, this is an example of a fertile word, right?"},{"from":648.76,"to":654,"location":2,"content":"Because it needs to have several corresponding English words to translate it."},{"from":654,"to":657.38,"location":2,"content":"So, we can have one-to-many and many-to-one,"},{"from":657.38,"to":659.65,"location":2,"content":"you can also have many-to-many alignments."},{"from":659.65,"to":663.07,"location":2,"content":"You could call that kind of phrase level translation or phrase to phrase."},{"from":663.07,"to":666.1,"location":2,"content":"So, here, uh, the English sentence says,"},{"from":666.1,"to":670.74,"location":2,"content":"\"The poor don't have any money,\" and here don't have any money corresponds to the French,"},{"from":670.74,"to":672.38,"location":2,"content":"uh, phrase 'sont demunis'."},{"from":672.38,"to":674.47,"location":2,"content":"And this is a many-to-many alignment."},{"from":674.47,"to":678.75,"location":2,"content":"Because there's no obvious way to break down this phrase to phrase alignment into,"},{"from":678.75,"to":682.86,"location":2,"content":"uh, smaller word-to-word alignments."},{"from":682.86,"to":686.56,"location":2,"content":"Okay. So, that's what alignment is, and if you remember,"},{"from":686.56,"to":688.3,"location":2,"content":"we were thinking about how would you learn"},{"from":688.3,"to":690.96,"location":2,"content":"this probability distribution of what the alignment is,"},{"from":690.96,"to":693.94,"location":2,"content":"uh, in order to do statistical machine translation."},{"from":693.94,"to":696.82,"location":2,"content":"So, the idea is that you learn probability of x and"},{"from":696.82,"to":701.22,"location":2,"content":"a given y as a combination of many factors or many features."},{"from":701.22,"to":703.48,"location":2,"content":"So, you can say that for example, uh,"},{"from":703.48,"to":707.14,"location":2,"content":"what's the probability of a particular word aligning to another particular word?"},{"from":707.14,"to":708.91,"location":2,"content":"Like, you know, this English word and this French word,"},{"from":708.91,"to":710.05,"location":2,"content":"how often do they align?"},{"from":710.05,"to":712.06,"location":2,"content":"But then it also depends on for example,"},{"from":712.06,"to":713.74,"location":2,"content":"what's that position in the sentence?"},{"from":713.74,"to":716.71,"location":2,"content":"Like, uh, if they both appear near the end of the sentences,"},{"from":716.71,"to":718.48,"location":2,"content":"then it's more likely that they align,"},{"from":718.48,"to":721.86,"location":2,"content":"whereas if one's at the beginning and one's at the end, that's less likely."},{"from":721.86,"to":724.03,"location":2,"content":"You would also consider things like, uh,"},{"from":724.03,"to":728.08,"location":2,"content":"what's the probability of this particular French word having this particular fertility?"},{"from":728.08,"to":729.17,"location":2,"content":"Like, what's the, uh,"},{"from":729.17,"to":732.57,"location":2,"content":"probability of this word having three corresponding English words?"},{"from":732.57,"to":737.04,"location":2,"content":"And so on. So, all of these statistics are learned from your parallel data."},{"from":737.04,"to":740.24,"location":2,"content":"And there's many other things that you would take into consideration."},{"from":740.24,"to":743.86,"location":2,"content":"So, we're looking at a kind of overview of statistical machine translation today."},{"from":743.86,"to":746,"location":2,"content":"You're not going to understand it in full detail."},{"from":746,"to":748.12,"location":2,"content":"But we're understanding an overview of how it works,"},{"from":748.12,"to":749.7,"location":2,"content":"because we're going to be, uh,"},{"from":749.7,"to":753.51,"location":2,"content":"comparing it to neural machine translation."},{"from":753.51,"to":757.63,"location":2,"content":"Okay. So, we're learning this SMT system."},{"from":757.63,"to":760.83,"location":2,"content":"And so far we've broken it down into these two main components."},{"from":760.83,"to":763.9,"location":2,"content":"We've got the translation model and we've got the language model."},{"from":763.9,"to":766.93,"location":2,"content":"And, uh, we understand a little bit about how you might learn"},{"from":766.93,"to":770.34,"location":2,"content":"this translation model by breaking it down into alignments."},{"from":770.34,"to":774.02,"location":2,"content":"So, our question remains, how do you do the argmax over Y?"},{"from":774.02,"to":779.56,"location":2,"content":"How do you find your French sentence Y that maximizes this probability?"},{"from":779.56,"to":783.31,"location":2,"content":"So, one kind of brute force solution is you could say,"},{"from":783.31,"to":785.82,"location":2,"content":"uh, let's enumerate every possible Y,"},{"from":785.82,"to":789.46,"location":2,"content":"that's kind of every possible sequence of French words maybe up to some length,"},{"from":789.46,"to":792.43,"location":2,"content":"uh, and we'll calculate this probability for all of them."},{"from":792.43,"to":794.98,"location":2,"content":"And it should be pretty clear that that's just a no-go,"},{"from":794.98,"to":796.47,"location":2,"content":"that's way too expensive, uh,"},{"from":796.47,"to":799.52,"location":2,"content":"we're not going to be able to, uh, get anywhere with that."},{"from":799.52,"to":803.65,"location":2,"content":"So, the answer for how you actually do this in practice is you're going to use"},{"from":803.65,"to":808.13,"location":2,"content":"some kind of heuristic search algorithm to search for the best translation Y."},{"from":808.13,"to":810.67,"location":2,"content":"Uh, but along the way you're going to discard hypotheses"},{"from":810.67,"to":813.1,"location":2,"content":"that are too low proba- probability."},{"from":813.1,"to":816.85,"location":2,"content":"So, you're gonna search, but you're going to discard and prune the tree as you"},{"from":816.85,"to":820.51,"location":2,"content":"go to make sure that you're not keeping too many hypotheses, uh, on each step."},{"from":820.51,"to":827.68,"location":2,"content":"[NOISE] So, this process of finding your best sequence is also called decoding."},{"from":827.68,"to":830.5,"location":2,"content":"So, here's an overview of how that works for SMT."},{"from":830.5,"to":832.84,"location":2,"content":"Uh, this an example, uh,"},{"from":832.84,"to":837.16,"location":2,"content":"where you have this German sentence that translates to \"he does not go home\","},{"from":837.16,"to":841.93,"location":2,"content":"uh, and you can see that there's some kind of phrase to phrase alignments here."},{"from":841.93,"to":848,"location":2,"content":"So, uh, an overview of how this decoding would work in SMT is that you kind of"},{"from":848,"to":853.45,"location":2,"content":"consider lots of different hypotheses for how you might translate these individual words,"},{"from":853.45,"to":858.01,"location":2,"content":"uh, and then you build it up to consider how you might translate,"},{"from":858.01,"to":861.22,"location":2,"content":"uh, individual phrases and the phrases get bigger."},{"from":861.22,"to":864.19,"location":2,"content":"So, for example, you can see that on the top right if it's not too"},{"from":864.19,"to":867.19,"location":2,"content":"small you can see that the German word for house,"},{"from":867.19,"to":872.29,"location":2,"content":"uh, could be translated into the English word house or home or chamber and so on."},{"from":872.29,"to":876.34,"location":2,"content":"Uh, so we consider all of these different hypotheses and look into how we might"},{"from":876.34,"to":880.38,"location":2,"content":"put those together to translate the phrases but you don't keep all of them all the time."},{"from":880.38,"to":883.13,"location":2,"content":"You get rid of the ones that are too low probability."},{"from":883.13,"to":886.21,"location":2,"content":"So, this can also be depicted as a kind of a tree,"},{"from":886.21,"to":887.54,"location":2,"content":"where you are, uh,"},{"from":887.54,"to":889.81,"location":2,"content":"exploring different options, you're searching through"},{"from":889.81,"to":893.08,"location":2,"content":"the space of options but then you prune the tree as you go."},{"from":893.08,"to":894.79,"location":2,"content":"So, I know this is a very,"},{"from":894.79,"to":896.02,"location":2,"content":"very high level, uh,"},{"from":896.02,"to":897.57,"location":2,"content":"description of how decoding might work."},{"from":897.57,"to":899.08,"location":2,"content":"And in fact, later in this lecture,"},{"from":899.08,"to":900.77,"location":2,"content":"you're gonna see a detailed,"},{"from":900.77,"to":907.32,"location":2,"content":"um, explanation of how this kind of decoding works for neural machine translation."},{"from":907.32,"to":910.29,"location":2,"content":"Okay. So, what's our, um,"},{"from":910.29,"to":912.44,"location":2,"content":"overview of statistical machine translation?"},{"from":912.44,"to":913.73,"location":2,"content":"Uh, was it effective?"},{"from":913.73,"to":917.06,"location":2,"content":"Uh, so SMT was a huge research field,"},{"from":917.06,"to":920.4,"location":2,"content":"uh, from the 1990s to about maybe, uh, 2013."},{"from":920.4,"to":923.78,"location":2,"content":"And the best systems during this time were extremely complex,"},{"from":923.78,"to":926.71,"location":2,"content":"they were extremely sophisticated and impressive systems and"},{"from":926.71,"to":930.11,"location":2,"content":"SMT made the best machine translation systems in the world."},{"from":930.11,"to":931.82,"location":2,"content":"But they were very complex."},{"from":931.82,"to":932.98,"location":2,"content":"So, for example, you know,"},{"from":932.98,"to":936.58,"location":2,"content":"there were hundreds of important details that we haven't mentioned here at all."},{"from":936.58,"to":938.71,"location":2,"content":"There were many many techniques to make it, uh,"},{"from":938.71,"to":940.43,"location":2,"content":"more complex and more,"},{"from":940.43,"to":943.32,"location":2,"content":"um, sophisticated than what I've described today."},{"from":943.32,"to":948.54,"location":2,"content":"In particular, the systems had to have many separately designed, uh, subcomponents."},{"from":948.54,"to":952.88,"location":2,"content":"So, we already saw how you break down the translation model into two separate parts."},{"from":952.88,"to":954.05,"location":2,"content":"Uh, but there was, you know,"},{"from":954.05,"to":957.98,"location":2,"content":"many more sub-components than that and often they had to be learned separately."},{"from":957.98,"to":961.65,"location":2,"content":"This meant the engineers had to do a lot of feature engineering."},{"from":961.65,"to":963.71,"location":2,"content":"Uh, you have to design features to capture"},{"from":963.71,"to":967.2,"location":2,"content":"the particular language phenomena that you are interested in."},{"from":967.2,"to":970.22,"location":2,"content":"So, this meant that they had to require a lot"},{"from":970.22,"to":972.83,"location":2,"content":"of compiling and maintaining of extra resources."},{"from":972.83,"to":974.45,"location":2,"content":"And in fact, you had to have, uh,"},{"from":974.45,"to":976.43,"location":2,"content":"different resources for different languages."},{"from":976.43,"to":979.91,"location":2,"content":"So, the work kind of multiplied the more languages you had."},{"from":979.91,"to":981.23,"location":2,"content":"An example of this,"},{"from":981.23,"to":983.9,"location":2,"content":"is you had to have uh, tables of equivalent phrases."},{"from":983.9,"to":987.15,"location":2,"content":"So, for example, if you're doing French and English translation, then, uh,"},{"from":987.15,"to":990.32,"location":2,"content":"they would be collecting these phrases of, uh, sorry,"},{"from":990.32,"to":993.9,"location":2,"content":"these tables of phrases that they considered similar and those were learned from the data."},{"from":993.9,"to":997.36,"location":2,"content":"But this was a lot of information that had to be stored and maintained."},{"from":997.36,"to":1000.97,"location":2,"content":"So overall, this was just a lot of human effort to maintain."},{"from":1000.97,"to":1002.53,"location":2,"content":"Uh, and again yes,"},{"from":1002.53,"to":1005.13,"location":2,"content":"you had to put more human effort in if you wanted to"},{"from":1005.13,"to":1008.47,"location":2,"content":"learn an SMT system for a new language path."},{"from":1008.47,"to":1018.11,"location":2,"content":"Okay, are there any questions here about, uh, SMT?"},{"from":1018.11,"to":1021.66,"location":2,"content":"Okay. So, moving on, that's SMT."},{"from":1021.66,"to":1024.64,"location":2,"content":"Now we're going to move on to, uh, Section 2 of this lecture."},{"from":1024.64,"to":1029.91,"location":2,"content":"So, I want to take you back to the year 2014"},{"from":1029.91,"to":1032.37,"location":2,"content":"for a dramatic reenactment of what happened"},{"from":1032.37,"to":1035.4,"location":2,"content":"in the world of Machine Translation Research."},{"from":1035.4,"to":1037.8,"location":2,"content":"So, in 2014, something very dramatic happened,"},{"from":1037.8,"to":1040.93,"location":2,"content":"and that thing that happened is called Neural Machine Translation."},{"from":1040.93,"to":1043.68,"location":2,"content":"And I think it looked a little bit like this,"},{"from":1043.68,"to":1045.87,"location":2,"content":"if I'm not being too dramatic."},{"from":1045.87,"to":1048.81,"location":2,"content":"So, what is Neural Machine Translation?"},{"from":1048.81,"to":1052.58,"location":2,"content":"The idea is that NMT is a way to do machine translation,"},{"from":1052.58,"to":1055.74,"location":2,"content":"but using just a single neural network."},{"from":1055.74,"to":1059.7,"location":2,"content":"The neural network architecture that they used is called sequence-to-sequence,"},{"from":1059.7,"to":1061.23,"location":2,"content":"or sometime it's just called seq2seq,"},{"from":1061.23,"to":1064.21,"location":2,"content":"uh, and it involves two RNNs."},{"from":1064.21,"to":1066.93,"location":2,"content":"So, uh, it's called sequence-to-sequence because you're mapping"},{"from":1066.93,"to":1070.77,"location":2,"content":"one sequence to the other- the source sentence to the target sentence,"},{"from":1070.77,"to":1072.39,"location":2,"content":"and you need two RNNs, basically,"},{"from":1072.39,"to":1074.67,"location":2,"content":"to handle those two different sentences."},{"from":1074.67,"to":1079.1,"location":2,"content":"All right. Let's look at the diagram to see what sequence-to-sequence is in detail."},{"from":1079.1,"to":1083.37,"location":2,"content":"So, we start off with our source sentence and we're going to use our example from before,"},{"from":1083.37,"to":1085.38,"location":2,"content":"ah, il a m'entarte,"},{"from":1085.38,"to":1087.27,"location":2,"content":"which means he hit me with a pie."},{"from":1087.27,"to":1090.81,"location":2,"content":"So, we, uh, feed this into our encoder RNN,"},{"from":1090.81,"to":1092.82,"location":2,"content":"and, ah, this is- as you've seen before,"},{"from":1092.82,"to":1095.7,"location":2,"content":"I've drawn a unidirectional RNN,"},{"from":1095.7,"to":1097.26,"location":2,"content":"but this could be bidirectional,"},{"from":1097.26,"to":1099.05,"location":2,"content":"it also could be multi layer,"},{"from":1099.05,"to":1102.67,"location":2,"content":"it could be another or it could be LSTM and so on."},{"from":1102.67,"to":1108.24,"location":2,"content":"Another thing to note is that we're passing word embeddings into this encode RNN,"},{"from":1108.24,"to":1111,"location":2,"content":"but I'm just not explicitly depicting that step."},{"from":1111,"to":1113.25,"location":2,"content":"[NOISE] Okay."},{"from":1113.25,"to":1116.43,"location":2,"content":"So, the idea of the encoder RNN is that it's going to"},{"from":1116.43,"to":1119.79,"location":2,"content":"produce some kind of encoding of this source sentence."},{"from":1119.79,"to":1123.15,"location":2,"content":"So, for now, let's assume that the encoding of the source sentence is going"},{"from":1123.15,"to":1127.32,"location":2,"content":"to be the final hidden state of this encoder RNN."},{"from":1127.32,"to":1131.7,"location":2,"content":"So, what happens next is we pass this encoding of the source sentence."},{"from":1131.7,"to":1134.06,"location":2,"content":"We pass it over to the decoder RNN,"},{"from":1134.06,"to":1136.61,"location":2,"content":"which is going to translate into English."},{"from":1136.61,"to":1139.63,"location":2,"content":"So the decoder RNN is a language model,"},{"from":1139.63,"to":1141.78,"location":2,"content":"in particular, it's a conditional language model,"},{"from":1141.78,"to":1143.31,"location":2,"content":"like we talked about last time."},{"from":1143.31,"to":1145.44,"location":2,"content":"So, it's conditional because it's going to produce"},{"from":1145.44,"to":1148.45,"location":2,"content":"the target sentence but conditioned on this encoding."},{"from":1148.45,"to":1152.43,"location":2,"content":"And the encoding is that vector that has the orange box around it."},{"from":1152.43,"to":1156.72,"location":2,"content":"So, how does this work? Uh, we start off by feeding the start token"},{"from":1156.72,"to":1160.49,"location":2,"content":"into the decoder, and then, uh,"},{"from":1160.49,"to":1162.72,"location":2,"content":"we can get the first state of the decoder,"},{"from":1162.72,"to":1165.05,"location":2,"content":"because we're using the encoding of"},{"from":1165.05,"to":1168.74,"location":2,"content":"the source sentence as the initial hidden state for the decoder."},{"from":1168.74,"to":1171.24,"location":2,"content":"So then, we get our first output from"},{"from":1171.24,"to":1173.49,"location":2,"content":"the decoder which is a probability distribution of what"},{"from":1173.49,"to":1174.8,"location":2,"content":"word might come next."},{"from":1174.8,"to":1177.13,"location":2,"content":"And that's supposed to be taking the argmax over that,"},{"from":1177.13,"to":1178.89,"location":2,"content":"and then that gets us the word, uh,"},{"from":1178.89,"to":1183.36,"location":2,"content":"\"He\", which in this case is correct because that's probably the word you should start with."},{"from":1183.36,"to":1185.97,"location":2,"content":"Okay, so then we just take the word \"he\" and then we"},{"from":1185.97,"to":1188.79,"location":2,"content":"feed it back into the decoder on the next step."},{"from":1188.79,"to":1190.72,"location":2,"content":"And then we do the same thing again."},{"from":1190.72,"to":1193.96,"location":2,"content":"We take argmax and we get a new word, and we get, he hit."},{"from":1193.96,"to":1196.92,"location":2,"content":"The idea is that you can co- uh, continue doing this,"},{"from":1196.92,"to":1200.2,"location":2,"content":"ah, operation and in that way you're going to generate, uh,"},{"from":1200.2,"to":1202.34,"location":2,"content":"your target sentence, uh,"},{"from":1202.34,"to":1205.18,"location":2,"content":"which will be something like \"He hit me with a pie\","},{"from":1205.18,"to":1209.69,"location":2,"content":"and you stop once your decoder produces the end token."},{"from":1209.69,"to":1213.48,"location":2,"content":"So an important thing to note here is that"},{"from":1213.48,"to":1216.48,"location":2,"content":"this picture is showing you what happens at test time."},{"from":1216.48,"to":1218.64,"location":2,"content":"This shows you how to generate text."},{"from":1218.64,"to":1220.34,"location":2,"content":"This isn't what happens during training."},{"from":1220.34,"to":1222.32,"location":2,"content":"I'll show you what happens during training later."},{"from":1222.32,"to":1223.61,"location":2,"content":"Uh, but this thing with the,"},{"from":1223.61,"to":1225.99,"location":2,"content":"the pink dotted arrows where you feed the word back in,"},{"from":1225.99,"to":1229.24,"location":2,"content":"this is what you do to generate text at test-time."},{"from":1229.24,"to":1235.34,"location":2,"content":"Any questions on this? Uh, oh,"},{"from":1235.34,"to":1239.94,"location":2,"content":"another thing I should note is that you need two separate sets of word embeddings, right?"},{"from":1239.94,"to":1243.36,"location":2,"content":"You need word embeddings for French words and you need English word embeddings."},{"from":1243.36,"to":1244.65,"location":2,"content":"That's kind of two separate sets,"},{"from":1244.65,"to":1248.94,"location":2,"content":"two separate vocabularies, um, yeah."},{"from":1248.94,"to":1251.65,"location":2,"content":"Okay. So, as a side note,"},{"from":1251.65,"to":1254.58,"location":2,"content":"this architecture called sequence-to-sequence is actually pretty versatile."},{"from":1254.58,"to":1256.77,"location":2,"content":"It's not just a machine translation architecture."},{"from":1256.77,"to":1258.73,"location":2,"content":"Uh, you can, ah,"},{"from":1258.73,"to":1263.52,"location":2,"content":"phrase quite a few NLP tasks as sequence-to-sequence tasks."},{"from":1263.52,"to":1265.59,"location":2,"content":"So, for example, a summarization is"},{"from":1265.59,"to":1270.11,"location":2,"content":"a sequence-to-sequence task because in goes your long text and out comes your short text."},{"from":1270.11,"to":1272.61,"location":2,"content":"Uh, dialogue can be seq2seq because in"},{"from":1272.61,"to":1275.31,"location":2,"content":"goes the previous utterance and out comes your next utterance."},{"from":1275.31,"to":1279.9,"location":2,"content":"Uh, parsing can even be thought of as a sequence-to-sequence task because you could"},{"from":1279.9,"to":1284.71,"location":2,"content":"say in goes the input text and the output parse is going to be expressed as a sequence."},{"from":1284.71,"to":1286.35,"location":2,"content":"This might not be the best way to do parsing,"},{"from":1286.35,"to":1288.02,"location":2,"content":"but it is a way you can try."},{"from":1288.02,"to":1291.46,"location":2,"content":"Lastly, you could even do something like code generation."},{"from":1291.46,"to":1294.16,"location":2,"content":"So, suppose you want to build a system that takes some kind of"},{"from":1294.16,"to":1298.58,"location":2,"content":"natural language inputs such as: sum up the numbers from 1 to 10,"},{"from":1298.58,"to":1299.88,"location":2,"content":"and then outputs, let's say,"},{"from":1299.88,"to":1305.37,"location":2,"content":"some Python code that says some open brackets range 10, or something like that."},{"from":1305.37,"to":1307.17,"location":2,"content":"So, if you wanted to train,"},{"from":1307.17,"to":1309.38,"location":2,"content":"um, a system to do this, you could,"},{"from":1309.38,"to":1313.72,"location":2,"content":"in a way, view that as a translation task where you're translating from English to Python."},{"from":1313.72,"to":1315.96,"location":2,"content":"It's a pretty challenging translation task,"},{"from":1315.96,"to":1318.49,"location":2,"content":"it probably requires a lot more logic than just, uh, you know,"},{"from":1318.49,"to":1321.35,"location":2,"content":"French to English, but you can try and people have tried."},{"from":1321.35,"to":1329.51,"location":2,"content":"There are research papers where people have used seq2seq to do this kind of task."},{"from":1329.51,"to":1332.23,"location":2,"content":"Okay. So, to recap,"},{"from":1332.23,"to":1335.23,"location":2,"content":"seq2seq is an example of a conditional language model."},{"from":1335.23,"to":1337.95,"location":2,"content":"It's a language model because the decoder is"},{"from":1337.95,"to":1340.89,"location":2,"content":"a language model that's predicting the next target word."},{"from":1340.89,"to":1343.17,"location":2,"content":"But it's a conditional language model because it's"},{"from":1343.17,"to":1345.56,"location":2,"content":"also conditioning on your source sentence,"},{"from":1345.56,"to":1350.15,"location":2,"content":"which is represented by the encoding of the source sentence."},{"from":1350.15,"to":1352.39,"location":2,"content":"So, you could look- er,"},{"from":1352.39,"to":1353.7,"location":2,"content":"you could view it like this."},{"from":1353.7,"to":1356.7,"location":2,"content":"NMT is directly calculating the probability"},{"from":1356.7,"to":1359.82,"location":2,"content":"of the target sentence y given the source sentence x."},{"from":1359.82,"to":1362.41,"location":2,"content":"So, if you look at this, you see that this is just, uh,"},{"from":1362.41,"to":1365.09,"location":2,"content":"breaking down the probability of the sequence y,"},{"from":1365.09,"to":1366.62,"location":2,"content":"which we suppose is of length, uh,"},{"from":1366.62,"to":1370.08,"location":2,"content":"T. You can break it down into the being the probability of"},{"from":1370.08,"to":1374.19,"location":2,"content":"the first word of y given x and then the probability of the second word of y given,"},{"from":1374.19,"to":1376.9,"location":2,"content":"ah, the words that came before and x, and so on."},{"from":1376.9,"to":1380.34,"location":2,"content":"So, in fact, you can see that each of the terms in this product on the right,"},{"from":1380.34,"to":1382.68,"location":2,"content":"those are probabilities of the next target word,"},{"from":1382.68,"to":1385.37,"location":2,"content":"given all the ones so far and also the source sentence."},{"from":1385.37,"to":1389.65,"location":2,"content":"And that's exactly the conditional probability that your language model produces."},{"from":1389.65,"to":1391.82,"location":2,"content":"So, the reason I'm highlighting this,"},{"from":1391.82,"to":1392.88,"location":2,"content":"is because if you remember,"},{"from":1392.88,"to":1398.19,"location":2,"content":"in SMT, we didn't directly learn the translation model P of y given x."},{"from":1398.19,"to":1402.21,"location":2,"content":"We broke it down into smaller components."},{"from":1402.21,"to":1403.99,"location":2,"content":"Whereas here in NMT,"},{"from":1403.99,"to":1406.5,"location":2,"content":"we are directly learning this model."},{"from":1406.5,"to":1409.34,"location":2,"content":"And this is in some ways an advantage because it's simpler to do."},{"from":1409.34,"to":1412.55,"location":2,"content":"You don't have to learn all these different systems and optimize them separately."},{"from":1412.55,"to":1416.69,"location":2,"content":"It's, uh, kind of simpler and easier."},{"from":1416.69,"to":1419.85,"location":2,"content":"So, uh, this is- this is the model that we're learning."},{"from":1419.85,"to":1421.05,"location":2,"content":"Uh, the question is,"},{"from":1421.05,"to":1423.16,"location":2,"content":"'how do we train this NMT system?'"},{"from":1423.16,"to":1426.16,"location":2,"content":"So, hopefully you should already have a good idea of how this would work"},{"from":1426.16,"to":1429.05,"location":2,"content":"given that we've already seen how you would train a language model."},{"from":1429.05,"to":1430.8,"location":2,"content":"But here are the details, just in case."},{"from":1430.8,"to":1433.56,"location":2,"content":"So, you get your big co- parallel corpus, uh,"},{"from":1433.56,"to":1438.87,"location":2,"content":"and then, uh, let's say you have your sentence pair from your parallel corpus."},{"from":1438.87,"to":1441.9,"location":2,"content":"Uh, so this is what happens during training."},{"from":1441.9,"to":1445.59,"location":2,"content":"You feed your source sentence into the encoder RNN, ah,"},{"from":1445.59,"to":1449.46,"location":2,"content":"and then you feed your target sentence into the decoder RNN,"},{"from":1449.46,"to":1450.72,"location":2,"content":"and you're going to pass over"},{"from":1450.72,"to":1454.01,"location":2,"content":"that final hidden state to be the initial hidden state of the decoder."},{"from":1454.01,"to":1458.43,"location":2,"content":"And then for every step of the decoder RNN,"},{"from":1458.43,"to":1459.99,"location":2,"content":"you're going to produce the, ah,"},{"from":1459.99,"to":1462.03,"location":2,"content":"probability distribution of what comes next,"},{"from":1462.03,"to":1463.77,"location":2,"content":"which is, ah, the y hats."},{"from":1463.77,"to":1465.38,"location":2,"content":"And then from those,"},{"from":1465.38,"to":1466.88,"location":2,"content":"you can compute your loss,"},{"from":1466.88,"to":1469.32,"location":2,"content":"and the loss is just the same as we saw for,"},{"from":1469.32,"to":1471.15,"location":2,"content":"um, unconditional language models."},{"from":1471.15,"to":1474.02,"location":2,"content":"It's, uh, the cross entropy or you could also say"},{"from":1474.02,"to":1477.33,"location":2,"content":"negative log-likelihood of the true next word."},{"from":1477.33,"to":1479.71,"location":2,"content":"So, for example, on the selected ones, uh,"},{"from":1479.71,"to":1485.31,"location":2,"content":"the loss is the negative log probability of the correct next word [NOISE]."},{"from":1485.31,"to":1487.68,"location":2,"content":"And then, as before, we're going to average all of"},{"from":1487.68,"to":1492.41,"location":2,"content":"these losses to get the total loss for the example."},{"from":1492.41,"to":1495.87,"location":2,"content":"So, I think you might notice people saying in, for example,"},{"from":1495.87,"to":1498.75,"location":2,"content":"research papers is this phrase end-to-end."},{"from":1498.75,"to":1502.41,"location":2,"content":"And, uh, this is an example of learning a system end-to-end."},{"from":1502.41,"to":1504.18,"location":2,"content":"And what we mean by this is that,"},{"from":1504.18,"to":1506.61,"location":2,"content":"the back propagation is happening end-to-end."},{"from":1506.61,"to":1508.99,"location":2,"content":"One end is- is losses, the loss functions,"},{"from":1508.99,"to":1512.92,"location":2,"content":"and the other end I guess is kind of like the- the beginning of the encoder RNN."},{"from":1512.92,"to":1514.21,"location":2,"content":"The point is that you,"},{"from":1514.21,"to":1515.92,"location":2,"content":"um, back propagation, uh,"},{"from":1515.92,"to":1519.19,"location":2,"content":"flows throughout the entire system and you learn"},{"from":1519.19,"to":1523.2,"location":2,"content":"the entire system with respect to this single loss. Yeah."},{"from":1523.2,"to":1529.71,"location":2,"content":"If the decoder outputs the [NOISE] how you can't handle the,"},{"from":1529.71,"to":1534.38,"location":2,"content":"the loss, would it get to accrue by [inaudible]"},{"from":1534.38,"to":1541.04,"location":2,"content":"The question is if the decoder RNN outputs the end token too early,"},{"from":1541.04,"to":1543.84,"location":2,"content":"then how can you measure the loss on,"},{"from":1543.84,"to":1545.7,"location":2,"content":"uh, the words that came after that?"},{"from":1545.7,"to":1548.85,"location":2,"content":"So this is the difference between training time and test time,"},{"from":1548.85,"to":1549.99,"location":2,"content":"which is pretty confusing."},{"from":1549.99,"to":1555.96,"location":2,"content":"So, uh, during training we have this picture where you feed the token back in."},{"from":1555.96,"to":1558.16,"location":2,"content":"So in this scenario once you produce emds,"},{"from":1558.16,"to":1562.08,"location":2,"content":"then you have to stop because you can't feed end in as the input on the next step."},{"from":1562.08,"to":1567.05,"location":2,"content":"But in training you don't feed the thing that you've produced into the next step."},{"from":1567.05,"to":1571.05,"location":2,"content":"During training you feed the target sentence from the corpus."},{"from":1571.05,"to":1574.42,"location":2,"content":"So like the goal target sentence into the model."},{"from":1574.42,"to":1576.72,"location":2,"content":"So, no matter what the, uh,"},{"from":1576.72,"to":1579.21,"location":2,"content":"the decoder predicts on a step,"},{"from":1579.21,"to":1584.15,"location":2,"content":"you kind of- you don't use that for anything other than computing loss."},{"from":1584.15,"to":1586.74,"location":2,"content":"Any other questions? Uh, yeah."},{"from":1586.74,"to":1589.98,"location":2,"content":"Is there a reason why you would- back propagate"},{"from":1589.98,"to":1593.89,"location":2,"content":"end-to-end instead of maybe training an encoder on the English model,"},{"from":1593.89,"to":1597.69,"location":2,"content":"and then, try maybe training a decoder separately and then putting them together?"},{"from":1597.69,"to":1601.53,"location":2,"content":"The question is, is there a reason why you would want to train end-to-end when,"},{"from":1601.53,"to":1605.2,"location":2,"content":"for example, you might want to train the encoder and the decoder separately?"},{"from":1605.2,"to":1608.58,"location":2,"content":"Uh, so I think, uh, people view training end-to-end is favorable,"},{"from":1608.58,"to":1612,"location":2,"content":"because the idea is that you can optimize the system as a whole."},{"from":1612,"to":1614.61,"location":2,"content":"You might think that if you optimize the parts separately,"},{"from":1614.61,"to":1618.08,"location":2,"content":"then when you put them together they will not be optimal together necessarily."},{"from":1618.08,"to":1621.96,"location":2,"content":"So, if possible, directly optimizing the thing that you care about- about,"},{"from":1621.96,"to":1623.46,"location":2,"content":"with respect to all of the parameters,"},{"from":1623.46,"to":1625.11,"location":2,"content":"is more likely to succeed."},{"from":1625.11,"to":1627.4,"location":2,"content":"However, there is a notion of pre-training,"},{"from":1627.4,"to":1629.84,"location":2,"content":"and as you said, maybe you'd want to learn your, um,"},{"from":1629.84,"to":1632.34,"location":2,"content":"decoder RNN as a kind of,"},{"from":1632.34,"to":1633.57,"location":2,"content":"uh, a language model,"},{"from":1633.57,"to":1635.3,"location":2,"content":"an unconditional language model by itself."},{"from":1635.3,"to":1636.81,"location":2,"content":"And that's something that people do."},{"from":1636.81,"to":1640.5,"location":2,"content":"You might, uh, learn a very strong language model and then use that to"},{"from":1640.5,"to":1644.6,"location":2,"content":"initialize your decoder RNN and then fine tune it on your task."},{"from":1644.6,"to":1648.83,"location":2,"content":"That's a- a valid thing you might try to do. Yep."},{"from":1648.83,"to":1651,"location":2,"content":"So, are these windows fixed?"},{"from":1651,"to":1656.43,"location":2,"content":"Like, are you always feeding the RNN [inaudible] to decode?"},{"from":1656.43,"to":1660.51,"location":2,"content":"The question is, is the length of the source sentence"},{"from":1660.51,"to":1662.28,"location":2,"content":"and the length of the target sentence fixed?"},{"from":1662.28,"to":1664.71,"location":2,"content":"So, for example, is this source sentence always length four?"},{"from":1664.71,"to":1667.32,"location":2,"content":"Um, no, that's definitely not true because in"},{"from":1667.32,"to":1669.9,"location":2,"content":"your parallel corpus you're gonna have sentences of all lengths."},{"from":1669.9,"to":1673.72,"location":2,"content":"Uh, so this is more kind of an implementation or a practicality question."},{"from":1673.72,"to":1676.8,"location":2,"content":"Uh, the idea is that, this is what you mathematically want to be"},{"from":1676.8,"to":1681.02,"location":2,"content":"computing during training for each example and you're going to have batches of examples."},{"from":1681.02,"to":1684.88,"location":2,"content":"But the question is, how do you actually implement that in, uh, in practice?"},{"from":1684.88,"to":1686.43,"location":2,"content":"So what you usually do,"},{"from":1686.43,"to":1689.07,"location":2,"content":"just because it's easier to assume that your batch is"},{"from":1689.07,"to":1692.54,"location":2,"content":"this kind of even sized tensor where everything is the same length,"},{"from":1692.54,"to":1695.2,"location":2,"content":"is you pad any short sentences,"},{"from":1695.2,"to":1697.52,"location":2,"content":"up to some predefined maximum length or"},{"from":1697.52,"to":1700.78,"location":2,"content":"maybe the length of the maximum example in your batch,"},{"from":1700.78,"to":1705.47,"location":2,"content":"and then you make sure that you don't use any hidden states that"},{"from":1705.47,"to":1711.93,"location":2,"content":"came from the padding. Yep."},{"from":1711.93,"to":1723.6,"location":2,"content":"Wouldn't you have to train every two images together,"},{"from":1723.6,"to":1728.68,"location":2,"content":"that would be kind of universal between similar languages or something like that?"},{"from":1728.68,"to":1730.26,"location":2,"content":"Okay, so the question,"},{"from":1730.26,"to":1731.94,"location":2,"content":"I think, is, um,"},{"from":1731.94,"to":1734.07,"location":2,"content":"it seems like sometimes you wouldn't want to train"},{"from":1734.07,"to":1736.17,"location":2,"content":"things end-to-end and there are circumstances in which you"},{"from":1736.17,"to":1740.2,"location":2,"content":"might want to train things separately and you mentioned for example having,"},{"from":1740.2,"to":1741.81,"location":2,"content":"uh, different languages mapped to each other."},{"from":1741.81,"to":1744.16,"location":2,"content":"So this is a totally valid point and in fact,"},{"from":1744.16,"to":1746.79,"location":2,"content":"so far we've kind of assumed that you want to learn"},{"from":1746.79,"to":1749.41,"location":2,"content":"language A to language B as a pair, right?"},{"from":1749.41,"to":1753.08,"location":2,"content":"And that's different to language A to language C or even language B to language A."},{"from":1753.08,"to":1757.55,"location":2,"content":"And, um, that does mean you have kind of n-squared many systems in the number of,"},{"from":1757.55,"to":1758.91,"location":2,"content":"uh, languages you're considering."},{"from":1758.91,"to":1762.49,"location":2,"content":"So, yeah, that's actually a valid idea and this is something that people have researched."},{"from":1762.49,"to":1764.64,"location":2,"content":"The idea that maybe you could have a kind of mix"},{"from":1764.64,"to":1766.8,"location":2,"content":"and match with your encoders and decoders,"},{"from":1766.8,"to":1768.29,"location":2,"content":"and you could try to, uh,"},{"from":1768.29,"to":1770.43,"location":2,"content":"train a kind of general purpose, let's say,"},{"from":1770.43,"to":1774.35,"location":2,"content":"English decoder and then match it up with your different encoders."},{"from":1774.35,"to":1775.77,"location":2,"content":"Uh, but this is, I think,"},{"from":1775.77,"to":1778.93,"location":2,"content":"fairly complex to train and to- to make sure that they all work together."},{"from":1778.93,"to":1781.28,"location":2,"content":"But that- that is certainly something that people have done."},{"from":1781.28,"to":1783.86,"location":2,"content":"Let me just check on the time."},{"from":1783.86,"to":1787.34,"location":2,"content":"Oh, okay, let's take one more question. Yep."},{"from":1787.34,"to":1793.31,"location":2,"content":"So doesn't word embedding also come from the same corpus that we are training on?"},{"from":1793.31,"to":1795.99,"location":2,"content":"The question is, does the word embedding"},{"from":1795.99,"to":1797.92,"location":2,"content":"also come from the corpus that you're training on?"},{"from":1797.92,"to":1800.71,"location":2,"content":"So, I think, there's a few options just as we saw with language models."},{"from":1800.71,"to":1802.38,"location":2,"content":"You could download, uh,"},{"from":1802.38,"to":1806.27,"location":2,"content":"pre-trained word vectors like Word2Vec or GloVE and you could use those,"},{"from":1806.27,"to":1807.87,"location":2,"content":"and then you can either kind of freeze them,"},{"from":1807.87,"to":1810.81,"location":2,"content":"or you could fine tune them as part of the end-to-end training,"},{"from":1810.81,"to":1813.53,"location":2,"content":"or you could just initialize your word vectors as,"},{"from":1813.53,"to":1817.14,"location":2,"content":"uh, you know, close to zero random and then learn them from scratch."},{"from":1817.14,"to":1819.59,"location":2,"content":"All right, okay, moving on."},{"from":1819.59,"to":1822.39,"location":2,"content":"Uh, so now we understand how you would train"},{"from":1822.39,"to":1825.15,"location":2,"content":"a neural machine translation system and we talked"},{"from":1825.15,"to":1828.78,"location":2,"content":"briefly about how you might do decoding or generation."},{"from":1828.78,"to":1832.02,"location":2,"content":"So what I showed you before is something called, uh, greedy decoding,"},{"from":1832.02,"to":1835.11,"location":2,"content":"which is this idea that on each step you just choose the argmax,"},{"from":1835.11,"to":1836.4,"location":2,"content":"the top one best word,"},{"from":1836.4,"to":1838.75,"location":2,"content":"and then you feed that in on the next step."},{"from":1838.75,"to":1842.39,"location":2,"content":"So this is called greedy decoding because you're just taking the best- uh,"},{"from":1842.39,"to":1844.31,"location":2,"content":"the best option that you can see right"},{"from":1844.31,"to":1847.17,"location":2,"content":"now and then you don't really have a way to go back."},{"from":1847.17,"to":1850.84,"location":2,"content":"So can anyone see a problem with this method?"},{"from":1850.84,"to":1854.66,"location":2,"content":"Maybe I've kind of given it away, but, uh- Yeah."},{"from":1854.66,"to":1860.13,"location":2,"content":"Too expensive, too complex."},{"from":1860.13,"to":1861.63,"location":2,"content":"You said too expensive."},{"from":1861.63,"to":1865.08,"location":2,"content":"I guess, I mean, it is expensive in that you have to do a sequence,"},{"from":1865.08,"to":1867.43,"location":2,"content":"and a sequence is usually worse than something you can do in parallel,"},{"from":1867.43,"to":1868.68,"location":2,"content":"but I suppose, um, maybe,"},{"from":1868.68,"to":1870,"location":2,"content":"what's wrong with the greediness?"},{"from":1870,"to":1871.83,"location":2,"content":"Can anyone suggests what's wrong with the greediness? Yeah."},{"from":1871.83,"to":1876.1,"location":2,"content":"When we take argmax [inaudible]."},{"from":1876.1,"to":1878.97,"location":2,"content":"Yes, that's when you take an argmax on a token, it's"},{"from":1878.97,"to":1881.81,"location":2,"content":"not necessarily going to give you the argmax over the entire sentence."},{"from":1881.81,"to":1883.34,"location":2,"content":"That's exactly right, that's, uh,"},{"from":1883.34,"to":1886.13,"location":2,"content":"kind of, what, uh, what greediness means."},{"from":1886.13,"to":1889.08,"location":2,"content":"So in practice, this might give you something like this."},{"from":1889.08,"to":1893.16,"location":2,"content":"Uh, we're trying to translate our running example sentence and then, let's suppose,"},{"from":1893.16,"to":1895.32,"location":2,"content":"on the first step we say, he, and then we say,"},{"from":1895.32,"to":1896.98,"location":2,"content":"he hit, and then he- we say,"},{"from":1896.98,"to":1898.44,"location":2,"content":"he hit a, oh no."},{"from":1898.44,"to":1900.62,"location":2,"content":"That wasn't right, that wasn't the best thing to choose,"},{"from":1900.62,"to":1902.85,"location":2,"content":"but we kind of have no way to go back now, right?"},{"from":1902.85,"to":1906.27,"location":2,"content":"We just have to continue and try to make the best of it after saying he hit a,"},{"from":1906.27,"to":1908.73,"location":2,"content":"which, uh, isn't going to work out well."},{"from":1908.73,"to":1911.28,"location":2,"content":"So that's the main problem with greedy decoding; there's, kind of,"},{"from":1911.28,"to":1913.96,"location":2,"content":"no way to backtrack, no way to go back."},{"from":1913.96,"to":1916.45,"location":2,"content":"So, how can we fix this?"},{"from":1916.45,"to":1917.91,"location":2,"content":"And this relates back to, uh,"},{"from":1917.91,"to":1919.8,"location":2,"content":"what I told you earlier about how we might use,"},{"from":1919.8,"to":1924.32,"location":2,"content":"uh, a kind of searching algorithm to do decoding in SMT."},{"from":1924.32,"to":1927.8,"location":2,"content":"Uh, but first, you might,"},{"from":1927.8,"to":1929.98,"location":2,"content":"uh, think exhaustive search is a good idea."},{"from":1929.98,"to":1933.12,"location":2,"content":"Well, probably not because it's still a bad idea for the same reasons as before."},{"from":1933.12,"to":1935.76,"location":2,"content":"So, if you did want to do exhaustive search and search through"},{"from":1935.76,"to":1938.6,"location":2,"content":"the space of all possible French translations, uh,"},{"from":1938.6,"to":1939.9,"location":2,"content":"then you would be again,"},{"from":1939.9,"to":1943.02,"location":2,"content":"trying to consider which Y maximizes,"},{"from":1943.02,"to":1946.88,"location":2,"content":"uh, this product of all of these individual probability distributions."},{"from":1946.88,"to":1950.1,"location":2,"content":"So as before, if you tried to do this, uh,"},{"from":1950.1,"to":1954.16,"location":2,"content":"then on each step T of the decoder you're going to be having to track, uh,"},{"from":1954.16,"to":1957.72,"location":2,"content":"V to the power of t possible partial translations,"},{"from":1957.72,"to":1960.32,"location":2,"content":"uh, where V is your vocabulary size."},{"from":1960.32,"to":1963.06,"location":2,"content":"So here when I say partial translation, I just mean, uh,"},{"from":1963.06,"to":1966.91,"location":2,"content":"kind of, you know, like half of a sentence so far or something like that."},{"from":1966.91,"to":1968.85,"location":2,"content":"So of course this, uh,"},{"from":1968.85,"to":1972,"location":2,"content":"exponential in V complexity is just far too expensive."},{"from":1972,"to":1974.79,"location":2,"content":"So, yes, we're going to use some kind of search algorithm."},{"from":1974.79,"to":1976.1,"location":2,"content":"And in particular we're going to use,"},{"from":1976.1,"to":1977.95,"location":2,"content":"uh, Beam search decoding."},{"from":1977.95,"to":1982.47,"location":2,"content":"So the core idea of Beam search decoding is that on each step of"},{"from":1982.47,"to":1988.52,"location":2,"content":"the decoder you're going to be keeping track of the k most probable partial translations."},{"from":1988.52,"to":1990.97,"location":2,"content":"And we call partial translations hypotheses"},{"from":1990.97,"to":1993.08,"location":2,"content":"because we're kind of tracking multiple of them,"},{"from":1993.08,"to":1994.41,"location":2,"content":"and we're not sure which one is best,"},{"from":1994.41,"to":1996.56,"location":2,"content":"so we're thinking about several."},{"from":1996.56,"to":1998.54,"location":2,"content":"Here k is an"},{"from":1998.54,"to":2001.22,"location":2,"content":"integer and we call this the beam size."},{"from":2001.22,"to":2002.84,"location":2,"content":"And in practice, for an MT,"},{"from":2002.84,"to":2005.09,"location":2,"content":"this is usually maybe 5 to 10."},{"from":2005.09,"to":2007.84,"location":2,"content":"So, you can think of k kind of as,"},{"from":2007.84,"to":2010.34,"location":2,"content":"how big is your search space at any one time."},{"from":2010.34,"to":2013.19,"location":2,"content":"So if you increase k, then you're going to be considering, uh,"},{"from":2013.19,"to":2015.62,"location":2,"content":"more different options on each step and you might"},{"from":2015.62,"to":2018.41,"location":2,"content":"hope that this will mean that you get a better quality solution in the end,"},{"from":2018.41,"to":2021.01,"location":2,"content":"though of course it will be more expansive."},{"from":2021.01,"to":2024.02,"location":2,"content":"So, I said that we want to keep track of"},{"from":2024.02,"to":2027.85,"location":2,"content":"the k most probable partial translations, that is, hypotheses."},{"from":2027.85,"to":2030.21,"location":2,"content":"So this means that we need some kind of notion of, you know,"},{"from":2030.21,"to":2033.17,"location":2,"content":"how probable is this hypothesis or what's its score."},{"from":2033.17,"to":2036.05,"location":2,"content":"So, the score of the hypothesis, and uh,"},{"from":2036.05,"to":2039.88,"location":2,"content":"we're representing that as Y1 up to Yt,"},{"from":2039.88,"to":2042.98,"location":2,"content":"is just its log probability."},{"from":2042.98,"to":2046.91,"location":2,"content":"So, uh, the log probability of this partial translation,"},{"from":2046.91,"to":2048.49,"location":2,"content":"uh, according to the language model,"},{"from":2048.49,"to":2051.01,"location":2,"content":"can be broken down as we saw before into the sum"},{"from":2051.01,"to":2053.74,"location":2,"content":"of the individual log probabilities of the words,"},{"from":2053.74,"to":2056.92,"location":2,"content":"given everything that came before."},{"from":2056.92,"to":2059.38,"location":2,"content":"So it's- if it's not obvious,"},{"from":2059.38,"to":2061.03,"location":2,"content":"these scores are all negative,"},{"from":2061.03,"to":2062.09,"location":2,"content":"because we're taking log of,"},{"from":2062.09,"to":2064.07,"location":2,"content":"uh, of a number between zero and one."},{"from":2064.07,"to":2070.16,"location":2,"content":"Uh, and a higher score is better, yes,"},{"from":2070.16,"to":2073.46,"location":2,"content":"because you want a higher probability of, uh,"},{"from":2073.46,"to":2077.45,"location":2,"content":"of the hypothesis according to the language model."},{"from":2077.45,"to":2080.66,"location":2,"content":"So, the idea is that we're going to use this score, uh,"},{"from":2080.66,"to":2082.19,"location":2,"content":"and the search algorithm to search for"},{"from":2082.19,"to":2086.26,"location":2,"content":"high-scoring hypotheses and we're going to track the top k on each step."},{"from":2086.26,"to":2089.27,"location":2,"content":"So, I'll show- I'm going to show you a detailed example in a moment."},{"from":2089.27,"to":2091.49,"location":2,"content":"But the, uh, important things to note are"},{"from":2091.49,"to":2094.91,"location":2,"content":"that Beam search is not guaranteed to find an optimal solution."},{"from":2094.91,"to":2097.28,"location":2,"content":"Uh, exhaustive search, the one where you numate-"},{"from":2097.28,"to":2099.8,"location":2,"content":"enumerate all V to the T possible translations,"},{"from":2099.8,"to":2102.11,"location":2,"content":"that is guaranteed to find the optimal solution but it's just"},{"from":2102.11,"to":2104.96,"location":2,"content":"completely infeasible because it's so expe- expensive."},{"from":2104.96,"to":2108.05,"location":2,"content":"So Beam search is not guaranteed to find the optimal solution"},{"from":2108.05,"to":2111.86,"location":2,"content":"but it is a much more efficient and exhaustive search, of course."},{"from":2111.86,"to":2118.05,"location":2,"content":"All right, um, will- is the question- would the question be solved by seeing an example?"},{"from":2118.05,"to":2120.17,"location":2,"content":"Uh, I was just wondering, um,"},{"from":2120.17,"to":2122.36,"location":2,"content":"if you guys [NOISE] also find partial filtering or it gets-"},{"from":2122.36,"to":2125.8,"location":2,"content":"[inaudible] ."},{"from":2125.8,"to":2133.2,"location":2,"content":"Um, I'm not entirely sure about that."},{"from":2133.2,"to":2135.7,"location":2,"content":"I mean you certainly do some kind of sampling sometimes um,"},{"from":2135.7,"to":2137.5,"location":2,"content":"and that's something I'm gonna talk more about later in"},{"from":2137.5,"to":2139.39,"location":2,"content":"the natural language generation lecture."},{"from":2139.39,"to":2143.67,"location":2,"content":"Um, so I haven't heard of those being applied here."},{"from":2143.67,"to":2145.39,"location":2,"content":"Okay."},{"from":2145.39,"to":2149.54,"location":2,"content":"Okay. Um, so here's an example of beam search decoding in action."},{"from":2149.54,"to":2153.24,"location":2,"content":"Um, so let's suppose that beam size equals K, um, is 2."},{"from":2153.24,"to":2154.73,"location":2,"content":"And then as a reminder,"},{"from":2154.73,"to":2159.64,"location":2,"content":"we have- this is the school that you apply to a partial, um, hypothesis."},{"from":2159.64,"to":2162.59,"location":2,"content":"Um, partial translation which is a hypothesis."},{"from":2162.59,"to":2165.23,"location":2,"content":"So we start off with our starting token."},{"from":2165.23,"to":2167.24,"location":2,"content":"And the idea is that we're going to compute"},{"from":2167.24,"to":2170.42,"location":2,"content":"the probability distribution of what word might come next."},{"from":2170.42,"to":2174.53,"location":2,"content":"So, having computed that probability distribution using our seq2seq model,"},{"from":2174.53,"to":2176.09,"location":2,"content":"then we just take the top k,"},{"from":2176.09,"to":2178.1,"location":2,"content":"that is top two possible options."},{"from":2178.1,"to":2181.36,"location":2,"content":"So let's suppose that the top two are the words he and I."},{"from":2181.36,"to":2185.57,"location":2,"content":"So the idea is that we can compute the score of these two hypotheses,"},{"from":2185.57,"to":2188.21,"location":2,"content":"uh, by using the formula above."},{"from":2188.21,"to":2192.49,"location":2,"content":"It's just the log probability of this word given the context so far."},{"from":2192.49,"to":2197.8,"location":2,"content":"So here let's say that he has a score of minus 0.7 and I has a score of minus 0.9."},{"from":2197.8,"to":2200.39,"location":2,"content":"So this means that he is currently the better one."},{"from":2200.39,"to":2202.2,"location":2,"content":"Okay. So what we do is,"},{"from":2202.2,"to":2205.32,"location":2,"content":"we have our two, our k hypotheses."},{"from":2205.32,"to":2207.36,"location":2,"content":"And then for each of those,"},{"from":2207.36,"to":2210.8,"location":2,"content":"we find the top k words that could come next."},{"from":2210.8,"to":2212.66,"location":2,"content":"And we calculate their scores."},{"from":2212.66,"to":2214.47,"location":2,"content":"So this means that for both he and I,"},{"from":2214.47,"to":2216.71,"location":2,"content":"we find the top two words that could come next."},{"from":2216.71,"to":2218.84,"location":2,"content":"And for each of these four possibilities,"},{"from":2218.84,"to":2221.75,"location":2,"content":"um, the school of the hypothesis is equal to,"},{"from":2221.75,"to":2225.14,"location":2,"content":"um, the log probability of this new word given the context so far,"},{"from":2225.14,"to":2226.76,"location":2,"content":"plus the score so far."},{"from":2226.76,"to":2229.34,"location":2,"content":"Because you can accumulate this sum of log probability."},{"from":2229.34,"to":2232.75,"location":2,"content":"You didn't have to compute it from scratch each time."},{"from":2232.75,"to":2236.99,"location":2,"content":"So here you can see that we have these four possibilities and that"},{"from":2236.99,"to":2241.26,"location":2,"content":"the top two schools are minus 1.6 and minus 1.7."},{"from":2241.26,"to":2243.44,"location":2,"content":"So this means that hit and was,"},{"from":2243.44,"to":2244.84,"location":2,"content":"are the two best ones."},{"from":2244.84,"to":2248.22,"location":2,"content":"So the idea is that all of these k squared equals four hypotheses."},{"from":2248.22,"to":2251.45,"location":2,"content":"We're just gonna keep the k equals to top ones."},{"from":2251.45,"to":2253.49,"location":2,"content":"And then we just keep doing the same thing."},{"from":2253.49,"to":2256.63,"location":2,"content":"For these two, we expand to get the two next ones."},{"from":2256.63,"to":2258.95,"location":2,"content":"And then of those, we compute the scores,"},{"from":2258.95,"to":2262.74,"location":2,"content":"and then we keep the two best ones and discard the others."},{"from":2262.74,"to":2264.57,"location":2,"content":"And then of those, we expand."},{"from":2264.57,"to":2266.33,"location":2,"content":"So we keep doing this again and again."},{"from":2266.33,"to":2269.68,"location":2,"content":"Expanding and then just keeping the top k and expanding,"},{"from":2269.68,"to":2272.08,"location":2,"content":"like this until, um,"},{"from":2272.08,"to":2274.7,"location":2,"content":"you get some kind of, um, finished translation."},{"from":2274.7,"to":2278.22,"location":2,"content":"I'm gonna tell you more in a moment about what exactly the stopping criterion is."},{"from":2278.22,"to":2280.23,"location":2,"content":"But let's suppose that we stop here."},{"from":2280.23,"to":2283.78,"location":2,"content":"Looking at the four hypotheses that we have on the far right,"},{"from":2283.78,"to":2285.83,"location":2,"content":"the one with the top score is, um,"},{"from":2285.83,"to":2288.91,"location":2,"content":"the top pie one with minus 4.3."},{"from":2288.91,"to":2290.84,"location":2,"content":"So let's suppose that we are going to stop now and we"},{"from":2290.84,"to":2292.61,"location":2,"content":"decide that this is the top hypothesis,"},{"from":2292.61,"to":2295.01,"location":2,"content":"then all we need to do is just backtrack"},{"from":2295.01,"to":2297.71,"location":2,"content":"through this tree in order to find the full translation,"},{"from":2297.71,"to":2300.43,"location":2,"content":"which is, he hit me with a pie."},{"from":2300.43,"to":2303.38,"location":2,"content":"All right. So, um, let me tell you more detail"},{"from":2303.38,"to":2305.82,"location":2,"content":"about how exactly we decide when to stop."},{"from":2305.82,"to":2308.2,"location":2,"content":"So if you remember in greedy decoding,"},{"from":2308.2,"to":2312.01,"location":2,"content":"usually we just keep decoding until the model produces the end token."},{"from":2312.01,"to":2315.71,"location":2,"content":"So for example, this means that your model is actually producing the sequence."},{"from":2315.71,"to":2317.93,"location":2,"content":"Um, I guess it doesn't produce START and you give it START."},{"from":2317.93,"to":2319.32,"location":2,"content":"But then it produces the sequence,"},{"from":2319.32,"to":2321.92,"location":2,"content":"he hit me with a pie, end."},{"from":2321.92,"to":2324.95,"location":2,"content":"So the problem in Beam search decoding is"},{"from":2324.95,"to":2327.59,"location":2,"content":"that you're considering all these different hypotheses,"},{"from":2327.59,"to":2329.45,"location":2,"content":"k different hypotheses at once."},{"from":2329.45,"to":2333.91,"location":2,"content":"And the thing is, those hypotheses might produce n tokens at different times."},{"from":2333.91,"to":2336.76,"location":2,"content":"So there's no one obvious place to stop."},{"from":2336.76,"to":2338.96,"location":2,"content":"So what we do in practice is,"},{"from":2338.96,"to":2341.16,"location":2,"content":"when a hypothesis produces the END token,"},{"from":2341.16,"to":2345.57,"location":2,"content":"then we regard this hypothesis as complete and we kind of place it aside."},{"from":2345.57,"to":2347.84,"location":2,"content":"We have a collection of competed hypotheses."},{"from":2347.84,"to":2349.64,"location":2,"content":"Um, so we kind of take it out of Beam search."},{"from":2349.64,"to":2352.22,"location":2,"content":"We no longer keep exploring it, because it's finished."},{"from":2352.22,"to":2354.45,"location":2,"content":"Um, and we, yeah, place it aside."},{"from":2354.45,"to":2358.36,"location":2,"content":"And you continue exploring other hypotheses with Beam search."},{"from":2358.36,"to":2360.02,"location":2,"content":"So the remaining question is,"},{"from":2360.02,"to":2361.76,"location":2,"content":"when do you stop doing Beam search?"},{"from":2361.76,"to":2364.19,"location":2,"content":"When do you stop iterating through this algorithm?"},{"from":2364.19,"to":2367.74,"location":2,"content":"So there's, um, multiple possible stopping criterions,"},{"from":2367.74,"to":2370.53,"location":2,"content":"but two common ones are, you might say, um,"},{"from":2370.53,"to":2373.79,"location":2,"content":"\"We're gonna stop doing Beam search once we reach time step t,"},{"from":2373.79,"to":2375.18,"location":2,"content":"where t is some, um,"},{"from":2375.18,"to":2376.79,"location":2,"content":"predefined threshold that you choose.\""},{"from":2376.79,"to":2380.27,"location":2,"content":"So you might say, um, \"We're gonna stop Beam search after 30 steps,"},{"from":2380.27,"to":2384.4,"location":2,"content":"because we don't want any output sentences that are longer than 30 words, for example."},{"from":2384.4,"to":2385.74,"location":2,"content":"Or you might say, um,"},{"from":2385.74,"to":2387.55,"location":2,"content":"\"We're gonna stop doing Beam search once we've"},{"from":2387.55,"to":2390.04,"location":2,"content":"collected at least n completed hypotheses.\""},{"from":2390.04,"to":2391.15,"location":2,"content":"So you might say, um,"},{"from":2391.15,"to":2397.18,"location":2,"content":"\"I want at least 10 complete translations before I stop doing Beam search.\""},{"from":2397.18,"to":2400.43,"location":2,"content":"Okay. So what's the final thing you have to do?"},{"from":2400.43,"to":2405.23,"location":2,"content":"Uh, we finished doing Beam search and we have this collection of completed hypotheses."},{"from":2405.23,"to":2407.16,"location":2,"content":"Um, we want to choose the top one."},{"from":2407.16,"to":2409.47,"location":2,"content":"Um, the one that we're going to use as our translation."},{"from":2409.47,"to":2413.99,"location":2,"content":"So, um, how do we select the top one that has the highest score?"},{"from":2413.99,"to":2416,"location":2,"content":"Um, you might think this is simple given that all of"},{"from":2416,"to":2418.52,"location":2,"content":"these hypotheses already have scores attached."},{"from":2418.52,"to":2420.07,"location":2,"content":"But if we just look at this, um,"},{"from":2420.07,"to":2424.79,"location":2,"content":"formula again, um, for what the score is of each hypothesis."},{"from":2424.79,"to":2427.79,"location":2,"content":"Um, can anyone see a problem with this?"},{"from":2427.79,"to":2430.59,"location":2,"content":"If we have our set of hypotheses and then"},{"from":2430.59,"to":2433.51,"location":2,"content":"we're choosing the top one based on the one that has the best score,"},{"from":2433.51,"to":2437.09,"location":2,"content":"can anyone see a problem? Yep?"},{"from":2437.09,"to":2438.83,"location":2,"content":"You choose the shortest one?"},{"from":2438.83,"to":2440.05,"location":2,"content":"Yes. So the answer was,"},{"from":2440.05,"to":2441.77,"location":2,"content":"you're gonna end up choosing the shortest one."},{"from":2441.77,"to":2446.84,"location":2,"content":"Um, the problem here is that longer hypotheses have lower scores in general."},{"from":2446.84,"to":2450.84,"location":2,"content":"Because you're, um, multiplying more probabilities and getting a smaller,"},{"from":2450.84,"to":2452.16,"location":2,"content":"a smaller overall value."},{"from":2452.16,"to":2453.83,"location":2,"content":"Or I guess if we're adding more to the t,"},{"from":2453.83,"to":2455.53,"location":2,"content":"we're gonna get more negative values."},{"from":2455.53,"to":2459.5,"location":2,"content":"So it's not quite that you'll definitely choose the shortest hypothesis,"},{"from":2459.5,"to":2463.09,"location":2,"content":"because you could overall have a lower score."},{"from":2463.09,"to":2466.19,"location":2,"content":"But there's definitely going to be a bias towards shorter translations."},{"from":2466.19,"to":2468.99,"location":2,"content":"Because they'll in general have lower scores."},{"from":2468.99,"to":2471.29,"location":2,"content":"So the way you can fix this is pretty simple,"},{"from":2471.29,"to":2472.91,"location":2,"content":"you just normalize by length."},{"from":2472.91,"to":2474.91,"location":2,"content":"So instead of using the score that we have above,"},{"from":2474.91,"to":2478.1,"location":2,"content":"you're going to use the score divided by [inaudible]"},{"from":2478.1,"to":2480.59,"location":2,"content":"and the length of the hypothesis."},{"from":2480.59,"to":2483.65,"location":2,"content":"And then you use this, just like the top one."},{"from":2483.65,"to":2493.59,"location":2,"content":"Any questions on this? Yeah."},{"from":2493.59,"to":2501.14,"location":2,"content":"Do we train with the end token, so that it's possible to [inaudible] [NOISE]."},{"from":2501.14,"to":2502.13,"location":2,"content":"[NOISE] I didn't quite hear, do we train with the end token?"},{"from":2502.13,"to":2506.36,"location":2,"content":"Yeah. Like we added a token [inaudible]."},{"from":2506.36,"to":2509.11,"location":2,"content":"Yeah. So you train with the end token, if that's your question."},{"from":2509.11,"to":2512.6,"location":2,"content":"Um, because the whole point is you're relying on your language model,"},{"from":2512.6,"to":2516.8,"location":2,"content":"your decoder to produce the end token in order to know when to stop."},{"from":2516.8,"to":2518,"location":2,"content":"So you need to train,"},{"from":2518,"to":2521.03,"location":2,"content":"it's produce the end token by giving it examples of training sentences"},{"from":2521.03,"to":2526.14,"location":2,"content":"with end tokens. Yeah."},{"from":2526.14,"to":2527.84,"location":2,"content":"Why don't we use this score,"},{"from":2527.84,"to":2529.68,"location":2,"content":"the one at the bottom of the screen during Beam search."},{"from":2529.68,"to":2531.32,"location":2,"content":"Great question. The question is,"},{"from":2531.32,"to":2532.93,"location":2,"content":"why don't we use this normalized score,"},{"from":2532.93,"to":2534.18,"location":2,"content":"the one at the bottom of the screen,"},{"from":2534.18,"to":2535.99,"location":2,"content":"during Beam search in the first place."},{"from":2535.99,"to":2537.84,"location":2,"content":"So the reason why that's not necessary,"},{"from":2537.84,"to":2539.48,"location":2,"content":"you could, but it's not necessary."},{"from":2539.48,"to":2541.46,"location":2,"content":"Is because during Beam search,"},{"from":2541.46,"to":2546.76,"location":2,"content":"we only ever compare the scores of hypotheses that have the same length, right?"},{"from":2546.76,"to":2548,"location":2,"content":"So on each of these steps."},{"from":2548,"to":2551.81,"location":2,"content":"So when we look at, let's say the top k squared and we want to choose which ones are"},{"from":2551.81,"to":2553.91,"location":2,"content":"the top k. We're comparing"},{"from":2553.91,"to":2556.91,"location":2,"content":"the scores of four different hypotheses that are of length one,"},{"from":2556.91,"to":2558.49,"location":2,"content":"two, three, four, five."},{"from":2558.49,"to":2562.79,"location":2,"content":"So, um, it's true that these scores are getting lower and lower."},{"from":2562.79,"to":2570.7,"location":2,"content":"But, in the same way, because they're all length five right now."},{"from":2570.7,"to":2574.21,"location":2,"content":"Okay. So, we now understand how you would train"},{"from":2574.21,"to":2575.95,"location":2,"content":"an NMT system and how would you,"},{"from":2575.95,"to":2578.83,"location":2,"content":"you would use your trained NMT system to"},{"from":2578.83,"to":2582.25,"location":2,"content":"generate your translations using let's say, Beam search."},{"from":2582.25,"to":2585.4,"location":2,"content":"So, let's, uh, take a step back and think about what are"},{"from":2585.4,"to":2588.79,"location":2,"content":"the overall advantages of NMT in comparison to SMT."},{"from":2588.79,"to":2593.65,"location":2,"content":"[NOISE] Uh, so, the first advantage is just better performance."},{"from":2593.65,"to":2598.33,"location":2,"content":"Uh, NMT systems tend to give better output than SMT systems in several ways."},{"from":2598.33,"to":2601.42,"location":2,"content":"One is that the output often tends to be more fluent."},{"from":2601.42,"to":2603.99,"location":2,"content":"Uh, this is probably because NMT, uh,"},{"from":2603.99,"to":2606.13,"location":2,"content":"this is probably because RNNs are particularly good at"},{"from":2606.13,"to":2608.65,"location":2,"content":"learning language models, as you learned last week."},{"from":2608.65,"to":2611.17,"location":2,"content":"Uh, another way that they're better is they're often use,"},{"from":2611.17,"to":2612.72,"location":2,"content":"uh, the context better."},{"from":2612.72,"to":2615.16,"location":2,"content":"That is, uh, they're better at conditioning on"},{"from":2615.16,"to":2618.58,"location":2,"content":"the source sentence and using that to change the output."},{"from":2618.58,"to":2621.7,"location":2,"content":"Another way they're better is they often, uh,"},{"from":2621.7,"to":2624.64,"location":2,"content":"are more able to generalize what they learn"},{"from":2624.64,"to":2626.45,"location":2,"content":"about phrases and how to translate them."},{"from":2626.45,"to":2629.59,"location":2,"content":"So, for example, if it sees an example of how to translate"},{"from":2629.59,"to":2631.66,"location":2,"content":"a certain source phrase and then later it"},{"from":2631.66,"to":2634.51,"location":2,"content":"sees a slightly different version of that source phrase,"},{"from":2634.51,"to":2638.39,"location":2,"content":"it's, uh, more able to generalize what it learned about the first phrase,"},{"from":2638.39,"to":2641.6,"location":2,"content":"than SMT system as well."},{"from":2641.6,"to":2644.6,"location":2,"content":"Another big advantage of NMT systems"},{"from":2644.6,"to":2646.23,"location":2,"content":"compared to SMT that we talked about"},{"from":2646.23,"to":2648.51,"location":2,"content":"before is that it's a single neural network"},{"from":2648.51,"to":2650.16,"location":2,"content":"that can be optimized end-to-end."},{"from":2650.16,"to":2652.66,"location":2,"content":"And the advantage here I suppose is primarily"},{"from":2652.66,"to":2655.26,"location":2,"content":"simplicity and convenience."},{"from":2655.26,"to":2659.91,"location":2,"content":"So, there's no sub-components that need to be individually optimized."},{"from":2659.91,"to":2664.69,"location":2,"content":"Another big advantage is that it requires much less human engineering efforts."},{"from":2664.69,"to":2666.64,"location":2,"content":"When I told you earlier about all the different things"},{"from":2666.64,"to":2668.64,"location":2,"content":"that people had to do to build, uh,"},{"from":2668.64,"to":2671.22,"location":2,"content":"big, uh, powerful SMT systems,"},{"from":2671.22,"to":2674.22,"location":2,"content":"uh, there's relatively less engineering effort for NMT."},{"from":2674.22,"to":2678.14,"location":2,"content":"NMT is certainly not easy but it's- it's less complicated than SMT."},{"from":2678.14,"to":2680.91,"location":2,"content":"In particular, there's no feature engineering."},{"from":2680.91,"to":2682.9,"location":2,"content":"You don't have to define what features"},{"from":2682.9,"to":2685.18,"location":2,"content":"of linguistic phenomena that you want to capture."},{"from":2685.18,"to":2688.28,"location":2,"content":"You can mostly just view it as a sequence of words although,"},{"from":2688.28,"to":2692.19,"location":2,"content":"uh, there are different views on that."},{"from":2692.19,"to":2695.59,"location":2,"content":"Uh, lastly, a great thing about NMT is that you can"},{"from":2695.59,"to":2698.66,"location":2,"content":"use pretty much the same method for all language pairs."},{"from":2698.66,"to":2700.03,"location":2,"content":"So, if you've, uh, you know,"},{"from":2700.03,"to":2701.95,"location":2,"content":"built your French to English translation system and"},{"from":2701.95,"to":2703.95,"location":2,"content":"now you want to build a Spanish to English one, uh,"},{"from":2703.95,"to":2707.03,"location":2,"content":"you can probably use basically the same architecture and the same method"},{"from":2707.03,"to":2711.54,"location":2,"content":"as long as you can go find a big enough parallel corpus of Spanish to English."},{"from":2711.54,"to":2715.72,"location":2,"content":"All right. So, what are the disadvantages of NMT, uh, remaining?"},{"from":2715.72,"to":2717.22,"location":2,"content":"So, compared to SMT,"},{"from":2717.22,"to":2718.75,"location":2,"content":"there are some disadvantages."},{"from":2718.75,"to":2721.63,"location":2,"content":"One is that NMT is less interpretable."},{"from":2721.63,"to":2724.66,"location":2,"content":"Uh, what I mean by this is you feed in"},{"from":2724.66,"to":2728.98,"location":2,"content":"your source sentence into the neural network and then it feeds out some target sentence,"},{"from":2728.98,"to":2732.82,"location":2,"content":"and you don't really have any way to figure out why that happened."},{"from":2732.82,"to":2736.38,"location":2,"content":"Right? So, in particular, if the target sentence could contain some kind of error,"},{"from":2736.38,"to":2739.64,"location":2,"content":"um, you can't really look at the neurons and understand what happened."},{"from":2739.64,"to":2741.46,"location":2,"content":"It's pretty hard to attribute errors."},{"from":2741.46,"to":2742.87,"location":2,"content":"So, this means that, uh,"},{"from":2742.87,"to":2744.88,"location":2,"content":"NMT systems are pretty hard to debug."},{"from":2744.88,"to":2749.35,"location":2,"content":"So, by comparison, SMT systems were more interpretable,"},{"from":2749.35,"to":2752.23,"location":2,"content":"in that you had all of these different sub-components that were doing"},{"from":2752.23,"to":2755.26,"location":2,"content":"different jobs and you were more able to look at those."},{"from":2755.26,"to":2757.57,"location":2,"content":"They weren't, you know, neurons often would be, uh,"},{"from":2757.57,"to":2760.54,"location":2,"content":"you know, probabilities of certain words given other words and so on."},{"from":2760.54,"to":2762.73,"location":2,"content":"And, you know, that's by no means easy to"},{"from":2762.73,"to":2765.94,"location":2,"content":"interpret but it was at least more interpretable than NMT."},{"from":2765.94,"to":2770.99,"location":2,"content":"Uh, another disadvantage is NMT is pretty difficult to control."},{"from":2770.99,"to":2772.87,"location":2,"content":"So, uh, for example,"},{"from":2772.87,"to":2775.15,"location":2,"content":"if your NMT system is,"},{"from":2775.15,"to":2776.68,"location":2,"content":"uh, doing a particular error,"},{"from":2776.68,"to":2779.92,"location":2,"content":"it's not very easy for you the programmer to"},{"from":2779.92,"to":2783.94,"location":2,"content":"specify some kind of rule or guideline that you want the NMT system to follow."},{"from":2783.94,"to":2785.57,"location":2,"content":"So, for example if you want to say,"},{"from":2785.57,"to":2789.03,"location":2,"content":"\"I want to always translate this word in this way,"},{"from":2789.03,"to":2791.95,"location":2,"content":"um, when- when this other thing is present.\""},{"from":2791.95,"to":2795.41,"location":2,"content":"Like that's not particularly easy to, uh,"},{"from":2795.41,"to":2798.28,"location":2,"content":"to impose as a rule on the NMT system, uh,"},{"from":2798.28,"to":2799.89,"location":2,"content":"because you can't, uh,"},{"from":2799.89,"to":2803.2,"location":2,"content":"easily control what it's doing on a step-by-step basis."},{"from":2803.2,"to":2804.76,"location":2,"content":"So, sometimes you have some kind of,"},{"from":2804.76,"to":2806.97,"location":2,"content":"uh, post-processing rules you might try to do."},{"from":2806.97,"to":2808.9,"location":2,"content":"But overall, you can't,"},{"from":2808.9,"to":2811.64,"location":2,"content":"it's- it's- it's harder than you'd expect to try to, um,"},{"from":2811.64,"to":2815.53,"location":2,"content":"impose a fairly simple rule."},{"from":2815.53,"to":2818.01,"location":2,"content":"[NOISE] So, this means that you have some kind of safety concerns in fact."},{"from":2818.01,"to":2819.25,"location":2,"content":"Because, uh, let's say, you know,"},{"from":2819.25,"to":2822.31,"location":2,"content":"you don't want your NMT system to say bad things, right?"},{"from":2822.31,"to":2824.41,"location":2,"content":"It's- it's pretty hard to actually put, um,"},{"from":2824.41,"to":2826.84,"location":2,"content":"these, uh, controls in"},{"from":2826.84,"to":2829.39,"location":2,"content":"place to stop it from saying these things you don't want it to say."},{"from":2829.39,"to":2832.3,"location":2,"content":"I mean, on the level of maybe just never saying particular bad words,"},{"from":2832.3,"to":2834.47,"location":2,"content":"then sure, you can remove them from the vocabulary."},{"from":2834.47,"to":2836.66,"location":2,"content":"But overall, they're pretty hard to control,"},{"from":2836.66,"to":2840.3,"location":2,"content":"and we're actually gonna see some examples of NMT systems being,"},{"from":2840.3,"to":2843.28,"location":2,"content":"you know, doing things that their designer certainly didn't intend."},{"from":2843.28,"to":2846.49,"location":2,"content":"[NOISE] Okay."},{"from":2846.49,"to":2849.88,"location":2,"content":"So, uh, how do we evaluate MT?"},{"from":2849.88,"to":2853.75,"location":2,"content":"Uh, every good NLP task needs to have an automatic metric so that we can,"},{"from":2853.75,"to":2855.05,"location":2,"content":"uh, measure our progress."},{"from":2855.05,"to":2859.42,"location":2,"content":"So, the, uh, most commonly used evaluation metric for MT is called BLEU,"},{"from":2859.42,"to":2862.9,"location":2,"content":"and that stands for Bilingual Evaluation Understudy."},{"from":2862.9,"to":2865.75,"location":2,"content":"So, the main idea is that BLEU is gonna"},{"from":2865.75,"to":2870.24,"location":2,"content":"compare the translation that's produced by your machine translation system."},{"from":2870.24,"to":2871.81,"location":2,"content":"It's going to compare that to"},{"from":2871.81,"to":2875.53,"location":2,"content":"one or maybe several human written translations of the same sentence,"},{"from":2875.53,"to":2880.34,"location":2,"content":"and then it's gonna compute a similarity score that's based on n-gram precision."},{"from":2880.34,"to":2882.19,"location":2,"content":"So, when I say n-gram precision,"},{"from":2882.19,"to":2884.41,"location":2,"content":"I mean you're gonna look at all the one, two, three,"},{"from":2884.41,"to":2886.81,"location":2,"content":"and four grams that appear in your, uh,"},{"from":2886.81,"to":2889.99,"location":2,"content":"machine written translation and your human written translation."},{"from":2889.99,"to":2892.51,"location":2,"content":"And then n-gram precision is basically saying,"},{"from":2892.51,"to":2895.55,"location":2,"content":"for all of the n-grams that appeared in the machine written translation,"},{"from":2895.55,"to":2897.58,"location":2,"content":"how many of those appeared in, you know,"},{"from":2897.58,"to":2901.11,"location":2,"content":"at least one of the human written translations?"},{"from":2901.11,"to":2905.83,"location":2,"content":"Another thing that you need to add to BLEU is a brevity penalty."},{"from":2905.83,"to":2908.71,"location":2,"content":"Uh, so, you're saying that you get a lower BLEU score if"},{"from":2908.71,"to":2911.05,"location":2,"content":"your system translation is significantly shorter"},{"from":2911.05,"to":2913.57,"location":2,"content":"than all of the human written translations."},{"from":2913.57,"to":2916.27,"location":2,"content":"And the reason why you need to add this is because"},{"from":2916.27,"to":2920.39,"location":2,"content":"n-gram precision alone doesn't really punish using, uh, fewer words."},{"from":2920.39,"to":2925.3,"location":2,"content":"So, you might try to maximize n-gram precision by being very conservative and writing,"},{"from":2925.3,"to":2928.47,"location":2,"content":"uh, short sentences that only contain words that you're really sure about,"},{"from":2928.47,"to":2930.07,"location":2,"content":"and then you get a good precision score."},{"from":2930.07,"to":2932.89,"location":2,"content":"But this doesn't make a good translation because you're probably missing a bunch of"},{"from":2932.89,"to":2935.78,"location":2,"content":"information that you needed to translate from the source sentence."},{"from":2935.78,"to":2938.97,"location":2,"content":"So, that's why you need to add a brevity, uh, penalty."},{"from":2938.97,"to":2943.45,"location":2,"content":"So, overall, um, BLEU is very useful because,"},{"from":2943.45,"to":2946.84,"location":2,"content":"uh, we need an automatic metric in order to, uh, measure progress."},{"from":2946.84,"to":2948.82,"location":2,"content":"You can't measure progress on human evaluation"},{"from":2948.82,"to":2951.34,"location":2,"content":"alone because it takes too long to compute."},{"from":2951.34,"to":2954.11,"location":2,"content":"Um, but of course, it's pretty imperfect."},{"from":2954.11,"to":2957.86,"location":2,"content":"So, for example, you can think about how there are many ways,"},{"from":2957.86,"to":2959.83,"location":2,"content":"many valid ways to translate a sentence."},{"from":2959.83,"to":2961.36,"location":2,"content":"At the very beginning of this lecture,"},{"from":2961.36,"to":2963.66,"location":2,"content":"I asked how do we translate that sentence, uh,"},{"from":2963.66,"to":2966.43,"location":2,"content":"by Rousseau, and there were at least a few different options that came up."},{"from":2966.43,"to":2970.41,"location":2,"content":"Uh, so, if there's many valid ways to translate a sentence,"},{"from":2970.41,"to":2972.24,"location":2,"content":"how does BLEU recognize that?"},{"from":2972.24,"to":2976.43,"location":2,"content":"BLEU is rewarding sentences that have a high n-gram overlap with,"},{"from":2976.43,"to":2979.95,"location":2,"content":"uh, one or some of the human written translations."},{"from":2979.95,"to":2983.74,"location":2,"content":"But if, uh, you write one- if your model writes one valid translation and"},{"from":2983.74,"to":2987.67,"location":2,"content":"the humans write a different valid translation and they don't have high n-gram overlap,"},{"from":2987.67,"to":2989.18,"location":2,"content":"then BLEU is going to,"},{"from":2989.18,"to":2991.2,"location":2,"content":"uh, give you a low score."},{"from":2991.2,"to":2996.09,"location":2,"content":"So, um, you're going to learn about BLEU in detail in assignment four,"},{"from":2996.09,"to":2998.77,"location":2,"content":"and in fact assignment four has a full description,"},{"from":2998.77,"to":3000.91,"location":2,"content":"mathematical description of what the BLEU score is."},{"from":3000.91,"to":3004.05,"location":2,"content":"So, I'm not gonna tell you about that now. Uh, yeah."},{"from":3004.05,"to":3010.8,"location":2,"content":"So, you're gonna think about BLEU and the ways in which it's imperfect but useful. Yeah."},{"from":3010.8,"to":3015.97,"location":2,"content":"So would one n-gram be a one-to-one equivalency?"},{"from":3015.97,"to":3016.99,"location":2,"content":"Sorry."},{"from":3016.99,"to":3020.09,"location":2,"content":"Wouldn't one n-gram be a one-to-one equivalency?"},{"from":3020.09,"to":3023.91,"location":2,"content":"The question is would a one n-gram be a one-to-one equivalency."},{"from":3023.91,"to":3027.01,"location":2,"content":"I'm not sure I answered the question. Are you asking about alignment or something else?"},{"from":3027.01,"to":3030.96,"location":2,"content":"Uh, just trying to get an idea of how they're doing n-gram checks."},{"from":3030.96,"to":3036.16,"location":2,"content":"Is it doing all n-gram permutations or is it doing like window size of one?"},{"from":3036.16,"to":3038.64,"location":2,"content":"Well, I guess one- one gram it doesn't"},{"from":3038.64,"to":3040.44,"location":2,"content":"make a difference because you can't permute a one gram."},{"from":3040.44,"to":3041.46,"location":2,"content":"Okay. So, you're asking, for example, for"},{"from":3041.46,"to":3043.65,"location":2,"content":"four grams are they checking, uh,"},{"from":3043.65,"to":3047.93,"location":2,"content":"whether this exact sequence of four appears, or any permutation of it, its exact sequences."},{"from":3047.93,"to":3052.32,"location":2,"content":"So, by definition, n-grams are sequences where the order matters."},{"from":3052.32,"to":3056.52,"location":2,"content":"Okay. All right."},{"from":3056.52,"to":3059.05,"location":2,"content":"So, uh, that's how you evaluate machine translation."},{"from":3059.05,"to":3061.11,"location":2,"content":"So, now you can understand this metric of how we"},{"from":3061.11,"to":3063.21,"location":2,"content":"evaluate our progress on machine translation."},{"from":3063.21,"to":3066.68,"location":2,"content":"Um, I can show you this graph and you might understand what it means."},{"from":3066.68,"to":3068.43,"location":2,"content":"So, this is a, uh,"},{"from":3068.43,"to":3074.52,"location":2,"content":"a bar plot which shows in a nutshell how NMT changed the machine translation,"},{"from":3074.52,"to":3076.76,"location":2,"content":"uh, landscape in just a few years."},{"from":3076.76,"to":3080.73,"location":2,"content":"So, in this plot we've got BLEU score is the y-axis."},{"from":3080.73,"to":3083.52,"location":2,"content":"Uh, and you have two different types of SMT,"},{"from":3083.52,"to":3086.2,"location":2,"content":"which is the red and the dark blue, uh, bar plots."},{"from":3086.2,"to":3087.82,"location":2,"content":"And what's happening is,"},{"from":3087.82,"to":3089.18,"location":2,"content":"uh, in 2015, uh,"},{"from":3089.18,"to":3094.29,"location":2,"content":"neural MT enters the scene for the first time and it isn't doi- doing as well as SMT,"},{"from":3094.29,"to":3097.39,"location":2,"content":"and then the next year it's suddenly outperforming SMT."},{"from":3097.39,"to":3100.88,"location":2,"content":"And here, these are BLEU scores on some particular fixed data set,"},{"from":3100.88,"to":3103.26,"location":2,"content":"like a shared task that many people were,"},{"from":3103.26,"to":3104.76,"location":2,"content":"uh, submitting systems for."},{"from":3104.76,"to":3108.03,"location":2,"content":"[NOISE]. So, the main thing to notice here is that"},{"from":3108.03,"to":3111.53,"location":2,"content":"the progress that was being made by SMT systems was, you know,"},{"from":3111.53,"to":3114.15,"location":2,"content":"a fairly gentle increase in BLEU year by year,"},{"from":3114.15,"to":3115.95,"location":2,"content":"and then in just one year,"},{"from":3115.95,"to":3119.7,"location":2,"content":"NMT arrives and is suddenly doing a much more rapid progress."},{"from":3119.7,"to":3123.45,"location":2,"content":"So, I think this justifies why the picture of the meteor maybe isn't too dramatic here."},{"from":3123.45,"to":3131.19,"location":2,"content":"[NOISE] So, you could in fact call NMT the biggest success story of NLP in deep learning."},{"from":3131.19,"to":3133.59,"location":2,"content":"Uh, because if you think about the history of this,"},{"from":3133.59,"to":3138.51,"location":2,"content":"NMT went from being a fringe research activity in 2014 to being actually"},{"from":3138.51,"to":3143.9,"location":2,"content":"the leading standard method for machine translation in the wild in 2016."},{"from":3143.9,"to":3147.53,"location":2,"content":"In particular, in 2014 the first seq2seq paper was published,"},{"from":3147.53,"to":3151.82,"location":2,"content":"and in 2016 Google Translate switches from SMT to NMT."},{"from":3151.82,"to":3155.83,"location":2,"content":"This is a pretty remarkable turn around for just two years."},{"from":3155.83,"to":3159.72,"location":2,"content":"So, this is amazing not just because it was a quick turnaround,"},{"from":3159.72,"to":3162.57,"location":2,"content":"but also if you think about the level of human effort involved."},{"from":3162.57,"to":3164.3,"location":2,"content":"Uh, these SMT systems,"},{"from":3164.3,"to":3166.84,"location":2,"content":"for example the Google Translate SMT system,"},{"from":3166.84,"to":3170.61,"location":2,"content":"was built by doubtless hundreds of engineers over many years."},{"from":3170.61,"to":3176.52,"location":2,"content":"And this, uh, this SMT system was outperformed by an NMT system that was trained by,"},{"from":3176.52,"to":3179.91,"location":2,"content":"uh, you know, relatively few, like a handful of engineers, in a few months."},{"from":3179.91,"to":3182.34,"location":2,"content":"So, I'm not- I'm not diminishing how difficult it is to,"},{"from":3182.34,"to":3183.82,"location":2,"content":"um, build NMT systems,"},{"from":3183.82,"to":3186.33,"location":2,"content":"and certainly I'm sure Google's NMT system"},{"from":3186.33,"to":3189.41,"location":2,"content":"today is built by more than a handful of engineers in a few months."},{"from":3189.41,"to":3191.07,"location":2,"content":"I'm sure it's a very big operation now."},{"from":3191.07,"to":3193.17,"location":2,"content":"Uh, but when NcMT,"},{"from":3193.17,"to":3195.01,"location":2,"content":"uh, began to outperform SMT,"},{"from":3195.01,"to":3197.86,"location":2,"content":"it was pretty remarkable how it was able to do that,"},{"from":3197.86,"to":3201.2,"location":2,"content":"uh, based on the amount of effort involved. Yeah."},{"from":3201.2,"to":3205.88,"location":2,"content":"Given the still the same cons of NMT,"},{"from":3205.88,"to":3210.2,"location":2,"content":"has there been research to combine the two and if there is, what does that look like?"},{"from":3210.2,"to":3214.54,"location":2,"content":"[NOISE] Yeah, great, the question is, given that we noted that there are some disadvantages of NMT,"},{"from":3214.54,"to":3216.53,"location":2,"content":"even in comparison to SMT,"},{"from":3216.53,"to":3218.43,"location":2,"content":"is there any work on combining the two?"},{"from":3218.43,"to":3219.82,"location":2,"content":"So yes, I think there is."},{"from":3219.82,"to":3223.03,"location":2,"content":"Ah, there's a lot of NMT research ongoing, and in particular,"},{"from":3223.03,"to":3226.54,"location":2,"content":"people sometimes focus on these particular shortcomings and, ah,"},{"from":3226.54,"to":3231.04,"location":2,"content":"there's a lot of work in kind of taking techniques and ideas and wisdom from"},{"from":3231.04,"to":3233.59,"location":2,"content":"the many decades of SMT research and then integrating"},{"from":3233.59,"to":3236.23,"location":2,"content":"them into the new NMT paradigm, so yes."},{"from":3236.23,"to":3243.78,"location":2,"content":"[NOISE] Okay."},{"from":3243.78,"to":3247.93,"location":2,"content":"So, is machine translation solved?"},{"from":3247.93,"to":3250.81,"location":2,"content":"Can we all go home? I think the answer is clearly, no."},{"from":3250.81,"to":3254.72,"location":2,"content":"Ah, NMT definitely is not doing machine translation perfectly."},{"from":3254.72,"to":3259.15,"location":2,"content":"So, um, just to highlight some of the difficulties that remain with NMT."},{"from":3259.15,"to":3261.89,"location":2,"content":"One, is out of vocabulary words, um,"},{"from":3261.89,"to":3264.78,"location":2,"content":"this is the kind of basic problem but it- it's- it's pretty tricky."},{"from":3264.78,"to":3267.1,"location":2,"content":"You know, what do you do if you're trying to translate"},{"from":3267.1,"to":3270.26,"location":2,"content":"a sentence that contains a word that is not in your source vocabulary?"},{"from":3270.26,"to":3273.58,"location":2,"content":"Or, what if you're trying to produce a word that's not in your target vocabulary?"},{"from":3273.58,"to":3276.3,"location":2,"content":"Um, there's certainly been lots of work on doing this,"},{"from":3276.3,"to":3280.15,"location":2,"content":"and you're going to hear later in the class how you might try to attack this with,"},{"from":3280.15,"to":3282.88,"location":2,"content":"for example, ah, sub-word modeling can make it easier."},{"from":3282.88,"to":3286.09,"location":2,"content":"Ah, but this is a rema- this is, ah, a significant problem."},{"from":3286.09,"to":3288.43,"location":2,"content":"Another one is domain mismatch."},{"from":3288.43,"to":3290.47,"location":2,"content":"So, let's suppose that you train your, uh,"},{"from":3290.47,"to":3292.81,"location":2,"content":"machine translation system on a bunch of fairly,"},{"from":3292.81,"to":3294.63,"location":2,"content":"ah, formal texts, like, let's say, ah,"},{"from":3294.63,"to":3296.47,"location":2,"content":"Wikipedia, or something like that,"},{"from":3296.47,"to":3300.04,"location":2,"content":"but then you try to deploy it to translate informal texts,"},{"from":3300.04,"to":3302.24,"location":2,"content":"like people chatting on Twitter or something."},{"from":3302.24,"to":3305.95,"location":2,"content":"Then often, you'll find that it doesn't perform very well on this different domain,"},{"from":3305.95,"to":3307.15,"location":2,"content":"because you've got a domain mismatch,"},{"from":3307.15,"to":3309.7,"location":2,"content":"ah, so that's quite a big problem."},{"from":3309.7,"to":3313.14,"location":2,"content":"Another one is maintaining context over longer text."},{"from":3313.14,"to":3315.79,"location":2,"content":"So, everything we've talked about so far has assumed that you were"},{"from":3315.79,"to":3318.45,"location":2,"content":"just translating a single sentence to a single sentence,"},{"from":3318.45,"to":3321.05,"location":2,"content":"and there's no other [NOISE] wider context."},{"from":3321.05,"to":3322.3,"location":2,"content":"Ah, but, you know,"},{"from":3322.3,"to":3324.73,"location":2,"content":"if you want to use a machine translation system to"},{"from":3324.73,"to":3327.79,"location":2,"content":"translate a whole news article or maybe even a book, then,"},{"from":3327.79,"to":3330.91,"location":2,"content":"ah, you're probably going to want to use the context that came in"},{"from":3330.91,"to":3336.11,"location":2,"content":"previous sentences in order to translate things correctly in the current sentence."},{"from":3336.11,"to":3338.71,"location":2,"content":"So, ah, this is an active area of research,"},{"from":3338.71,"to":3341.05,"location":2,"content":"how can you get an NMT system to condition on"},{"from":3341.05,"to":3345.84,"location":2,"content":"larger pieces of context without it becoming too expensive and so on."},{"from":3345.84,"to":3349.03,"location":2,"content":"Another difficulty is low resource language pairs."},{"from":3349.03,"to":3352.15,"location":2,"content":"Um, everything we've talked about so far has assumed that you"},{"from":3352.15,"to":3355.55,"location":2,"content":"have access to a very large parallel corpus, but what if you don't."},{"from":3355.55,"to":3357.73,"location":2,"content":"What if you're trying to translate to or from a language"},{"from":3357.73,"to":3360.07,"location":2,"content":"that has relatively little text available,"},{"from":3360.07,"to":3362.03,"location":2,"content":"um, online, for example."},{"from":3362.03,"to":3364.28,"location":2,"content":"So, that can be pretty difficult."},{"from":3364.28,"to":3367.84,"location":2,"content":"Here are a few examples of machine translation screwing up,"},{"from":3367.84,"to":3370.2,"location":2,"content":"ah, with, with specif- specific errors."},{"from":3370.2,"to":3375.85,"location":2,"content":"So, here's an example of how common sense is really difficult for NMT systems."},{"from":3375.85,"to":3378.28,"location":2,"content":"On the left, we have the English phrase, paper jam,"},{"from":3378.28,"to":3381.13,"location":2,"content":"which means when your printer gets jammed up with paper,"},{"from":3381.13,"to":3383.49,"location":2,"content":"and it's all, ah, tangled inside."},{"from":3383.49,"to":3384.91,"location":2,"content":"And then on the right,"},{"from":3384.91,"to":3387.61,"location":2,"content":"we have a very literal translation of that into Spanish,"},{"from":3387.61,"to":3389.05,"location":2,"content":"and it's essentially saying jam,"},{"from":3389.05,"to":3391.06,"location":2,"content":"edible jam made of paper,"},{"from":3391.06,"to":3393.86,"location":2,"content":"which clearly isn't the right interpretation."},{"from":3393.86,"to":3396.7,"location":2,"content":"So here, we have an NMT system that's just doing"},{"from":3396.7,"to":3399.78,"location":2,"content":"very literal translation and clearly doesn't have any notion of common sense."},{"from":3399.78,"to":3401.29,"location":2,"content":"You can't make jams in paper."},{"from":3401.29,"to":3404.18,"location":2,"content":"Ah, here's another example."},{"from":3404.18,"to":3406.99,"location":2,"content":"NMT can pick up biases in the training data."},{"from":3406.99,"to":3409,"location":2,"content":"We already talked about this at the,"},{"from":3409,"to":3410.98,"location":2,"content":"ah, the word embedding level,"},{"from":3410.98,"to":3412.54,"location":2,"content":"the representation of words, ah,"},{"from":3412.54,"to":3414.72,"location":2,"content":"but it can also be a problem at the,"},{"from":3414.72,"to":3416.82,"location":2,"content":"you know, the sentence level when you're translating things."},{"from":3416.82,"to":3418.81,"location":2,"content":"So, here in this example,"},{"from":3418.81,"to":3419.98,"location":2,"content":"ah, on the left,"},{"from":3419.98,"to":3423.16,"location":2,"content":"we have two sentences in Malay that roughly mean,"},{"from":3423.16,"to":3425.17,"location":2,"content":"ah, they work as a nurse,"},{"from":3425.17,"to":3426.74,"location":2,"content":"and they work as a programmer."},{"from":3426.74,"to":3428.2,"location":2,"content":"The point is, on the left there,"},{"from":3428.2,"to":3430.78,"location":2,"content":"is no information about gender in the pronouns,"},{"from":3430.78,"to":3431.93,"location":2,"content":"but when it gets, ah,"},{"from":3431.93,"to":3435.79,"location":2,"content":"translated to English, then we've suddenly got gender coming out of nowhere,"},{"from":3435.79,"to":3438.78,"location":2,"content":"she works as a nurse and he works as a programmer."},{"from":3438.78,"to":3441.1,"location":2,"content":"This is likely happening because in our training data,"},{"from":3441.1,"to":3444.74,"location":2,"content":"we had more examples of female nurses and male programmers."},{"from":3444.74,"to":3446.65,"location":2,"content":"So, you can understand why from,"},{"from":3446.65,"to":3448.03,"location":2,"content":"ah, a machine learning ah,"},{"from":3448.03,"to":3450.14,"location":2,"content":"maximizing the objective point of view,"},{"from":3450.14,"to":3452.65,"location":2,"content":"the, ah, English language model has learned to do that."},{"from":3452.65,"to":3455.17,"location":2,"content":"But the problem here is, this isn't good machine translation."},{"from":3455.17,"to":3458.02,"location":2,"content":"Ah, here, the system is making up"},{"from":3458.02,"to":3461.24,"location":2,"content":"information that was not present in the source sentence."},{"from":3461.24,"to":3463.42,"location":2,"content":"So, this is certainly an error that"},{"from":3463.42,"to":3466.45,"location":2,"content":"the machine translation shouldn't be doing because it's just simply inaccurate."},{"from":3466.45,"to":3470.24,"location":2,"content":"And even worse, it's propagating, ah, gender roles."},{"from":3470.24,"to":3473.49,"location":2,"content":"Here's another pretty weird example."},{"from":3473.49,"to":3482.31,"location":2,"content":"[LAUGHTER] What is happening here?"},{"from":3482.31,"to":3483.33,"location":2,"content":"Ah, on the left,"},{"from":3483.33,"to":3485.74,"location":2,"content":"[NOISE] we have a nonsense sentence,"},{"from":3485.74,"to":3488.51,"location":2,"content":"this is just kind of a syllable repeated,"},{"from":3488.51,"to":3490.9,"location":2,"content":"and we're supposedly tra- translating from Somali."},{"from":3490.9,"to":3493.74,"location":2,"content":"Ah, and then we're asking to translate this into English,"},{"from":3493.74,"to":3495.67,"location":2,"content":"and then we're getting this out of nowhere."},{"from":3495.67,"to":3499.34,"location":2,"content":"Um, as the name of the LORD was written in the Hebrew language,"},{"from":3499.34,"to":3501.36,"location":2,"content":"it was written in the language of the Hebrew nation."},{"from":3501.36,"to":3504.39,"location":2,"content":"And you might be thinking, \"Where on earth did that come from?\""},{"from":3504.39,"to":3506.99,"location":2,"content":"And, in fact, this got reported in the media as,"},{"from":3506.99,"to":3510.13,"location":2,"content":"you know, Google Translate wants to convert you to its religion or whatever."},{"from":3510.13,"to":3512.44,"location":2,"content":"[LAUGHTER]."},{"from":3512.44,"to":3513.61,"location":2,"content":"Um, so for sure,"},{"from":3513.61,"to":3514.86,"location":2,"content":"it is very startling,"},{"from":3514.86,"to":3518.53,"location":2,"content":"but the thing is there's actually quite a reasonable explanation."},{"from":3518.53,"to":3521.66,"location":2,"content":"So, what's going on here is that,"},{"from":3521.66,"to":3525.28,"location":2,"content":"um, often for low-resource languages such as,"},{"from":3525.28,"to":3532.07,"location":2,"content":"for example, Somali, one of the best resources of parallel text is the Bible."},{"from":3532.07,"to":3533.79,"location":2,"content":"So you train, for example,"},{"from":3533.79,"to":3538.26,"location":2,"content":"Somali's English using the Bible as a training text,  maybe among other texts."},{"from":3538.26,"to":3540.19,"location":2,"content":"Okay, so that's the first puzzle piece,"},{"from":3540.19,"to":3543.1,"location":2,"content":"but the other- the other puzzle piece is the nonsensical input."},{"from":3543.1,"to":3547.69,"location":2,"content":"So, when the input isn't really Somali or any kind of text, right?"},{"from":3547.69,"to":3549.55,"location":2,"content":"It's just the same syllable over and over,"},{"from":3549.55,"to":3553.26,"location":2,"content":"then the NMT system doesn't really have anything sensible to condition on."},{"from":3553.26,"to":3555.25,"location":2,"content":"It's basically nonsense. It's just noise."},{"from":3555.25,"to":3557.22,"location":2,"content":"So what does the NMT system do?"},{"from":3557.22,"to":3558.67,"location":2,"content":"Right? It can't really use,"},{"from":3558.67,"to":3560.74,"location":2,"content":"it can't really condition on the, ah, source sentence."},{"from":3560.74,"to":3564.11,"location":2,"content":"So what it does, is it just uses the English language model, right?"},{"from":3564.11,"to":3567.1,"location":2,"content":"You can think of it as like the English language model of the decoder RNN,"},{"from":3567.1,"to":3570.49,"location":2,"content":"just kind of goes into autopilot and starts generating random text."},{"from":3570.49,"to":3573.05,"location":2,"content":"Kind of like we saw, ah, last week when we saw, ah,"},{"from":3573.05,"to":3575.02,"location":2,"content":"a language model trained on Obama speeches or"},{"from":3575.02,"to":3577.36,"location":2,"content":"Harry Potter would just generate text in that style."},{"from":3577.36,"to":3579.43,"location":2,"content":"That's kind of what's happening here with the Bible,"},{"from":3579.43,"to":3581.47,"location":2,"content":"because we don't have any useful information,"},{"from":3581.47,"to":3583.99,"location":2,"content":"um, from the sentence on the left."},{"from":3583.99,"to":3588.2,"location":2,"content":"Um, so this is an example why, ah,"},{"from":3588.2,"to":3591.39,"location":2,"content":"neural machine translation in particular makes these kinds of errors ah,"},{"from":3591.39,"to":3593.47,"location":2,"content":"because the system is uninterpretable."},{"from":3593.47,"to":3596.32,"location":2,"content":"So, you don't know that this is going to happen until it happens,"},{"from":3596.32,"to":3598.09,"location":2,"content":"and perhaps Google didn't know this was gonna"},{"from":3598.09,"to":3600.31,"location":2,"content":"happen until it happened, and it got reported."},{"from":3600.31,"to":3603.43,"location":2,"content":"Um, so this is one downside of uninterpretability,"},{"from":3603.43,"to":3605.62,"location":2,"content":"is that really weird effects can happen,"},{"from":3605.62,"to":3606.86,"location":2,"content":"and you don't see them coming,"},{"from":3606.86,"to":3609.79,"location":2,"content":"and it's not always even easy to explain why they happened. Yeah? [NOISE]."},{"from":3609.79,"to":3615.82,"location":2,"content":"Looks like it says Irish, [inaudible] can translate that back to Irish?"},{"from":3615.82,"to":3620.03,"location":2,"content":"Ah, the question is what happens if you did translate from Irish?"},{"from":3620.03,"to":3623.11,"location":2,"content":"I suppose that's the part where Google tries to  auto-detect the language,"},{"from":3623.11,"to":3626.17,"location":2,"content":"maybe it thinks that ag, ag, ag is more like Irish than Somali [LAUGHTER]."},{"from":3626.17,"to":3629.17,"location":2,"content":"Um, I imagine if you did put Irish to English,"},{"from":3629.17,"to":3631.3,"location":2,"content":"there's probably more, ah,"},{"from":3631.3,"to":3632.8,"location":2,"content":"training data for Irish English,"},{"from":3632.8,"to":3635.38,"location":2,"content":"so maybe it wouldn't be so Bible focused."},{"from":3635.38,"to":3638.32,"location":2,"content":"Um, yeah, and there's a lot of examples of these online where"},{"from":3638.32,"to":3642.57,"location":2,"content":"you do different kinds of nonsense syllables in different languages."},{"from":3642.57,"to":3644.66,"location":2,"content":"So, there's a lot of, ah,"},{"from":3644.66,"to":3647.41,"location":2,"content":"challenges remaining in NMT and,"},{"from":3647.41,"to":3649.01,"location":2,"content":"ah, the research continues."},{"from":3649.01,"to":3653.95,"location":2,"content":"So, NMT, I think remains one of the flagship tasks for NLP Deep Learning."},{"from":3653.95,"to":3657.07,"location":2,"content":"And, in fact, NMT research has pioneered many of"},{"from":3657.07,"to":3659.8,"location":2,"content":"the successful innovations of NLP Deep Learning in general."},{"from":3659.8,"to":3662.53,"location":2,"content":"Ah, so today in 2019,"},{"from":3662.53,"to":3664.78,"location":2,"content":"ah, NMT research continues to thrive."},{"from":3664.78,"to":3666.55,"location":2,"content":"There are still many, many papers, ah,"},{"from":3666.55,"to":3669.1,"location":2,"content":"published all the time on NMT, and in fact,"},{"from":3669.1,"to":3671.56,"location":2,"content":"researchers have found lots of improvements to"},{"from":3671.56,"to":3674.35,"location":2,"content":"the fairly \"vanilla\" seq2seq models that I've shown you today."},{"from":3674.35,"to":3675.93,"location":2,"content":"Ah, but in fact,"},{"from":3675.93,"to":3677.8,"location":2,"content":"there's one improvement that is so"},{"from":3677.8,"to":3681.3,"location":2,"content":"integral to seq2seq that you could regard it as the new \"vanilla\"."},{"from":3681.3,"to":3683.43,"location":2,"content":"And that's the improvement we're going to learn about today,"},{"from":3683.43,"to":3685.7,"location":2,"content":"and it's called attention."},{"from":3685.7,"to":3690.89,"location":2,"content":"Okay. So, section three is on attention. What is attention?"},{"from":3690.89,"to":3694.54,"location":2,"content":"First, I'm going to motivate why we need this thing called attention."},{"from":3694.54,"to":3698.01,"location":2,"content":"So, let's look at this diagram that we saw before of sequence-to-sequence."},{"from":3698.01,"to":3700.36,"location":2,"content":"And remember, when we assumed that this, ah,"},{"from":3700.36,"to":3702.79,"location":2,"content":"encoding of the source sentence th- th- the one in"},{"from":3702.79,"to":3705.76,"location":2,"content":"the orange box is going to represent the whole sentence."},{"from":3705.76,"to":3711.87,"location":2,"content":"Ah, can anyone volunteer a problem you can see with this architecture?"},{"from":3711.87,"to":3715.24,"location":2,"content":"In particular, perhaps a problem with this idea that"},{"from":3715.24,"to":3717.7,"location":2,"content":"that single vector is the encoding of the source sentence."},{"from":3717.7,"to":3720.91,"location":2,"content":"[NOISE] Yep."},{"from":3720.91,"to":3723.1,"location":2,"content":"[inaudible] [NOISE]."},{"from":3723.1,"to":3725.14,"location":2,"content":"It doesn't- you are only looking at one word,"},{"from":3725.14,"to":3727.06,"location":2,"content":"and that word could have many meanings potentially."},{"from":3727.06,"to":3728.62,"location":2,"content":"You don't know what's going on,"},{"from":3728.62,"to":3729.67,"location":2,"content":"so you can't figure out which meaning."},{"from":3729.67,"to":3733.21,"location":2,"content":"Okay, so the answer is something like,"},{"from":3733.21,"to":3734.74,"location":2,"content":"um, you're only looking at one word."},{"from":3734.74,"to":3736.75,"location":2,"content":"You mean like the last word of the source sentence,"},{"from":3736.75,"to":3738.63,"location":2,"content":"and you're not seeing more information."},{"from":3738.63,"to":3740.35,"location":2,"content":"Yeah, som- it's, it's something like that."},{"from":3740.35,"to":3741.79,"location":2,"content":"Any other ideas? Yep."},{"from":3741.79,"to":3745.99,"location":2,"content":"You might have lost information from the beginning of the sentence by the time you get to the end."},{"from":3745.99,"to":3748.76,"location":2,"content":"Yeah. You might have lost information from the sentence"},{"from":3748.76,"to":3752.34,"location":2,"content":"by the time you get to the end, especially if it was longer than four words."},{"from":3752.34,"to":3754.38,"location":2,"content":"Right. I think these are different ways of saying, uh,"},{"from":3754.38,"to":3756.75,"location":2,"content":"a similar idea [NOISE],"},{"from":3756.75,"to":3759.39,"location":2,"content":"which is that we have a kind of informational bottleneck."},{"from":3759.39,"to":3763.26,"location":2,"content":"Uh, we're forcing all of the information about the source sentence to be captured"},{"from":3763.26,"to":3767.3,"location":2,"content":"in this single vector because that's the only thing that gets given to the decoder."},{"from":3767.3,"to":3769.93,"location":2,"content":"If some information about source sentence isn't in that vector,"},{"from":3769.93,"to":3773.29,"location":2,"content":"then there's no way the decoder is going to be able to translate it correctly."},{"from":3773.29,"to":3774.58,"location":2,"content":"So, this is, yeah."},{"from":3774.58,"to":3776.3,"location":2,"content":"This is an informational bottleneck."},{"from":3776.3,"to":3778.33,"location":2,"content":"It's putting, kind of, too much pressure on"},{"from":3778.33,"to":3781.84,"location":2,"content":"this single vector to be a good representation of the encoder."},{"from":3781.84,"to":3784.95,"location":2,"content":"So, this is the motivation for attention."},{"from":3784.95,"to":3789.09,"location":2,"content":"Attention is a neural technique and it provides a solution to the bottleneck problem."},{"from":3789.09,"to":3792.18,"location":2,"content":"The core idea is that on each step of the decoder,"},{"from":3792.18,"to":3794.17,"location":2,"content":"you're going to use a direct connection to"},{"from":3794.17,"to":3799.74,"location":2,"content":"the encoder to focus on a particular part of the source sequence."},{"from":3799.74,"to":3804.1,"location":2,"content":"So first, I'm going to show you what attention is via a diagram,"},{"from":3804.1,"to":3805.8,"location":2,"content":"so that's kind of an intuitive explanation,"},{"from":3805.8,"to":3808.05,"location":2,"content":"and then I'm going to show you the equations later."},{"from":3808.05,"to":3812.2,"location":2,"content":"So, here's how se- seq- sequence-to-sequence with attention works."},{"from":3812.2,"to":3814.6,"location":2,"content":"So, on the first step of our decoder,"},{"from":3814.6,"to":3817.74,"location":2,"content":"uh, we have our first decoder hidden state."},{"from":3817.74,"to":3820.98,"location":2,"content":"So, what we do is we take the dot product between"},{"from":3820.98,"to":3824.24,"location":2,"content":"that decoder hidden state and the first encoder hidden state,"},{"from":3824.24,"to":3826.86,"location":2,"content":"and then we get something called an attention score,"},{"from":3826.86,"to":3829.78,"location":2,"content":"which I'm representing by a dot, so that's a scalar."},{"from":3829.78,"to":3832.42,"location":2,"content":"And in fact, we take the dot product between the decoder"},{"from":3832.42,"to":3835.75,"location":2,"content":"hidden state and all of the encoder hidden states."},{"from":3835.75,"to":3838.06,"location":2,"content":"So, this means that we get one attention score,"},{"from":3838.06,"to":3840.2,"location":2,"content":"one scalar for each of these,"},{"from":3840.2,"to":3842.65,"location":2,"content":"uh, source words effectively."},{"from":3842.65,"to":3848.51,"location":2,"content":"So, next what we do is we take those four numbers scores and we apply the softmax,"},{"from":3848.51,"to":3851.22,"location":2,"content":"uh, distribution, the softmax function to them,"},{"from":3851.22,"to":3853.82,"location":2,"content":"and then we get a probability distribution."},{"from":3853.82,"to":3858.36,"location":2,"content":"So here, I'm going to represent that probability distribution as a bar chart,"},{"from":3858.36,"to":3860.93,"location":2,"content":"um, and we call this the attention distribution,"},{"from":3860.93,"to":3862.86,"location":2,"content":"and this one sums up to one."},{"from":3862.86,"to":3867.46,"location":2,"content":"So here, you can see that most of the probability mass is on the first word,"},{"from":3867.46,"to":3871,"location":2,"content":"and that kind of makes sense because our first words essentially means \"he\" and"},{"from":3871,"to":3875.05,"location":2,"content":"we're going to be producing the word \"he\" first in our target sentence."},{"from":3875.05,"to":3877.63,"location":2,"content":"So, once we've got this attention distribution,"},{"from":3877.63,"to":3883.95,"location":2,"content":"uh, we're going to use it to produce something called the attention output."},{"from":3883.95,"to":3886.78,"location":2,"content":"So, the idea here is that the attention output is"},{"from":3886.78,"to":3889.7,"location":2,"content":"a weighted sum of the encoder hidden states,"},{"from":3889.7,"to":3893.18,"location":2,"content":"and the waiting is the attention distribution."},{"from":3893.18,"to":3895.09,"location":2,"content":"So, I've got these dotted arrows that go from"},{"from":3895.09,"to":3897.05,"location":2,"content":"the attention distribution to the attention output."},{"from":3897.05,"to":3899.89,"location":2,"content":"Probably there should be dotted arrows also from the encoder RNN,"},{"from":3899.89,"to":3900.91,"location":2,"content":"but that's hard to depict."},{"from":3900.91,"to":3905.05,"location":2,"content":"[NOISE] But the idea is that you're summing up these encoder RNN, uh,"},{"from":3905.05,"to":3907.15,"location":2,"content":"hidden states, [NOISE] but you're going to weight each"},{"from":3907.15,"to":3910.2,"location":2,"content":"one according to how much attention distribution it has on it."},{"from":3910.2,"to":3914.29,"location":2,"content":"So, this means that your attention output which is a single vector is going to be"},{"from":3914.29,"to":3918.03,"location":2,"content":"mostly containing information from the hidden states that had high attention."},{"from":3918.03,"to":3924.87,"location":2,"content":"In this case, it's going to be mostly information from the first hidden state."},{"from":3924.87,"to":3927.05,"location":2,"content":"So, after you do this,"},{"from":3927.05,"to":3931.06,"location":2,"content":"you're going to use the attention output to influence your prediction of the next word."},{"from":3931.06,"to":3933.19,"location":2,"content":"So, what you usually do is you concatenate"},{"from":3933.19,"to":3935.49,"location":2,"content":"the attention output with your decoder hidden states,"},{"from":3935.49,"to":3938.26,"location":2,"content":"and then, uh, use that kind of concatenated pair"},{"from":3938.26,"to":3941.59,"location":2,"content":"in the way you would have used the decoder hidden state alone before."},{"from":3941.59,"to":3944.39,"location":2,"content":"So, that way you can get your probability distribution"},{"from":3944.39,"to":3946.86,"location":2,"content":"y hat one of what's coming next."},{"from":3946.86,"to":3949.84,"location":2,"content":"So, as before, we can use that to sample your next word."},{"from":3949.84,"to":3952.03,"location":2,"content":"[NOISE] So, on the next step,"},{"from":3952.03,"to":3953.39,"location":2,"content":"you just do the same thing again."},{"from":3953.39,"to":3955.75,"location":2,"content":"You've got your second decoder hidden state, again,"},{"from":3955.75,"to":3958.14,"location":2,"content":"you take dot product with all of the encoder hidden states,"},{"from":3958.14,"to":3960.89,"location":2,"content":"you take softmax layer without getting attention distribution,"},{"from":3960.89,"to":3963.34,"location":2,"content":"and here you can see the attention distribution is different."},{"from":3963.34,"to":3967.33,"location":2,"content":"We're putting more attention on, uh, the word entarte"},{"from":3967.33,"to":3969.19,"location":2,"content":"because we're about to produce the word hits."},{"from":3969.19,"to":3971.88,"location":2,"content":"Uh, but we're also attending a little bit to the second one,"},{"from":3971.88,"to":3974.95,"location":2,"content":"ah, because that's telling us that hit is past tense."},{"from":3974.95,"to":3979.15,"location":2,"content":"So, a cool thing that's happening here is we're getting a soft alignment."},{"from":3979.15,"to":3983.08,"location":2,"content":"If you remember when we looked at alignment in SMT systems, it was mostly this,"},{"from":3983.08,"to":3984.58,"location":2,"content":"uh, hard binary thing,"},{"from":3984.58,"to":3987.25,"location":2,"content":"it was on or off, either these words are aligned or they're not."},{"from":3987.25,"to":3991.14,"location":2,"content":"Here, you have a much more flexible soft notion of alignment,"},{"from":3991.14,"to":3992.77,"location":2,"content":"where, uh, each word, kind of,"},{"from":3992.77,"to":3996.79,"location":2,"content":"has a distribution over the corresponding words in the source sentence."},{"from":3996.79,"to":3998.5,"location":2,"content":"So, another thing to note,"},{"from":3998.5,"to":4001.08,"location":2,"content":"kind of a side note, is that sometimes, uh,"},{"from":4001.08,"to":4004.56,"location":2,"content":"we take the attention output from the previous hidden state, uh,"},{"from":4004.56,"to":4009.16,"location":2,"content":"and we kind of feed it into the decoder again along with the usual word."},{"from":4009.16,"to":4011.13,"location":2,"content":"So, that would mean you take the attention output from"},{"from":4011.13,"to":4014.22,"location":2,"content":"the first step and kind of concatenate it to the word vector for \"he\","},{"from":4014.22,"to":4016.08,"location":2,"content":"and then use it in the decoder."},{"from":4016.08,"to":4019.17,"location":2,"content":"The reason for this is sometimes is useful to have this, uh,"},{"from":4019.17,"to":4023.41,"location":2,"content":"information from the- the attention on the previous step, on the next step."},{"from":4023.41,"to":4025.47,"location":2,"content":"So, I'm telling you this because this is something we do in"},{"from":4025.47,"to":4027.39,"location":2,"content":"assignment four and it's a fairly common technique,"},{"from":4027.39,"to":4029.8,"location":2,"content":"but also sometimes people don't do it."},{"from":4029.8,"to":4034.83,"location":2,"content":"Okay. So, um, the idea is that you just do this attention,"},{"from":4034.83,"to":4036.91,"location":2,"content":"uh, computation on every step,"},{"from":4036.91,"to":4039.48,"location":2,"content":"and on each step you're going to be tending to different things."},{"from":4039.48,"to":4040.62,"location":2,"content":"So, in our example,"},{"from":4040.62,"to":4043.11,"location":2,"content":"on this third step we look at an apostrophe,"},{"from":4043.11,"to":4045.18,"location":2,"content":"which means me, when we produce me,"},{"from":4045.18,"to":4047.55,"location":2,"content":"and then on the last three were probably mostly just going to be looking at"},{"from":4047.55,"to":4050.12,"location":2,"content":"this fertile word entarte,"},{"from":4050.12,"to":4051.21,"location":2,"content":"to produce hit me with a pie."},{"from":4051.21,"to":4054.24,"location":2,"content":"[NOISE] I'm going to keep going because we don't have a lot of time."},{"from":4054.24,"to":4057.48,"location":2,"content":"Uh, so, here are the equations to describe attention."},{"from":4057.48,"to":4060.06,"location":2,"content":"I think it's probably easier to look at these in your own time later,"},{"from":4060.06,"to":4061.7,"location":2,"content":"rather than look at them in the lecture now,"},{"from":4061.7,"to":4063.51,"location":2,"content":"but these are the equations that essentially"},{"from":4063.51,"to":4065.98,"location":2,"content":"say the same thing as what the diagram just said."},{"from":4065.98,"to":4069.76,"location":2,"content":"So, you have your encoder hidden states h_1 up to h_N,"},{"from":4069.76,"to":4072.45,"location":2,"content":"and then on timestep t of the decoder,"},{"from":4072.45,"to":4075.51,"location":2,"content":"we also have a decoder hidden state, s_t."},{"from":4075.51,"to":4078.95,"location":2,"content":"So, you're going to get the attention scores, which we are going to call e_t,"},{"from":4078.95,"to":4080.85,"location":2,"content":"by taking the dot product of"},{"from":4080.85,"to":4083.37,"location":2,"content":"your decoder hidden state with each of the encoder hidden states."},{"from":4083.37,"to":4085.14,"location":2,"content":"[NOISE] And that gives you, uh,"},{"from":4085.14,"to":4086.94,"location":2,"content":"a vector of same length as"},{"from":4086.94,"to":4092.03,"location":2,"content":"the encoder sentence because you've got one score per source word."},{"from":4092.03,"to":4095.34,"location":2,"content":"Next, you take softmax over these scores to"},{"from":4095.34,"to":4097.94,"location":2,"content":"get attention distribution that sums up to one,"},{"from":4097.94,"to":4100.27,"location":2,"content":"and we call that Alpha."},{"from":4100.27,"to":4105.05,"location":2,"content":"And then you use Alpha to take a weighted sum of the encoder hidden states,"},{"from":4105.05,"to":4107.19,"location":2,"content":"and that gives you your attention outputs."},{"from":4107.19,"to":4110.13,"location":2,"content":"So, the attention output which we call a is a vector that's"},{"from":4110.13,"to":4114.3,"location":2,"content":"the same size as your encoder hidden states."},{"from":4114.3,"to":4118.08,"location":2,"content":"Lastly, you take your attention output a,"},{"from":4118.08,"to":4119.43,"location":2,"content":"and then you, uh,"},{"from":4119.43,"to":4121.59,"location":2,"content":"concatenate it with your decoder hidden state,"},{"from":4121.59,"to":4127.22,"location":2,"content":"and then proceed with that a s you were taught before in the no-attention model."},{"from":4127.22,"to":4130.41,"location":2,"content":"So, attention, if it's not clear, it's pretty cool."},{"from":4130.41,"to":4132.16,"location":2,"content":"It has a number of advantages."},{"from":4132.16,"to":4136.59,"location":2,"content":"So, one advantage is that attention just significantly improves NMT performance."},{"from":4136.59,"to":4140.49,"location":2,"content":"And the main reason why it improves it is because it turns out it's super useful to allow"},{"from":4140.49,"to":4144.87,"location":2,"content":"the decoder to focus on certain parts of the source sentence when it's translating."},{"from":4144.87,"to":4146.61,"location":2,"content":"And you can see why this makes sense, right?"},{"from":4146.61,"to":4148.41,"location":2,"content":"Because there's a very natural notion of alignment,"},{"from":4148.41,"to":4151.29,"location":2,"content":"and if you can focus on the specific word or words that you're translating,"},{"from":4151.29,"to":4153.23,"location":2,"content":"you can probably do a better job."},{"from":4153.23,"to":4157.08,"location":2,"content":"Another reason why attention is cool is that it solves the bottleneck problem."},{"from":4157.08,"to":4160.89,"location":2,"content":"Uh, we were noting that the problem with having a single vector that has to represent"},{"from":4160.89,"to":4163.11,"location":2,"content":"the entire source sentence and that's the only way"},{"from":4163.11,"to":4165.45,"location":2,"content":"information can pass from encoder to decoder,"},{"from":4165.45,"to":4167.72,"location":2,"content":"means that if that encoding isn't very good,"},{"from":4167.72,"to":4169.24,"location":2,"content":"then you're not going to do well."},{"from":4169.24,"to":4171.81,"location":2,"content":"So, by contrast in- with attention,"},{"from":4171.81,"to":4174.33,"location":2,"content":"the decoder can look directly at the encoder and"},{"from":4174.33,"to":4179.21,"location":2,"content":"the source sentence and translate without the bottleneck."},{"from":4179.21,"to":4183.59,"location":2,"content":"Another great thing about attention is that it helps with the vanishing gradient problem,"},{"from":4183.59,"to":4186.14,"location":2,"content":"especially if your sentences are quite long."},{"from":4186.14,"to":4188.7,"location":2,"content":"The reason why attention helps is because you have"},{"from":4188.7,"to":4192.05,"location":2,"content":"these direct connections between the decoder and the encoder,"},{"from":4192.05,"to":4193.81,"location":2,"content":"kind of, over many time steps,"},{"from":4193.81,"to":4195.47,"location":2,"content":"so it's like a shortcut connection."},{"from":4195.47,"to":4197.25,"location":2,"content":"And just as we learned last time about, ah,"},{"from":4197.25,"to":4200.9,"location":2,"content":"skip connections being [NOISE] useful for reducing vanishing gradients,"},{"from":4200.9,"to":4202.23,"location":2,"content":"here, it's the same notion."},{"from":4202.23,"to":4203.94,"location":2,"content":"We have these, ah, long distance"},{"from":4203.94,"to":4207.26,"location":2,"content":"[NOISE] direct connections that help the gradients flow better."},{"from":4207.26,"to":4210.87,"location":2,"content":"Another great thing about attention is it provides some interpretability."},{"from":4210.87,"to":4215.52,"location":2,"content":"Ah, if you look at the attention distribution after you've produced your translation,"},{"from":4215.52,"to":4219.03,"location":2,"content":"ah, you can see what the decoder was focusing on on each step."},{"from":4219.03,"to":4220.65,"location":2,"content":"So for example, if we run our system,"},{"from":4220.65,"to":4222.78,"location":2,"content":"and we translate our running example here,"},{"from":4222.78,"to":4225.15,"location":2,"content":"then we can produce a plot kind of like this,"},{"from":4225.15,"to":4227.13,"location":2,"content":"that shows the attention distribution."},{"from":4227.13,"to":4229.32,"location":2,"content":"So here, dark means high attention,"},{"from":4229.32,"to":4230.9,"location":2,"content":"and white means low attention."},{"from":4230.9,"to":4232.95,"location":2,"content":"So you might see something like this where,"},{"from":4232.95,"to":4236.2,"location":2,"content":"um, it was, it was focusing on the different words and different steps."},{"from":4236.2,"to":4238.8,"location":2,"content":"And this is basically the same kind of"},{"from":4238.8,"to":4241.38,"location":2,"content":"plot that we had earlier with the hard notion of alignment,"},{"from":4241.38,"to":4243.9,"location":2,"content":"ah, in SMT, except that we are,"},{"from":4243.9,"to":4247.03,"location":2,"content":"we have more flexibility to have a more soft version of alignment."},{"from":4247.03,"to":4250.68,"location":2,"content":"Like for example, ah, when we produce the English would hit,"},{"from":4250.68,"to":4252.54,"location":2,"content":"perhaps we were mostly looking at entarte,"},{"from":4252.54,"to":4255.55,"location":2,"content":"but we're also looking a little bit at a."},{"from":4255.55,"to":4258.65,"location":2,"content":"So, this, ah, means that we're getting,"},{"from":4258.65,"to":4259.98,"location":2,"content":"ah, alignment for free."},{"from":4259.98,"to":4261.64,"location":2,"content":"And the reason I say for free,"},{"from":4261.64,"to":4264.06,"location":2,"content":"is because when you remember the SMT systems,"},{"from":4264.06,"to":4265.86,"location":2,"content":"the whole point there is that you had to learn"},{"from":4265.86,"to":4269.22,"location":2,"content":"an alignment system deliberately and separately."},{"from":4269.22,"to":4271.05,"location":2,"content":"You had to define the notion of alignment."},{"from":4271.05,"to":4272.73,"location":2,"content":"You had to define the model of calculating"},{"from":4272.73,"to":4275.52,"location":2,"content":"what the probability of different alignments were, and train it."},{"from":4275.52,"to":4279.7,"location":2,"content":"Whereas here, we never told the NMT system about alignments."},{"from":4279.7,"to":4281.74,"location":2,"content":"We never explicitly trained an alignment system."},{"from":4281.74,"to":4284.9,"location":2,"content":"We never had a loss function that tells you how good your alignment was."},{"from":4284.9,"to":4288.24,"location":2,"content":"We just gave the NMT system the apparatus to"},{"from":4288.24,"to":4291.72,"location":2,"content":"do something like alignment and told it to maximize the,"},{"from":4291.72,"to":4295.03,"location":2,"content":"ah, the cross entropy loss for doing machine translation,"},{"from":4295.03,"to":4298.22,"location":2,"content":"and then the network just learned alignment by itself."},{"from":4298.22,"to":4300.56,"location":2,"content":"I think this is the coolest thing about attention,"},{"from":4300.56,"to":4305.93,"location":2,"content":"is that it's learned some structure in a somewhat unsupervised way."},{"from":4305.93,"to":4308.18,"location":2,"content":"Okay. So in the last few minutes,"},{"from":4308.18,"to":4310.99,"location":2,"content":"I'm just going to, ah, generalize the notion of attention,"},{"from":4310.99,"to":4313.95,"location":2,"content":"because it turns out that attention is actually a very general, ah,"},{"from":4313.95,"to":4317.98,"location":2,"content":"deep learning technique that you can apply in lots of different circumstances."},{"from":4317.98,"to":4320.25,"location":2,"content":"So, you've seen that attention is a great way to improve"},{"from":4320.25,"to":4322.55,"location":2,"content":"the sequence-to-sequence model for MT,"},{"from":4322.55,"to":4325.38,"location":2,"content":"but you can actually use attention for other architectures that aren't"},{"from":4325.38,"to":4328.88,"location":2,"content":"seq2seq and also tasks that aren't MT."},{"from":4328.88,"to":4330.65,"location":2,"content":"So, to understand this,"},{"from":4330.65,"to":4334.74,"location":2,"content":"I'm going to somewhat redefine attention to a more general definition."},{"from":4334.74,"to":4336.89,"location":2,"content":"So here's our more general definition."},{"from":4336.89,"to":4339.52,"location":2,"content":"Suppose you have a set of values,"},{"from":4339.52,"to":4340.85,"location":2,"content":"each of which is a vector,"},{"from":4340.85,"to":4342.48,"location":2,"content":"and you also have a single vector,"},{"from":4342.48,"to":4344.02,"location":2,"content":"which we're calling the query."},{"from":4344.02,"to":4346.11,"location":2,"content":"Then attention is a way, ah,"},{"from":4346.11,"to":4349.03,"location":2,"content":"to compute a weighted sum of the values,"},{"from":4349.03,"to":4354.99,"location":2,"content":"but the way you weight it is dependent on the query [NOISE]."},{"from":4354.99,"to":4356.56,"location":2,"content":"So, we often phrased this, ah,"},{"from":4356.56,"to":4359.31,"location":2,"content":"as saying that the query is attending to the values,"},{"from":4359.31,"to":4361.94,"location":2,"content":"the idea being that you have all this information, that's in the values,"},{"from":4361.94,"to":4366.47,"location":2,"content":"and the query is somehow determining how it's gonna pay attention to the values."},{"from":4366.47,"to":4369,"location":2,"content":"So for example, in seq2seq, ah,"},{"from":4369,"to":4372.09,"location":2,"content":"the decoder hidden state is the query,"},{"from":4372.09,"to":4374.28,"location":2,"content":"the decoder hidden state on a particular time step is"},{"from":4374.28,"to":4377.7,"location":2,"content":"the query and is attending to all the encoder hidden states,"},{"from":4377.7,"to":4379.73,"location":2,"content":"which are the values."},{"from":4379.73,"to":4382.15,"location":2,"content":"All right, here's that definition again."},{"from":4382.15,"to":4386.11,"location":2,"content":"So, here's a way to kind of understand this intuitively, two alternative ways."},{"from":4386.11,"to":4388.17,"location":2,"content":"One is to think of it like this,"},{"from":4388.17,"to":4390.96,"location":2,"content":"you could think of it as the weighted sum is like"},{"from":4390.96,"to":4394.9,"location":2,"content":"a selective summary of the information in the values."},{"from":4394.9,"to":4396.3,"location":2,"content":"And I say selective,"},{"from":4396.3,"to":4399.15,"location":2,"content":"because your choice of how much you choose to draw from"},{"from":4399.15,"to":4401.98,"location":2,"content":"each value depends on the attention distribution."},{"from":4401.98,"to":4405.78,"location":2,"content":"Ah, so the distribution, ah, depends on the queries."},{"from":4405.78,"to":4409.44,"location":2,"content":"The query is determining how much you're gonna select from different, ah,"},{"from":4409.44,"to":4414.23,"location":2,"content":"values, and this is kind of similar to LSTMs that we learned about earlier this week."},{"from":4414.23,"to":4416.97,"location":2,"content":"LSTMs were all based on the idea of a gate that, ah,"},{"from":4416.97,"to":4419.77,"location":2,"content":"[NOISE] that defines how much information sho-"},{"from":4419.77,"to":4421.48,"location":2,"content":"should [NOISE] come from different elements,"},{"from":4421.48,"to":4424.03,"location":2,"content":"and the gate depends on the context."},{"from":4424.03,"to":4427.35,"location":2,"content":"So, the strength of LSTMs came from the idea that based on the context,"},{"from":4427.35,"to":4429.82,"location":2,"content":"you decide where you're going to draw information from,"},{"from":4429.82,"to":4432.1,"location":2,"content":"and this is kind of like the same idea."},{"from":4432.1,"to":4436.74,"location":2,"content":"The second way to think about attention is you could say that it's a way to obtain"},{"from":4436.74,"to":4441.07,"location":2,"content":"a fixed-size representation from an arbitrary set of representations."},{"from":4441.07,"to":4442.6,"location":2,"content":"So when I say arbitrary sets,"},{"from":4442.6,"to":4445.55,"location":2,"content":"I'm saying that you have this set of vectors called the values, right?"},{"from":4445.55,"to":4448.05,"location":2,"content":"And you could have ten values or you could have 100 values."},{"from":4448.05,"to":4450.94,"location":2,"content":"You could have, ah, [NOISE] any arbitrary number of these vectors."},{"from":4450.94,"to":4455.01,"location":2,"content":"But attention gives you a way to get a single vector,"},{"from":4455.01,"to":4457.06,"location":2,"content":"um, summary of that,"},{"from":4457.06,"to":4458.25,"location":2,"content":"which is the attention output,"},{"from":4458.25,"to":4461.06,"location":2,"content":"ah, using your query."},{"from":4461.06,"to":4463.69,"location":2,"content":"Okay. Ah, so the last thing, ah,"},{"from":4463.69,"to":4467.04,"location":2,"content":"is that there's actually several variants of attention,"},{"from":4467.04,"to":4470.25,"location":2,"content":"and, ah, this is something that you're going to look at a little in assignment four."},{"from":4470.25,"to":4472.59,"location":2,"content":"So, in our more general setting,"},{"from":4472.59,"to":4474.93,"location":2,"content":"we've seen that we have some values in the query."},{"from":4474.93,"to":4477.21,"location":2,"content":"Doing attention always involves computing"},{"from":4477.21,"to":4482.54,"location":2,"content":"the attention scores and then you apply Softmax to get the attention distribution,"},{"from":4482.54,"to":4486.53,"location":2,"content":"and then you use that attention distribution to take a weighted sum."},{"from":4486.53,"to":4489.89,"location":2,"content":"So this is, ah, always the outline of how attention works."},{"from":4489.89,"to":4492.6,"location":2,"content":"The part that can be different is this, ah, number one."},{"from":4492.6,"to":4496.19,"location":2,"content":"There are multiple ways you can compute the scores."},{"from":4496.19,"to":4499.8,"location":2,"content":"So, ah, last slides,"},{"from":4499.8,"to":4502.68,"location":2,"content":"here are the different ways you can compute the scores."},{"from":4502.68,"to":4505.02,"location":2,"content":"So the first one, which we've already seen today,"},{"from":4505.02,"to":4507.28,"location":2,"content":"is basic dot-product attention."},{"from":4507.28,"to":4511.52,"location":2,"content":"And the idea here, is that the score for a particulu- a particular value,"},{"from":4511.52,"to":4516.16,"location":2,"content":"h_i, is just the dot-product of the query and that particular value."},{"from":4516.16,"to":4517.89,"location":2,"content":"And, ah, in particular,"},{"from":4517.89,"to":4519.48,"location":2,"content":"this assumes that the size of"},{"from":4519.48,"to":4522.3,"location":2,"content":"your query vector and the size of your value vectors has to be the same,"},{"from":4522.3,"to":4524.94,"location":2,"content":"because you're taking dot-product [NOISE]."},{"from":4524.94,"to":4527.52,"location":2,"content":"Another, ah, version of ah,"},{"from":4527.52,"to":4530.34,"location":2,"content":"attention is called multiplicative attention."},{"from":4530.34,"to":4532.83,"location":2,"content":"And here, the idea is that the score of your ah,"},{"from":4532.83,"to":4539.28,"location":2,"content":"value h_i is going to be this bi-linear function of your query and that value."},{"from":4539.28,"to":4541.8,"location":2,"content":"So, in particular, we're putting this weight matrix in the middle,"},{"from":4541.8,"to":4543.35,"location":2,"content":"and that's a learnable parameter."},{"from":4543.35,"to":4547.29,"location":2,"content":"You're learning the best way matric- ma- weight matrix in order to get the scores,"},{"from":4547.29,"to":4549.56,"location":2,"content":"the attention scores that are useful."},{"from":4549.56,"to":4552.49,"location":2,"content":"The last one is called additive attention."},{"from":4552.49,"to":4553.99,"location":2,"content":"So what's happening here,"},{"from":4553.99,"to":4558.68,"location":2,"content":"is that the score of the value h_i is ah."},{"from":4558.68,"to":4561.09,"location":2,"content":"You get it by applying"},{"from":4561.09,"to":4566.06,"location":2,"content":"a linear transformation to both the value and the query and then you add them together,"},{"from":4566.06,"to":4568.14,"location":2,"content":"and then you put them through a non-linearity like tan"},{"from":4568.14,"to":4570.99,"location":2,"content":"h. And then lastly, you take that vector,"},{"from":4570.99,"to":4572.93,"location":2,"content":"and you take the dot-product with"},{"from":4572.93,"to":4577.31,"location":2,"content":"a weight vector to give you a single number that is the score."},{"from":4577.31,"to":4581.58,"location":2,"content":"So here, you've got two different weight matrices and also a weight vector,"},{"from":4581.58,"to":4583.88,"location":2,"content":"which are the learnable parameters."},{"from":4583.88,"to":4585.93,"location":2,"content":"One thing that's different here,"},{"from":4585.93,"to":4588.44,"location":2,"content":"is that there's kind of an additional hyperparameter,"},{"from":4588.44,"to":4590.65,"location":2,"content":"which is the attention dimensionality."},{"from":4590.65,"to":4592.89,"location":2,"content":"So that's kind of, ah,"},{"from":4592.89,"to":4597.36,"location":2,"content":"the- I think it's the heights of the W-1 and W-2 and it's the length of V, right?"},{"from":4597.36,"to":4600.06,"location":2,"content":"You can choose what size that dimension is."},{"from":4600.06,"to":4602.88,"location":2,"content":"It's kind of like a hidden layer in the computation."},{"from":4602.88,"to":4607.65,"location":2,"content":"So, um, you can decide how big you want that intermediate representation to be."},{"from":4607.65,"to":4610.11,"location":2,"content":"Okay. So I'm not going to tell you anymore about that because that's"},{"from":4610.11,"to":4612.15,"location":2,"content":"actually one of the questions in the assignment, ah,"},{"from":4612.15,"to":4613.5,"location":2,"content":"assignment four is to think about"},{"from":4613.5,"to":4617.3,"location":2,"content":"the relative advantages and disadvantages of these models."},{"from":4617.3,"to":4619.55,"location":2,"content":"Okay. So here's a summary of today."},{"from":4619.55,"to":4621.12,"location":2,"content":"It really is the last slide."},{"from":4621.12,"to":4623.43,"location":2,"content":"Second last, last time, but this was the last slide."},{"from":4623.43,"to":4627,"location":2,"content":"So, we learned about the history of MT [NOISE]."},{"from":4627,"to":4628.57,"location":2,"content":"We learned about how in 2014,"},{"from":4628.57,"to":4632.13,"location":2,"content":"neural MT revolutionized MT [NOISE]."},{"from":4632.13,"to":4635.34,"location":2,"content":"We learned about how sequence-to-sequence is the right architecture for NMT,"},{"from":4635.34,"to":4637.77,"location":2,"content":"and it uses two RNNs, and lastly,"},{"from":4637.77,"to":4639.87,"location":2,"content":"we learned about how attention is a way to focus on"},{"from":4639.87,"to":4643.36,"location":2,"content":"particular parts of the input. All right, thanks."}]}