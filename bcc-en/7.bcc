{"font_size":0.4,"font_color":"#FFFFFF","background_alpha":0.5,"background_color":"#9C27B0","Stroke":"none","body":[{"from":4.7,"to":7.14,"location":2,"content":"Hi, everyone. I'm Abby."},{"from":7.14,"to":8.36,"location":2,"content":"If you weren't here last week,"},{"from":8.36,"to":10.19,"location":2,"content":"I'm the head TA of this course."},{"from":10.19,"to":13.32,"location":2,"content":"And this is the second [NOISE] of three lectures that I'm"},{"from":13.32,"to":17.36,"location":2,"content":"going to be giving on RNNs and related topics."},{"from":17.36,"to":20.43,"location":2,"content":"Okay. So, welcome to week four."},{"from":20.43,"to":22.96,"location":2,"content":"Today, we're going to be learning about vanishing gradients,"},{"from":22.96,"to":25.12,"location":2,"content":"and some more complex types of RNNs."},{"from":25.12,"to":26.82,"location":2,"content":"So, before we get started,"},{"from":26.82,"to":28,"location":2,"content":"I've got a few announcements."},{"from":28,"to":33.15,"location":2,"content":"Uh, the first announcement is that assignment four is released today, uh,"},{"from":33.15,"to":35.84,"location":2,"content":"it's due Thursday of next week, not Tuesday,"},{"from":35.84,"to":39.8,"location":2,"content":"so that means you have two days more to do it than you did for all the other homeworks."},{"from":39.8,"to":41.41,"location":2,"content":"And the reason for that is assignment four is"},{"from":41.41,"to":43.36,"location":2,"content":"probably more work than the other homework so far,"},{"from":43.36,"to":45.41,"location":2,"content":"so don't be surprised by that."},{"from":45.41,"to":48.58,"location":2,"content":"Uh, assignment four is all about Neural Machine Translation."},{"from":48.58,"to":52.13,"location":2,"content":"Uh, we're gonna learn about NMT on Thursday's lecture this week."},{"from":52.13,"to":54.27,"location":2,"content":"And, uh, this is really exciting,"},{"from":54.27,"to":57.6,"location":2,"content":"because actually CS 224 has never had an NMT assignment before,"},{"from":57.6,"to":58.74,"location":2,"content":"so this is all new this year,"},{"from":58.74,"to":62.42,"location":2,"content":"and you're gonna be the first year students who are going to be doing an NMT assignment."},{"from":62.42,"to":64.91,"location":2,"content":"Uh, something else that's different about"},{"from":64.91,"to":67.61,"location":2,"content":"assignment four is that you're going to be using Azure, which is, uh,"},{"from":67.61,"to":69.17,"location":2,"content":"a cloud computing service,"},{"from":69.17,"to":73.14,"location":2,"content":"in order to train your NMT systems on a virtual machine with a GPU."},{"from":73.14,"to":77.36,"location":2,"content":"And, uh, this is necessary in order to be able to do it in a reasonable amount of time."},{"from":77.36,"to":79.25,"location":2,"content":"So, I have a warning which is,"},{"from":79.25,"to":80.93,"location":2,"content":"if you're a person who perhaps doesn't have, ah,"},{"from":80.93,"to":84.14,"location":2,"content":"learnt- a lot of experience working on remote machines,"},{"from":84.14,"to":86.36,"location":2,"content":"so for example if you're not very familiar with SSH,"},{"from":86.36,"to":88.88,"location":2,"content":"or tmux, or remote text editing,"},{"from":88.88,"to":91.58,"location":2,"content":"then I advise you to budget some extra time for assignment four,"},{"from":91.58,"to":94.94,"location":2,"content":"because that's probably gonna take you a little while to set up and get used to."},{"from":94.94,"to":97.28,"location":2,"content":"So, again, I'm going to emphasize,"},{"from":97.28,"to":100.25,"location":2,"content":"do get started early on assignment four, because, uh,"},{"from":100.25,"to":103.78,"location":2,"content":"the NMT system takes about four hours to train on your virtual machine,"},{"from":103.78,"to":107.23,"location":2,"content":"so you really can't start it the night before and expect to get it in on time."},{"from":107.23,"to":111.25,"location":2,"content":"Uh, and assignment four is really quite a lot more complicated than assignment three."},{"from":111.25,"to":115.25,"location":2,"content":"So, uh, don't get into a false sense of security if you found assignment three easy."},{"from":115.25,"to":120.64,"location":2,"content":"Um, so Thursday's slides on NMT are ready on the website today,"},{"from":120.64,"to":122.61,"location":2,"content":"so you can even start looking at it today if you"},{"from":122.61,"to":125.91,"location":2,"content":"want- if you wanna get started on assignment four early."},{"from":125.91,"to":128.31,"location":2,"content":"Uh, so, I have a few more announcements, uh,"},{"from":128.31,"to":130.08,"location":2,"content":"on the subject of projects, uh,"},{"from":130.08,"to":131.86,"location":2,"content":"next week's lectures are going to be all about projects."},{"from":131.86,"to":134.63,"location":2,"content":"So, you're going to hear about, uh, question answering,"},{"from":134.63,"to":136.44,"location":2,"content":"and the default final projects,"},{"from":136.44,"to":138.62,"location":2,"content":"and then you're also gonna get some tips about how you might,"},{"from":138.62,"to":141.21,"location":2,"content":"uh, choose and define your own custom projects."},{"from":141.21,"to":144.18,"location":2,"content":"So, it's fine if you're not thinking about a project this week, that's okay."},{"from":144.18,"to":147.13,"location":2,"content":"You can delay until next week to start thinking about it for the first time."},{"from":147.13,"to":149.63,"location":2,"content":"But if you are a person who is already thinking about your projects,"},{"from":149.63,"to":152.2,"location":2,"content":"for example, if you're trying to choose your custom projects, uh,"},{"from":152.2,"to":154.29,"location":2,"content":"then you should check out the website's project page,"},{"from":154.29,"to":156.23,"location":2,"content":"because it has quite a lot of information about, uh,"},{"from":156.23,"to":159.17,"location":2,"content":"how to choose your projects, and also some inspiration."},{"from":159.17,"to":161.65,"location":2,"content":"And that includes- we've collected some, uh,"},{"from":161.65,"to":164.65,"location":2,"content":"project ideas from various members of the Stanford AI Lab."},{"from":164.65,"to":168.16,"location":2,"content":"So, these are faculty and PhD students and postdocs,"},{"from":168.16,"to":169.76,"location":2,"content":"who have ideas for, uh,"},{"from":169.76,"to":171.56,"location":2,"content":"NLP deep learning projects that they would like"},{"from":171.56,"to":174.15,"location":2,"content":"CS224n students such as yourself to work on."},{"from":174.15,"to":177.09,"location":2,"content":"So, especially, if you're looking to maybe get into research later,"},{"from":177.09,"to":179.18,"location":2,"content":"this is a really great opportunity, uh,"},{"from":179.18,"to":180.91,"location":2,"content":"to work with someone in the Stanford AI Lab,"},{"from":180.91,"to":183.7,"location":2,"content":"and maybe get some mentorship as well."},{"from":183.7,"to":186.84,"location":2,"content":"Okay. So here's an overview."},{"from":186.84,"to":190.08,"location":2,"content":"Uh, last week, we learned about Recurrent Neural Networks,"},{"from":190.08,"to":192.72,"location":2,"content":"um, we learned about why they're really great for Language Modeling."},{"from":192.72,"to":195.34,"location":2,"content":"And today, we're gonna learn about some problems with RNNs,"},{"from":195.34,"to":196.88,"location":2,"content":"and we're gonna learn about how to fix them."},{"from":196.88,"to":202.03,"location":2,"content":"And this is gonna motiva- motivate us to learn about some more complex RNN variants."},{"from":202.03,"to":204.3,"location":2,"content":"And then, uh, next lecture on Thursday,"},{"from":204.3,"to":207.57,"location":2,"content":"we're going to, uh, have some more application-based, uh, contents,"},{"from":207.57,"to":209.75,"location":2,"content":"so we are going to be learning about Neural Machine Translation,"},{"from":209.75,"to":211.7,"location":2,"content":"which is a really important task in, uh,"},{"from":211.7,"to":213.83,"location":2,"content":"NLP and deep learning, and in particular,"},{"from":213.83,"to":217.44,"location":2,"content":"we're gonna learn about this architecture called sequence-to-sequence with attention."},{"from":217.44,"to":220.08,"location":2,"content":"But in more detail,"},{"from":220.08,"to":221.64,"location":2,"content":"today's lecture, uh, first,"},{"from":221.64,"to":223.65,"location":2,"content":"we are going to learn about the vanishing gradient problem."},{"from":223.65,"to":226.22,"location":2,"content":"And this is gonna motivate us to learn about two new types of"},{"from":226.22,"to":228.89,"location":2,"content":"RNN called Long Short-Term Memory,"},{"from":228.89,"to":230.85,"location":2,"content":"and Gated Recurrent Unit."},{"from":230.85,"to":232.94,"location":2,"content":"We're also going to learn about some other kind of"},{"from":232.94,"to":235.64,"location":2,"content":"miscellaneous fixes for the vanishing gradient problem,"},{"from":235.64,"to":237.19,"location":2,"content":"or the exploding gradient problem."},{"from":237.19,"to":238.37,"location":2,"content":"Uh, so in particular,"},{"from":238.37,"to":239.79,"location":2,"content":"we're going to learn about gradient clipping,"},{"from":239.79,"to":242.19,"location":2,"content":"which is, uh, fairly simple, but quite important."},{"from":242.19,"to":244.71,"location":2,"content":"Uh, we're also going to learn about skip connections,"},{"from":244.71,"to":247.08,"location":2,"content":"which is a fairly new neural architecture,"},{"from":247.08,"to":248.15,"location":2,"content":"which tries to, uh,"},{"from":248.15,"to":249.35,"location":2,"content":"fix the vanishing gradient problem."},{"from":249.35,"to":251.64,"location":2,"content":"[NOISE] And then, at the end of the lecture,"},{"from":251.64,"to":254.21,"location":2,"content":"we're gonna learn about some more fancy RNN variants such as, uh,"},{"from":254.21,"to":257.78,"location":2,"content":"bidirectional RN- RNNs, those are the ones which go not just left to right,"},{"from":257.78,"to":258.98,"location":2,"content":"but also right to left,"},{"from":258.98,"to":261.74,"location":2,"content":"and we're going to learn about multi-layer RNNs."},{"from":261.74,"to":264.88,"location":2,"content":"And that's when you stack multiple RNNs on top of each other."},{"from":264.88,"to":267.56,"location":2,"content":"So, there's a lot of important definitions today."},{"from":267.56,"to":269.87,"location":2,"content":"Um, so, you're gonna find that the information in"},{"from":269.87,"to":271.16,"location":2,"content":"this lecture is pretty important for"},{"from":271.16,"to":275.45,"location":2,"content":"assignment four and probably for your project as well."},{"from":275.45,"to":280.25,"location":2,"content":"Okay. So, let's get started thinking about the vanishing gradients."},{"from":280.25,"to":282.35,"location":2,"content":"Uh, so here we have an RNN,"},{"from":282.35,"to":284.39,"location":2,"content":"with, let say, ah, four steps,"},{"from":284.39,"to":286.95,"location":2,"content":"and suppose that we have some kind of loss that's, uh,"},{"from":286.95,"to":290.92,"location":2,"content":"J4, and that's computed based on the four hidden states."},{"from":290.92,"to":296.3,"location":2,"content":"So, let's suppose we're interested in asking what is the derivative of this loss J4,"},{"from":296.3,"to":298.34,"location":2,"content":"with respect to the hidden states,"},{"from":298.34,"to":300.87,"location":2,"content":"uh, h1, the first hidden state?"},{"from":300.87,"to":303.39,"location":2,"content":"So, I'm representing that with this, uh,"},{"from":303.39,"to":305.72,"location":2,"content":"blue arrow notation to kind of represent how we have"},{"from":305.72,"to":309.07,"location":2,"content":"to make the gradients flow backwards in order to complete this."},{"from":309.07,"to":311.75,"location":2,"content":"So, if we're interested in what this gradient is,"},{"from":311.75,"to":313.85,"location":2,"content":"we can apply the chain rule and say, \"Well,"},{"from":313.85,"to":315.04,"location":2,"content":"it's the product of the, uh,"},{"from":315.04,"to":317.56,"location":2,"content":"gradient of the loss with respect to h2,"},{"from":317.56,"to":320.98,"location":2,"content":"and then gradient of h2, with respect to h1.\""},{"from":320.98,"to":323.67,"location":2,"content":"And then, similarly, we can decompose that"},{"from":323.67,"to":327.25,"location":2,"content":"again using the chain rule, and we can do it again."},{"from":327.25,"to":331.79,"location":2,"content":"So, what we've done here is we've decomposed the gradient that we were interested in,"},{"from":331.79,"to":335.38,"location":2,"content":"into the products of these various intermediate gradients."},{"from":335.38,"to":339.29,"location":2,"content":"And in particular, we're seeing all these ht by ht minus 1,"},{"from":339.29,"to":341.98,"location":2,"content":"uh, adjacent gradients of the hidden states."},{"from":341.98,"to":343.91,"location":2,"content":"So, the thing I want to ask you is,"},{"from":343.91,"to":347.13,"location":2,"content":"what happens if these gradients are small?"},{"from":347.13,"to":349.02,"location":2,"content":"Given that there's a lot of them,"},{"from":349.02,"to":352.13,"location":2,"content":"uh, what happens if they're small in magnitude?"},{"from":352.13,"to":356.87,"location":2,"content":"So, the overall problem of the vanishing gradient problem,"},{"from":356.87,"to":359.34,"location":2,"content":"is that when these gradients are small,"},{"from":359.34,"to":362.52,"location":2,"content":"then our overall gradient is gonna get smaller and smaller,"},{"from":362.52,"to":364.38,"location":2,"content":"as it back propagates further."},{"from":364.38,"to":369.15,"location":2,"content":"Because the accumulated gradient is the product of all of these intermediate gradients."},{"from":369.15,"to":371.27,"location":2,"content":"And when you multiply something by something small,"},{"from":371.27,"to":372.97,"location":2,"content":"then the whole thing gets smaller."},{"from":372.97,"to":375.22,"location":2,"content":"So, that's what I'm representing here with these, uh,"},{"from":375.22,"to":378.29,"location":2,"content":"smaller and smaller blue arrows going backwards."},{"from":378.29,"to":381.56,"location":2,"content":"So, that's the general idea of the vanishing gradient problem."},{"from":381.56,"to":383.74,"location":2,"content":"Here's a slightly more formal definition."},{"from":383.74,"to":385.9,"location":2,"content":"So, if you remember from last time,"},{"from":385.9,"to":388.1,"location":2,"content":"uh, if we have a null RNN,"},{"from":388.1,"to":390.49,"location":2,"content":"then the hidden state ht is"},{"from":390.49,"to":393.49,"location":2,"content":"computed as a function of the previous hidden state ht minus 1,"},{"from":393.49,"to":394.81,"location":2,"content":"and the current input xt."},{"from":394.81,"to":397.88,"location":2,"content":"Uh, so you might remember in the previous lecture we"},{"from":397.88,"to":401.07,"location":2,"content":"said that xt were one-hot vectors representing words,"},{"from":401.07,"to":402.62,"location":2,"content":"and then ET is the embedding."},{"from":402.62,"to":404.06,"location":2,"content":"Uh, this lecture we're going to be,"},{"from":404.06,"to":405.11,"location":2,"content":"uh, getting rid of that detail,"},{"from":405.11,"to":407.06,"location":2,"content":"and we're just gonna be thinking very abstractly about"},{"from":407.06,"to":409.45,"location":2,"content":"an RNN that has some kind of input xt,"},{"from":409.45,"to":411.21,"location":2,"content":"and xt is just any kind of vector."},{"from":411.21,"to":412.26,"location":2,"content":"Probably a dense vector,"},{"from":412.26,"to":413.93,"location":2,"content":"but you know, it could be words or not."},{"from":413.93,"to":415.5,"location":2,"content":"It could be one-hot or dense."},{"from":415.5,"to":417.81,"location":2,"content":"Uh, but that's just the input."},{"from":417.81,"to":420.21,"location":2,"content":"So, that's the, uh,"},{"from":420.21,"to":423.37,"location":2,"content":"the definition that we learned last time for Vanilla RNNs."},{"from":423.37,"to":425.93,"location":2,"content":"So, this means that the derivative of ht,"},{"from":425.93,"to":428.48,"location":2,"content":"hidden state on step t with respect to the previous hidden state,"},{"from":428.48,"to":430.07,"location":2,"content":"uh, is this expression here."},{"from":430.07,"to":433.88,"location":2,"content":"Uh, so this is just an application of the chain rule, and, uh,"},{"from":433.88,"to":436.01,"location":2,"content":"if you looked long enough or refer back to"},{"from":436.01,"to":438.59,"location":2,"content":"the backprop lecture you'll see, uh, that that make sense."},{"from":438.59,"to":440.58,"location":2,"content":"So, in particular, we're, um,"},{"from":440.58,"to":443.21,"location":2,"content":"multiplying by Wh at the end, uh,"},{"from":443.21,"to":447.4,"location":2,"content":"because we have the multiplication of Wh and ht minus 1 on the inside."},{"from":447.4,"to":450.59,"location":2,"content":"Okay. So, if you remember, on the previous slide,"},{"from":450.59,"to":453.65,"location":2,"content":"we were thinking about what's the gradient of the loss on some step,"},{"from":453.65,"to":454.91,"location":2,"content":"step i I'd say,"},{"from":454.91,"to":456.98,"location":2,"content":"with respect to a hidden state hj,"},{"from":456.98,"to":458.8,"location":2,"content":"on some previous step j."},{"from":458.8,"to":461.95,"location":2,"content":"And maybe J is quite a few steps before i."},{"from":461.95,"to":464.24,"location":2,"content":"So, we can now write this,"},{"from":464.24,"to":466.13,"location":2,"content":"uh, in the following way."},{"from":466.13,"to":468.45,"location":2,"content":"So just by applying the chain rule,"},{"from":468.45,"to":471.37,"location":2,"content":"now on the first line we're saying that this derivative that we're interested in"},{"from":471.37,"to":474.92,"location":2,"content":"can be decomposed into the derivative with respect to step i,"},{"from":474.92,"to":476.27,"location":2,"content":"which is kind of the last step,"},{"from":476.27,"to":480.26,"location":2,"content":"and then do all of those intermediate gradients of the adjacent hidden states as well."},{"from":480.26,"to":484.1,"location":2,"content":"So, that- that first slide is just exactly the same thing as we were looking at on the,"},{"from":484.1,"to":488.3,"location":2,"content":"uh, the picture, uh, the diagram on the previous slide."},{"from":488.3,"to":491.44,"location":2,"content":"Okay. And then, given that we figured out what is, uh,"},{"from":491.44,"to":494.03,"location":2,"content":"dht by dht minus one,"},{"from":494.03,"to":495.05,"location":2,"content":"ah, further on the slide,"},{"from":495.05,"to":496.77,"location":2,"content":"then we can just substitute that in."},{"from":496.77,"to":499.37,"location":2,"content":"So, what we're finding is that this overall gradient that we're"},{"from":499.37,"to":501.8,"location":2,"content":"interested in, in particular,"},{"from":501.8,"to":503.44,"location":2,"content":"has this term, uh,"},{"from":503.44,"to":506.79,"location":2,"content":"Wh, the weight matrix, and it's, uh,"},{"from":506.79,"to":509.02,"location":2,"content":"multiplied by itself, i minus j times,"},{"from":509.02,"to":512.53,"location":2,"content":"because there's i minus j many steps between, uh,"},{"from":512.53,"to":513.83,"location":2,"content":"step j and step i,"},{"from":513.83,"to":517.52,"location":2,"content":"which is the- the distance that we're traveling with this gradient."},{"from":517.52,"to":519.75,"location":2,"content":"So, the big problem here is,"},{"from":519.75,"to":522.66,"location":2,"content":"if this weight matrix Wh is small,"},{"from":522.66,"to":525.45,"location":2,"content":"then this term is gonna get vanishingly small,"},{"from":525.45,"to":529.96,"location":2,"content":"exponentially small, as i and j get further apart."},{"from":529.96,"to":533.74,"location":2,"content":"So, to give this a little more detail, uh,"},{"from":533.74,"to":535.15,"location":2,"content":"we can think about the, uh,"},{"from":535.15,"to":538.36,"location":2,"content":"L2 matrix norms of all of these matrices, right?"},{"from":538.36,"to":543.84,"location":2,"content":"And, uh, as a- as a- uh, as a- sorry."},{"from":543.84,"to":546.4,"location":2,"content":"I'm- it's a known fact of,"},{"from":546.4,"to":548.34,"location":2,"content":"uh, L2 norms that you have this, um,"},{"from":548.34,"to":550.61,"location":2,"content":"inequality that's the, uh,"},{"from":550.61,"to":552.45,"location":2,"content":"norm of the products of"},{"from":552.45,"to":555.74,"location":2,"content":"some matrices is less and equal to the product of the norms of the matrices."},{"from":555.74,"to":559.71,"location":2,"content":"So, in particular, we're seeing that the norm of this gradient that we're interested in,"},{"from":559.71,"to":561.9,"location":2,"content":"is less than or equal to, uh,"},{"from":561.9,"to":566.69,"location":2,"content":"the product i minus j many times of the norm of the weight matrix Wh."},{"from":566.69,"to":570.41,"location":2,"content":"So, this is what we mean when we say we're concerned about Wh being small,"},{"from":570.41,"to":574.18,"location":2,"content":"because if it's small, then the thing on the left has to be exponentially small."},{"from":574.18,"to":576,"location":2,"content":"So in particular in this,"},{"from":576,"to":577.07,"location":2,"content":"uh, paper that, uh,"},{"from":577.07,"to":580.08,"location":2,"content":"you can take a look at the bottom if you're interested, um, uh,"},{"from":580.08,"to":581.96,"location":2,"content":"Pascanu et al showed that if"},{"from":581.96,"to":586.26,"location":2,"content":"the largest eigenvalue of the weight matrix Wh is less than one,"},{"from":586.26,"to":589.91,"location":2,"content":"then this gradient on the left is going to shrink exponentially."},{"from":589.91,"to":592.38,"location":2,"content":"And you can probably see intuitively why this is true."},{"from":592.38,"to":593.96,"location":2,"content":"So if, you know, as a simplifying assumption,"},{"from":593.96,"to":596.09,"location":2,"content":"we suppose that Wh was not a matrix,"},{"from":596.09,"to":598.76,"location":2,"content":"but simply a scalar that was just a single number,"},{"from":598.76,"to":601.76,"location":2,"content":"then you can see why if that number was greater than one,"},{"from":601.76,"to":603.35,"location":2,"content":"then the whole thing is gonna explode."},{"from":603.35,"to":604.96,"location":2,"content":"And if that number is less than one,"},{"from":604.96,"to":606.3,"location":2,"content":"then it is going to shrink"},{"from":606.3,"to":609.07,"location":2,"content":"exponentially as you multiply by the same number again and again."},{"from":609.07,"to":612.71,"location":2,"content":"Uh, so you can check out the paper for more details,"},{"from":612.71,"to":614.96,"location":2,"content":"but here, uh, the bound is one,"},{"from":614.96,"to":617.73,"location":2,"content":"partially because we have the sigmoid nonlinearity."},{"from":617.73,"to":621.14,"location":2,"content":"And that's, uh, based on the bounds of what we know as the,"},{"from":621.14,"to":624.49,"location":2,"content":"uh, norm of the sigmoid function to be."},{"from":624.49,"to":629.4,"location":2,"content":"So, uh, this shows you why if the, uh,"},{"from":629.4,"to":631.02,"location":2,"content":"Wh matrix is small,"},{"from":631.02,"to":632.6,"location":2,"content":"or if its largest eigenvalue was small,"},{"from":632.6,"to":634.28,"location":2,"content":"then we're going to have vanishing gradients."},{"from":634.28,"to":635.93,"location":2,"content":"And similarly, if you check out the paper,"},{"from":635.93,"to":638.14,"location":2,"content":"you can see that there's a similar proof, uh,"},{"from":638.14,"to":640.98,"location":2,"content":"relating if the largest eigenvalue is greater than one,"},{"from":640.98,"to":643,"location":2,"content":"to having exploding gradients."},{"from":643,"to":645.14,"location":2,"content":"So that's when the gradients get bigger and bigger,"},{"from":645.14,"to":648.27,"location":2,"content":"as you backprop further."},{"from":648.27,"to":652.08,"location":2,"content":"Okay. So hopefully I've convinced you that"},{"from":652.08,"to":655.24,"location":2,"content":"vanishing gradients is a phenomenon that happens in our norms."},{"from":655.24,"to":657.48,"location":2,"content":"But I haven't yet said why this is a problem."},{"from":657.48,"to":660.34,"location":2,"content":"So, why should we view this as a bad thing,"},{"from":660.34,"to":662.51,"location":2,"content":"if the gradients are getting larger and larger,"},{"from":662.51,"to":664.57,"location":2,"content":"or smaller and smaller as you backprop?"},{"from":664.57,"to":668.79,"location":2,"content":"So here's, uh, here's a picture that might illustrate why it's a bad thing."},{"from":668.79,"to":670.5,"location":2,"content":"So, uh, as before,"},{"from":670.5,"to":671.79,"location":2,"content":"suppose that we're thinking about,"},{"from":671.79,"to":674.13,"location":2,"content":"what's the derivative of the loss on"},{"from":674.13,"to":676.74,"location":2,"content":"the fourth step with respect to the first hidden state?"},{"from":676.74,"to":678.27,"location":2,"content":"And we have this situation where"},{"from":678.27,"to":681.56,"location":2,"content":"the gradient is getting smaller and smaller as it goes backwards."},{"from":681.56,"to":684.82,"location":2,"content":"But then, think about what is the gradient of let's say"},{"from":684.82,"to":688.27,"location":2,"content":"the loss in the second step also with respect to the first hidden state."},{"from":688.27,"to":690.75,"location":2,"content":"So I'm representing that with the orange arrows."},{"from":690.75,"to":692.63,"location":2,"content":"And what my point is here,"},{"from":692.63,"to":697.76,"location":2,"content":"is that the magnitude of the gradient signal from close by,"},{"from":697.76,"to":702.17,"location":2,"content":"is a lot bigger than the magnitude of the gradient signal from far away."},{"from":702.17,"to":705.79,"location":2,"content":"And this means that when you update your model weights,"},{"from":705.79,"to":708.52,"location":2,"content":"the signal that you're getting from close by is gonna"},{"from":708.52,"to":710.77,"location":2,"content":"be so much bigger than the signal from far away,"},{"from":710.77,"to":712.59,"location":2,"content":"that essentially you're only going to learn,"},{"from":712.59,"to":714.61,"location":2,"content":"you're only going to optimize with respect to"},{"from":714.61,"to":717.53,"location":2,"content":"these nearby effects and not the long-term effects."},{"from":717.53,"to":722.2,"location":2,"content":"So you're gonna, you're gonna lose the long-term effects, er, inside the,"},{"from":722.2,"to":727.38,"location":2,"content":"the nearby effects. Any questions about this, yeah?"},{"from":727.38,"to":731.49,"location":2,"content":"So, uh, where they say there that you do actual updates."},{"from":731.49,"to":735.24,"location":2,"content":"You know, there are actually some that are multiple chains, not just one chain."},{"from":735.24,"to":737.82,"location":2,"content":"So the nearer term should cover it."},{"from":737.82,"to":739.33,"location":2,"content":"Sorry, what's the last part?"},{"from":739.33,"to":742.4,"location":2,"content":"The nearer term should have a larger effect considering you're"},{"from":742.4,"to":745.87,"location":2,"content":"updating the sum of the weights over different chains."},{"from":745.87,"to":749.03,"location":2,"content":"Okay. So I think, ah, the observation was that,"},{"from":749.03,"to":750.25,"location":2,"content":"given that, for example,"},{"from":750.25,"to":752.8,"location":2,"content":"in Language Modeling you might be summing over multiple losses."},{"from":752.8,"to":755.86,"location":2,"content":"There is a loss in every step and you sum all of them and that's your overall loss."},{"from":755.86,"to":760.6,"location":2,"content":"Then you do want to update more with respect to the nearby losses than the far losses."},{"from":760.6,"to":761.95,"location":2,"content":"So I think, uh, yeah,"},{"from":761.95,"to":763.75,"location":2,"content":"so if the design of your objective function"},{"from":763.75,"to":765.46,"location":2,"content":"is that it's the sum of the loss in every step,"},{"from":765.46,"to":766.76,"location":2,"content":"then you do want to, uh,"},{"from":766.76,"to":768.33,"location":2,"content":"weight all of them equally."},{"from":768.33,"to":770.41,"location":2,"content":"I think, uh, my point was more about,"},{"from":770.41,"to":772.67,"location":2,"content":"what is the influence of, uh,"},{"from":772.67,"to":775.28,"location":2,"content":"the action of the weight matrix at this early stage."},{"from":775.28,"to":777.6,"location":2,"content":"What is its influence on a loss that's nearby?"},{"from":777.6,"to":780.25,"location":2,"content":"And what is its influence on a loss that's far away?"},{"from":780.25,"to":782.88,"location":2,"content":"Um, and due to, uh,"},{"from":782.88,"to":785.29,"location":2,"content":"the dynamics of how the vanishing gradient, uh,"},{"from":785.29,"to":787.42,"location":2,"content":"problem works, then, uh,"},{"from":787.42,"to":789.25,"location":2,"content":"the influence on the loss that's far away"},{"from":789.25,"to":791.18,"location":2,"content":"is gonna be much less than the influence nearby."},{"from":791.18,"to":795.04,"location":2,"content":"And I'm gonna give some more linguistics examples later of why you might want to learn,"},{"from":795.04,"to":797.07,"location":2,"content":"uh, the connections that are farther away."},{"from":797.07,"to":798.19,"location":2,"content":"So essentially the problem is,"},{"from":798.19,"to":800.26,"location":2,"content":"in situations where you do want to learn the connection"},{"from":800.26,"to":803.13,"location":2,"content":"between something that happens early and something that happens later,"},{"from":803.13,"to":805.09,"location":2,"content":"then you're going to be unable to learn that connection."},{"from":805.09,"to":808.33,"location":2,"content":"Uh, so we'll see some motivating examples in a minute."},{"from":808.33,"to":816.13,"location":2,"content":"Any other questions on this? Yeah?"},{"from":816.13,"to":819.97,"location":2,"content":"Um, I'm getting confused like, why are you talking about like dh, dj dh."},{"from":819.97,"to":826.51,"location":2,"content":"Uh, it's like H parameter, like, are we going-"},{"from":826.51,"to":826.84,"location":2,"content":"Yeah."},{"from":826.84,"to":827.02,"location":2,"content":"from-"},{"from":827.02,"to":828.68,"location":2,"content":"Okay. That's a great question."},{"from":828.68,"to":832.51,"location":2,"content":"So you're asking why are we interested in some kind of dj by"},{"from":832.51,"to":836.93,"location":2,"content":"dh given that we're not updating H. H is an activation not a weight."},{"from":836.93,"to":840.52,"location":2,"content":"Um, so the reason why we're thinking about that,"},{"from":840.52,"to":844.33,"location":2,"content":"is because when you think about what is dj by dw,"},{"from":844.33,"to":845.8,"location":2,"content":"which is a thing that we're going to update."},{"from":845.8,"to":850.3,"location":2,"content":"That's always gonna be in terms of dj by dh at some point, right?"},{"from":850.3,"to":852.04,"location":2,"content":"So if we're thinking about W, you know,"},{"from":852.04,"to":853.62,"location":2,"content":"and how it acts on, uh,"},{"from":853.62,"to":855.55,"location":2,"content":"the transmission from h_1 to h_2,"},{"from":855.55,"to":861.36,"location":2,"content":"then dj4 by W in that position is going to have to go through dj4 by dh_2."},{"from":861.36,"to":863.62,"location":2,"content":"So if we're getting vanishing gradients,"},{"from":863.62,"to":865.79,"location":2,"content":"uh, as we back propagate further,"},{"from":865.79,"to":867.19,"location":2,"content":"then it's kind of like a bottleneck."},{"from":867.19,"to":869.74,"location":2,"content":"Then you're certainly going to have vanishing gradients as they affect, uh,"},{"from":869.74,"to":871.54,"location":2,"content":"the recurrence matrix there,"},{"from":871.54,"to":878.88,"location":2,"content":"and indeed the matrix that's applied to the inputs."},{"from":878.88,"to":882.15,"location":2,"content":"Okay. I'm gonna move off now."},{"from":882.15,"to":886.3,"location":2,"content":"Uh, so another way to explain why vanishing gradients is a problem,"},{"from":886.3,"to":889.43,"location":2,"content":"is you can think of it as, uh, a gradient."},{"from":889.43,"to":893.18,"location":2,"content":"You can think of it as a measure of the effect of the past on the future."},{"from":893.18,"to":894.76,"location":2,"content":"So we've already talked about this little bit."},{"from":894.76,"to":897.46,"location":2,"content":"Uh, gradient is like saying, if I change, uh,"},{"from":897.46,"to":899.58,"location":2,"content":"this weight or this activation a little bit,"},{"from":899.58,"to":903.01,"location":2,"content":"then how much and how does it affect this thing in the future."},{"from":903.01,"to":908.65,"location":2,"content":"So in particular, if our gradient is becoming vanishingly small over longer distances,"},{"from":908.65,"to":912.13,"location":2,"content":"let say from step T, step T to step T plus N,"},{"from":912.13,"to":915.79,"location":2,"content":"then we can't tell whether in one of two situations."},{"from":915.79,"to":918.97,"location":2,"content":"So the first situation is maybe there's no dependency between"},{"from":918.97,"to":922.16,"location":2,"content":"step T and step T plus N in the data."},{"from":922.16,"to":924.72,"location":2,"content":"So perhaps we're learning on a task where,"},{"from":924.72,"to":927.19,"location":2,"content":"in the task there truly is no collect, uh,"},{"from":927.19,"to":929.08,"location":2,"content":"connection or relationship to be"},{"from":929.08,"to":931.51,"location":2,"content":"learned between what happens on step T and what happens on"},{"from":931.51,"to":933.58,"location":2,"content":"step T plus N. So there truly is nothing to be"},{"from":933.58,"to":935.98,"location":2,"content":"learned and it's actually correct that there should be,"},{"from":935.98,"to":938.8,"location":2,"content":"you know, small gradients with respect to those two things."},{"from":938.8,"to":941.62,"location":2,"content":"But the second possibility is that, yes,"},{"from":941.62,"to":944.97,"location":2,"content":"that is a true connection between those two things in the data and in the task."},{"from":944.97,"to":947.65,"location":2,"content":"And really ideally we should be learning that connection."},{"from":947.65,"to":952.48,"location":2,"content":"Um, but we have the wrong parameters in our model to capture this thing,"},{"from":952.48,"to":954.12,"location":2,"content":"and therefore that is why the,"},{"from":954.12,"to":955.16,"location":2,"content":"the gradients are small."},{"from":955.16,"to":957.34,"location":2,"content":"Because the model doesn't see them as connected."},{"from":957.34,"to":960.94,"location":2,"content":"So we are not learning the true dependency between these two things."},{"from":960.94,"to":963.97,"location":2,"content":"And the problem with the vanishing gradient problem is that it's,"},{"from":963.97,"to":965.74,"location":2,"content":"we're unable to tell in this situation,"},{"from":965.74,"to":967.79,"location":2,"content":"which of these two situations we're in."},{"from":967.79,"to":969.82,"location":2,"content":"Okay. So this is all pretty theoretical."},{"from":969.82,"to":971.37,"location":2,"content":"I think this example should make it a little more,"},{"from":971.37,"to":974.59,"location":2,"content":"more clear why the vanishing gradient problem is bad."},{"from":974.59,"to":977.7,"location":2,"content":"So, uh, last week we learned about RNN-Language Models."},{"from":977.7,"to":980.68,"location":2,"content":"And if you remember Language Modeling is a task where you have some kind of"},{"from":980.68,"to":984.01,"location":2,"content":"text and then you're trying to predict what word should come next."},{"from":984.01,"to":985.63,"location":2,"content":"So, uh, here's a piece of text."},{"from":985.63,"to":988.42,"location":2,"content":"It says, um, ''When she tried to print her tickets,"},{"from":988.42,"to":990.31,"location":2,"content":"she found that the printer was out of toner."},{"from":990.31,"to":992.66,"location":2,"content":"She went to the stationery store to buy more toner."},{"from":992.66,"to":994.15,"location":2,"content":"It was very overpriced."},{"from":994.15,"to":996.12,"location":2,"content":"After installing the toner into the printer,"},{"from":996.12,"to":998.11,"location":2,"content":"she finally printed her,'' and"},{"from":998.11,"to":1000.52,"location":2,"content":"can someone shout out what word you think should come next?"},{"from":1000.52,"to":1001.39,"location":2,"content":"Tickets."},{"from":1001.39,"to":1002.79,"location":2,"content":"Tickets. Yes, exactly."},{"from":1002.79,"to":1004.92,"location":2,"content":"So that was easy for you to do because, uh,"},{"from":1004.92,"to":1007.27,"location":2,"content":"it makes sense logically that if that was the thing she was trying to do,"},{"from":1007.27,"to":1011.05,"location":2,"content":"that's the thing she's gonna do once she's gone the whole detour for the, for the toner."},{"from":1011.05,"to":1013.83,"location":2,"content":"Um, so the question is,"},{"from":1013.83,"to":1017.4,"location":2,"content":"can RNN-Language Models easily answer this question."},{"from":1017.4,"to":1020.43,"location":2,"content":"Would they do well at this particular Language Modeling example?"},{"from":1020.43,"to":1024.06,"location":2,"content":"So for an RNN-Language Model to do well at this kind of example,"},{"from":1024.06,"to":1027.36,"location":2,"content":"then they need to learn from this kind of example in the Training Data."},{"from":1027.36,"to":1029.74,"location":2,"content":"So if it solves the example in the Training Data,"},{"from":1029.74,"to":1032.47,"location":2,"content":"then the RNN-Language Model will need to model the dependency."},{"from":1032.47,"to":1034.56,"location":2,"content":"Learn the connection between the appearance of"},{"from":1034.56,"to":1037.07,"location":2,"content":"the word tickets early on on the 7th step,"},{"from":1037.07,"to":1040.2,"location":2,"content":"and the target word tickets at the end."},{"from":1040.2,"to":1042.94,"location":2,"content":"But if we have the vanishing gradient problem,"},{"from":1042.94,"to":1045.86,"location":2,"content":"then these gradients, uh, if they know the step,"},{"from":1045.86,"to":1047.95,"location":2,"content":"the, the last step with respect to the early step,"},{"from":1047.95,"to":1049.38,"location":2,"content":"it's gonna be very small because it's,"},{"from":1049.38,"to":1051.12,"location":2,"content":"it's a fairly long distance, right?"},{"from":1051.12,"to":1053.31,"location":2,"content":"And this means that the model is going to be unable to"},{"from":1053.31,"to":1056.04,"location":2,"content":"learn this dependency, easily or at all."},{"from":1056.04,"to":1059.34,"location":2,"content":"So if the model can't learn this kind of dependency during training,"},{"from":1059.34,"to":1061.59,"location":2,"content":"then the model is going to be unable to predict"},{"from":1061.59,"to":1064.85,"location":2,"content":"similar kinds of long distance dependencies at test-time."},{"from":1064.85,"to":1067.55,"location":2,"content":"Okay, here's another example."},{"from":1067.55,"to":1070.2,"location":2,"content":"Um, here's a piece of text."},{"from":1070.2,"to":1072.78,"location":2,"content":"Uh, the text says and this isn't a full sentence."},{"from":1072.78,"to":1074.16,"location":2,"content":"This is just a partial sentence."},{"from":1074.16,"to":1076.93,"location":2,"content":"It says, the writer of the books, blank."},{"from":1076.93,"to":1078.57,"location":2,"content":"And I'm gonna give you two options."},{"from":1078.57,"to":1083.33,"location":2,"content":"It's either, the writer of the books is or the writer of the books are."},{"from":1083.33,"to":1087.48,"location":2,"content":"So, uh, again shout out which one do you think it is, is or are?"},{"from":1087.48,"to":1088.2,"location":2,"content":"Is."},{"from":1088.2,"to":1090.93,"location":2,"content":"Is, that's right. So, uh, the correct answer,"},{"from":1090.93,"to":1093.43,"location":2,"content":"a correct possible continuation of the sentence would be,"},{"from":1093.43,"to":1095.7,"location":2,"content":"uh, the writer of the books is planning a sequel."},{"from":1095.7,"to":1098.91,"location":2,"content":"I can't think of a continuation that goes the writer of the books are,"},{"from":1098.91,"to":1101.91,"location":2,"content":"that would be, uh, grammatically correct."},{"from":1101.91,"to":1104.65,"location":2,"content":"So the reason why I'm bringing up this example,"},{"from":1104.65,"to":1107.49,"location":2,"content":"is because this shows a kind of tension between, uh,"},{"from":1107.49,"to":1108.6,"location":2,"content":"two things called, uh,"},{"from":1108.6,"to":1112.72,"location":2,"content":"syntactic recency and sem- uh, sequential recency."},{"from":1112.72,"to":1116.79,"location":2,"content":"So syntactic recency is the idea that in"},{"from":1116.79,"to":1120.78,"location":2,"content":"order to correctly predict the next word should be more is than are,"},{"from":1120.78,"to":1125.37,"location":2,"content":"is that the word writer is the kind of syntactically close word here."},{"from":1125.37,"to":1128.64,"location":2,"content":"So we say the writer of the books is because it's the writer is."},{"from":1128.64,"to":1131.31,"location":2,"content":"So you can see this as the word writer and is,"},{"from":1131.31,"to":1133.42,"location":2,"content":"are, uh, syntactically close."},{"from":1133.42,"to":1135.97,"location":2,"content":"Because if you looked at the dependency paths for example,"},{"from":1135.97,"to":1139.05,"location":2,"content":"then there would be a short path in that tree."},{"from":1139.05,"to":1145.15,"location":2,"content":"So by contrast, se- sequential recency is the,"},{"from":1145.15,"to":1150.96,"location":2,"content":"uh, simpler concepts of how close words are just in the sentence as a sequence of words."},{"from":1150.96,"to":1152.53,"location":2,"content":"So in this example,"},{"from":1152.53,"to":1155.87,"location":2,"content":"books and are, are very sequentially recent because they're right next to each other."},{"from":1155.87,"to":1158.47,"location":2,"content":"So the reason I'm bringing this up is because,"},{"from":1158.47,"to":1162.66,"location":2,"content":"the second one would be incorrect but it's kind of a tempting option."},{"from":1162.66,"to":1166.59,"location":2,"content":"Because if you're mostly only paying attention to things that happened recently,"},{"from":1166.59,"to":1169.15,"location":2,"content":"um, then you might get distracted and think,"},{"from":1169.15,"to":1171.09,"location":2,"content":"\"Oh, the books are, that sounds right.\""},{"from":1171.09,"to":1175.5,"location":2,"content":"So the problem here is that RNN-Language Models"},{"from":1175.5,"to":1181.17,"location":2,"content":"are better at learning from sequential recency than sicta- syntactic recency."},{"from":1181.17,"to":1182.65,"location":2,"content":"And this is partially due,"},{"from":1182.65,"to":1184.45,"location":2,"content":"due to the vanishing gradient problem."},{"from":1184.45,"to":1187.29,"location":2,"content":"Because especially perhaps, if your syntactically,"},{"from":1187.29,"to":1189.77,"location":2,"content":"uh, related word is actually kind of far away,"},{"from":1189.77,"to":1194.36,"location":2,"content":"then it might get really hard to use the information from the syntactically recent word,"},{"from":1194.36,"to":1198.39,"location":2,"content":"especially if there's a lot of strong signal from the sequentially recent word."},{"from":1198.39,"to":1203.52,"location":2,"content":"So, uh, there are some papers that show that RNN-Language Models make this kind of error,"},{"from":1203.52,"to":1205.2,"location":2,"content":"of saying are, rather than is."},{"from":1205.2,"to":1208.44,"location":2,"content":"Uh, they make this kind of error more often than you would like, uh,"},{"from":1208.44,"to":1211.86,"location":2,"content":"especially if you have multiple of these distracting words such as books, uh,"},{"from":1211.86,"to":1214.38,"location":2,"content":"in between, uh, the word you're trying to predict"},{"from":1214.38,"to":1219.47,"location":2,"content":"and the true word that you should be, uh, referring to."},{"from":1219.47,"to":1227.49,"location":2,"content":"Okay, any questions on this? All right, moving on."},{"from":1227.49,"to":1231.78,"location":2,"content":"So, we briefly mentioned that exploding gradients, uh, is a problem."},{"from":1231.78,"to":1234.96,"location":2,"content":"So, I'm briefly going to justify why is exploding gradients a problem,"},{"from":1234.96,"to":1236.97,"location":2,"content":"and why does it, uh, what does it look like?"},{"from":1236.97,"to":1240.02,"location":2,"content":"[NOISE] So, the reason why exploding gradients are a problem,"},{"from":1240.02,"to":1242.46,"location":2,"content":"is if you remember this is how SGD works."},{"from":1242.46,"to":1244.86,"location":2,"content":"Uh, we say that the new parameters of the model,"},{"from":1244.86,"to":1246.49,"location":2,"content":"which we represent by Theta,"},{"from":1246.49,"to":1248.43,"location":2,"content":"is equal to the old premises,"},{"from":1248.43,"to":1250.62,"location":2,"content":"and then you take some step in the direction of"},{"from":1250.62,"to":1254.04,"location":2,"content":"negative gradients because you're trying to minimize the loss of J."},{"from":1254.04,"to":1258.05,"location":2,"content":"So, the problem is if your gradient gets really big, uh,"},{"from":1258.05,"to":1261.89,"location":2,"content":"then your SGD update step is going to become really big too."},{"from":1261.89,"to":1263.69,"location":2,"content":"So, you're going to be taking a very big step,"},{"from":1263.69,"to":1267.17,"location":2,"content":"and you're going to be drastically changing your model parameters, Theta."},{"from":1267.17,"to":1270.7,"location":2,"content":"And this means that you can end up with some bad updates."},{"from":1270.7,"to":1273.08,"location":2,"content":"We end up taking too large a step."},{"from":1273.08,"to":1275.58,"location":2,"content":"And we're changing the parameters too much."},{"from":1275.58,"to":1276.78,"location":2,"content":"And this means that, uh,"},{"from":1276.78,"to":1278.14,"location":2,"content":"we kind of take a big step,"},{"from":1278.14,"to":1279.84,"location":2,"content":"and we end up in some, uh,"},{"from":1279.84,"to":1281.94,"location":2,"content":"area where the parameters are actually very bad."},{"from":1281.94,"to":1285.45,"location":2,"content":"Uh, with example the- for example,"},{"from":1285.45,"to":1287.81,"location":2,"content":"they might have a much larger loss than they had before."},{"from":1287.81,"to":1289.86,"location":2,"content":"So, in the worst case,"},{"from":1289.86,"to":1292.76,"location":2,"content":"this can often manifest as seeing, uh,"},{"from":1292.76,"to":1297.92,"location":2,"content":"infinities or NaNs, not a number in your network when you're training it in practice."},{"from":1297.92,"to":1301.48,"location":2,"content":"[NOISE] So, this can happen because if you take such a big step"},{"from":1301.48,"to":1305.44,"location":2,"content":"that maybe you update your parameters so much that now they're infinity,"},{"from":1305.44,"to":1307.29,"location":2,"content":"or minus infinity, something like that,"},{"from":1307.29,"to":1310.24,"location":2,"content":"then you're gonna have all of these infinities within your activations as well,"},{"from":1310.24,"to":1312.19,"location":2,"content":"and then all of your losses are going to be infinity,"},{"from":1312.19,"to":1314.38,"location":2,"content":"and the whole thing just isn't going to work, at all."},{"from":1314.38,"to":1316.17,"location":2,"content":"So, it's very annoying when this happens,"},{"from":1316.17,"to":1318.46,"location":2,"content":"and unfortunately it happens, uh, fairly often."},{"from":1318.46,"to":1320.36,"location":2,"content":"And if it does then you have to essentially"},{"from":1320.36,"to":1322.59,"location":2,"content":"restart training from some earlier checkpoint before you"},{"from":1322.59,"to":1324.48,"location":2,"content":"got the NaNs and the infinities because there's"},{"from":1324.48,"to":1326.58,"location":2,"content":"no kind of salvaging it from its new state."},{"from":1326.58,"to":1330.9,"location":2,"content":"[NOISE] So, what's the solution to this exploding gradient problem?"},{"from":1330.9,"to":1333.3,"location":2,"content":"[NOISE] Uh, the solution is actually pretty"},{"from":1333.3,"to":1336.32,"location":2,"content":"simple and it's this technique called gradient clipping."},{"from":1336.32,"to":1338.58,"location":2,"content":"So, the main idea of gradient clipping,"},{"from":1338.58,"to":1341.61,"location":2,"content":"[NOISE] is that if the norm of your gradient is"},{"from":1341.61,"to":1345.6,"location":2,"content":"greater than some threshold and the threshold is a hyperparameter that you choose."},{"from":1345.6,"to":1349.11,"location":2,"content":"uh, then you want to scale down that gradient,"},{"from":1349.11,"to":1352.04,"location":2,"content":"um, before you apply the SGD update."},{"from":1352.04,"to":1355.41,"location":2,"content":"So, the intuition is yo- you're still gonna take a step in the same direction."},{"from":1355.41,"to":1357.03,"location":2,"content":"But you're gonna make sure that it's a smaller step."},{"from":1357.03,"to":1358.95,"location":2,"content":"[NOISE] So, here, um,"},{"from":1358.95,"to":1361.99,"location":2,"content":"I've got a screenshot of some pseudocode from, uh,"},{"from":1361.99,"to":1363.38,"location":2,"content":"the related paper that, uh,"},{"from":1363.38,"to":1366.18,"location":2,"content":"proposed gradient clipping, or at least some version of gradient clipping."},{"from":1366.18,"to":1368.64,"location":2,"content":"[NOISE] And, um, it's pretty simple as you can see."},{"from":1368.64,"to":1371.31,"location":2,"content":"Uh, g hat is the vector which is the, uh,"},{"from":1371.31,"to":1374.47,"location":2,"content":"derivative of the error with respect to the premises,"},{"from":1374.47,"to":1376.77,"location":2,"content":"and it's saying that if the norm of"},{"from":1376.77,"to":1380.07,"location":2,"content":"this gradient is greater than the threshold's, then you just scale it down."},{"from":1380.07,"to":1383.43,"location":2,"content":"But the important thing to note is that it's still pointing in the same direction,"},{"from":1383.43,"to":1386.42,"location":2,"content":"it's just a smaller step."},{"from":1386.42,"to":1390.62,"location":2,"content":"So, here's a picture to show how that might work out in practice."},{"from":1390.62,"to":1393.11,"location":2,"content":"And, uh, this is a diagram from the, uh,"},{"from":1393.11,"to":1396.26,"location":2,"content":"deep learning textbook which is also linked on [NOISE] the website."},{"from":1396.26,"to":1399.62,"location":2,"content":"So, what's going on here, is that, uh,"},{"from":1399.62,"to":1403.2,"location":2,"content":"the picture here is the loss surface of a simple RNN."},{"from":1403.2,"to":1407.25,"location":2,"content":"So, they made a very simple RNN that instead of having, uh,"},{"from":1407.25,"to":1409.28,"location":2,"content":"a sequence of vectors as the hidden states,"},{"from":1409.28,"to":1412.55,"location":2,"content":"it just suppose that each hidden state is simply just a single scalar."},{"from":1412.55,"to":1415.02,"location":2,"content":"So, this means that instead of having a weight matrix, w,"},{"from":1415.02,"to":1416.49,"location":2,"content":"and the bias vector, b,"},{"from":1416.49,"to":1418.61,"location":2,"content":"you have a scalar w and a scalar b."},{"from":1418.61,"to":1422.55,"location":2,"content":"So, that's why in the picture, you just have this like two-dimensional parameter space."},{"from":1422.55,"to":1425.81,"location":2,"content":"And then the, the z-axis is your, is your loss."},{"from":1425.81,"to":1427.52,"location":2,"content":"So here, high loss is,"},{"from":1427.52,"to":1430.34,"location":2,"content":"is bad and low loss is good in what you're trying to get."},{"from":1430.34,"to":1432.83,"location":2,"content":"So, uh, here in this picture,"},{"from":1432.83,"to":1436.89,"location":2,"content":"you've got this kind of cliff, right, where you have this very steep cliff face,"},{"from":1436.89,"to":1439.29,"location":2,"content":"uh, where the loss changes very quickly."},{"from":1439.29,"to":1443.71,"location":2,"content":"[NOISE] And this cliff is really dangerous because it has steep, steep gradients."},{"from":1443.71,"to":1446.16,"location":2,"content":"And you might be in danger of taking a really big,"},{"from":1446.16,"to":1449.97,"location":2,"content":"[NOISE] uh, update step because you're on the area with a really steep gradient."},{"from":1449.97,"to":1452.51,"location":2,"content":"[NOISE] So, on the left,"},{"from":1452.51,"to":1457.41,"location":2,"content":"you've got a possible scenario of what might happen if you don't have gradient clipping."},{"from":1457.41,"to":1459.33,"location":2,"content":"[NOISE] So, on the left, uh,"},{"from":1459.33,"to":1462.62,"location":2,"content":"you can see that you start kind of at the bottom of the cliff,"},{"from":1462.62,"to":1465.46,"location":2,"content":"and you have a f- a si- a few small updates."},{"from":1465.46,"to":1468.15,"location":2,"content":"And then, in particular makes a bad update because you"},{"from":1468.15,"to":1470.76,"location":2,"content":"see there's a small kind of dip before it goes off the cliff."},{"from":1470.76,"to":1472.59,"location":2,"content":"So, th- the true local minimum,"},{"from":1472.59,"to":1476.14,"location":2,"content":"the optimal you're trying to get to is that the bottom of that small kind of ditch."},{"from":1476.14,"to":1480.38,"location":2,"content":"And, um, it starts off kind of near the edge of that ditch,"},{"from":1480.38,"to":1482.67,"location":2,"content":"and then there's a negative gradient going into it."},{"from":1482.67,"to":1485.78,"location":2,"content":"But unfortunately, the, the update kind of overshoots,"},{"from":1485.78,"to":1487.79,"location":2,"content":"and it ends up going a long way off the cliff."},{"from":1487.79,"to":1490.47,"location":2,"content":"So now, it's in this bad situation where it's taken a bad update,"},{"from":1490.47,"to":1492.93,"location":2,"content":"and now it's got a much bigger loss than it had [NOISE] before."},{"from":1492.93,"to":1494.65,"location":2,"content":"So now that it's on the cliff."},{"from":1494.65,"to":1496.48,"location":2,"content":"Again it, it measures the gradient,"},{"from":1496.48,"to":1498.18,"location":2,"content":"and the gradient is very steep, right?"},{"from":1498.18,"to":1499.42,"location":2,"content":"The gradient is very large."},{"from":1499.42,"to":1501.36,"location":2,"content":"So, when it takes a, uh,"},{"from":1501.36,"to":1503.12,"location":2,"content":"update with respect to that gradient,"},{"from":1503.12,"to":1504.3,"location":2,"content":"then because the gradient is so big,"},{"from":1504.3,"to":1505.9,"location":2,"content":"it takes a really huge step."},{"from":1505.9,"to":1508.02,"location":2,"content":"And that's, um, the, the one to the right."},{"from":1508.02,"to":1509.58,"location":2,"content":"You can see the step going to the right."},{"from":1509.58,"to":1512.19,"location":2,"content":"So, that's also a very bad update because it's just throwing"},{"from":1512.19,"to":1515.31,"location":2,"content":"it really far to some probably fairly random,"},{"from":1515.31,"to":1517.63,"location":2,"content":"uh, configuration of w and b."},{"from":1517.63,"to":1520.74,"location":2,"content":"So, on the left, you can see what can go wrong if you're taking"},{"from":1520.74,"to":1524.83,"location":2,"content":"these really big steps because you were in areas with a very steep gradient."},{"from":1524.83,"to":1526.52,"location":2,"content":"So, by contrast on the right,"},{"from":1526.52,"to":1529.61,"location":2,"content":"you can see what might happen if you do have a gradient clipping."},{"from":1529.61,"to":1532.55,"location":2,"content":"[NOISE] [NOISE] And, um, it's much less drastic, right?"},{"from":1532.55,"to":1535.78,"location":2,"content":"You've got a similar kind of pattern where it takes a few steps into the ditch,"},{"from":1535.78,"to":1537.74,"location":2,"content":"and then ends up going off the cliff a little bit,"},{"from":1537.74,"to":1539.84,"location":2,"content":"but not too much because the gradient was clipped."},{"from":1539.84,"to":1542.4,"location":2,"content":"And then, it's on the cliff and there's again a really steep gradient,"},{"from":1542.4,"to":1545.49,"location":2,"content":"but it doesn't take such a big step because again the gradient was clipped,"},{"from":1545.49,"to":1547.1,"location":2,"content":"so that it kind of comes back down."},{"from":1547.1,"to":1551.09,"location":2,"content":"So, you can see that plausibly by using this gradient clipping method,"},{"from":1551.09,"to":1553.21,"location":2,"content":"you've got a, a kind of safer update rule,"},{"from":1553.21,"to":1554.31,"location":2,"content":"where you're not gonna take any,"},{"from":1554.31,"to":1557.36,"location":2,"content":"any big crazy steps and you're more likely to kind of find the,"},{"from":1557.36,"to":1559.59,"location":2,"content":"the true minimum which is at the bottom of the ditch."},{"from":1559.59,"to":1562.24,"location":2,"content":"[NOISE] I think there was a question earlier."},{"from":1562.24,"to":1563.94,"location":2,"content":"Was there a question over here? [NOISE]"},{"from":1563.94,"to":1565.38,"location":2,"content":"I just want to see the value. [NOISE] [NOISE]"},{"from":1565.38,"to":1567.87,"location":2,"content":"Okay. Anyone else?"},{"from":1567.87,"to":1569.46,"location":2,"content":"[NOISE]"},{"from":1569.46,"to":1581.19,"location":2,"content":"Yeah?"},{"from":1581.19,"to":1581.46,"location":2,"content":"[NOISE] [inaudible]"},{"from":1581.46,"to":1583.01,"location":2,"content":"So, the question is, in assignment three,"},{"from":1583.01,"to":1586.32,"location":2,"content":"y- you saw the atom optimization algorithm which, uh,"},{"from":1586.32,"to":1587.92,"location":2,"content":"has this thing called momentum,"},{"from":1587.92,"to":1591.02,"location":2,"content":"which essentially says that kind of like physical momentum in,"},{"from":1591.02,"to":1595.76,"location":2,"content":"in the real world, that if you've been traveling in the same direction for a while,"},{"from":1595.76,"to":1599.19,"location":2,"content":"then you can take bigger steps,"},{"from":1599.19,"to":1601.32,"location":2,"content":"I think, and if you've recently kind of changed direction,"},{"from":1601.32,"to":1602.84,"location":2,"content":"then you should take smaller steps."},{"from":1602.84,"to":1607.62,"location":2,"content":"And I think there's another element as well, where you divide by some factor."},{"from":1607.62,"to":1609.6,"location":2,"content":"[NOISE] So, it is a similar kind of idea."},{"from":1609.6,"to":1611.19,"location":2,"content":"I suppose it's a different criterion, right?"},{"from":1611.19,"to":1614.43,"location":2,"content":"So, what they both have in common is it's a kind of criterion for when to"},{"from":1614.43,"to":1618.05,"location":2,"content":"scale up or scale down the size of your update step."},{"from":1618.05,"to":1620.19,"location":2,"content":"Um, and I think they're based on different notions"},{"from":1620.19,"to":1623.16,"location":2,"content":"of when should you take bigger steps and when should you take smaller steps."},{"from":1623.16,"to":1625.02,"location":2,"content":"When should you be cautious or less cautious?"},{"from":1625.02,"to":1627.51,"location":2,"content":"So, I guess here the criterion is different."},{"from":1627.51,"to":1629.88,"location":2,"content":"It's kind of a simple criterion saying, like if it's really steep,"},{"from":1629.88,"to":1646.73,"location":2,"content":"then be careful. Yeah. Another question?"},{"from":1646.73,"to":1651.88,"location":2,"content":"Uh, so the [inaudible]. [NOISE]"},{"from":1651.88,"to":1654.06,"location":2,"content":"Okay. So the question is,"},{"from":1654.06,"to":1657.15,"location":2,"content":"is this similar to regularization of some kind, right?"},{"from":1657.15,"to":1659.46,"location":2,"content":"So, I suppose, yeah, there is- there are some things in common."},{"from":1659.46,"to":1663.69,"location":2,"content":"Say for, example, L2 regularization says that you want, for example,"},{"from":1663.69,"to":1669.4,"location":2,"content":"your weight matrices to have a small L2 norm, right?"},{"from":1669.4,"to":1671.34,"location":2,"content":"And the idea is that you're trying to prevent"},{"from":1671.34,"to":1673.92,"location":2,"content":"your model from over-fitting the data by, um,"},{"from":1673.92,"to":1677.19,"location":2,"content":"having some kind of constraint that says you have to keep your weights fairly simple,"},{"from":1677.19,"to":1679.14,"location":2,"content":"that is keep them, you know, small."},{"from":1679.14,"to":1681.39,"location":2,"content":"So, I suppose the relationship is that here we're"},{"from":1681.39,"to":1683.49,"location":2,"content":"saying that we don't want the norm of the gradients to be too big."},{"from":1683.49,"to":1687.5,"location":2,"content":"Ah, I don't know if this is related to overfitting."},{"from":1687.5,"to":1689.49,"location":2,"content":"Um, I guess I have to think more carefully about that,"},{"from":1689.49,"to":1694.17,"location":2,"content":"but I guess it's a similar kind of constraint that you're placing."},{"from":1694.17,"to":1696.44,"location":2,"content":"Okay. I'm gonna move on for now."},{"from":1696.44,"to":1699.66,"location":2,"content":"Uh, so we've talked"},{"from":1699.66,"to":1703.08,"location":2,"content":"about how you might fix the exploding gradient problem with gradient clipping,"},{"from":1703.08,"to":1706.74,"location":2,"content":"but we haven't talked about how we might fix the vanishing gradient problem."},{"from":1706.74,"to":1709.41,"location":2,"content":"So, um, to recap,"},{"from":1709.41,"to":1714.27,"location":2,"content":"I think one way to characterize the problem with the- the vanishing gradients in RNNs is"},{"from":1714.27,"to":1719.62,"location":2,"content":"that it's too difficult for the RNN to learn to preserve information over many timesteps."},{"from":1719.62,"to":1721.34,"location":2,"content":"So, in our example with printing"},{"from":1721.34,"to":1724.47,"location":2,"content":"the tickets and re- remembering that it's the tickets that she wants to print,"},{"from":1724.47,"to":1728.42,"location":2,"content":"you could think of it as it's hard for the RNN language model to correctly"},{"from":1728.42,"to":1732.26,"location":2,"content":"predict tickets because in a way, it's too hard for the RNN language model to,"},{"from":1732.26,"to":1736.35,"location":2,"content":"uh, learn to retain the tickets information and use it later."},{"from":1736.35,"to":1738.9,"location":2,"content":"So, um, if you look at the equation"},{"from":1738.9,"to":1741.63,"location":2,"content":"for vanilla RNNs and how we compute the hidden state, uh,"},{"from":1741.63,"to":1743.95,"location":2,"content":"based on the previous hidden state and- and the inputs,"},{"from":1743.95,"to":1747.49,"location":2,"content":"you can see that the hidden state is in a way constantly being rewritten."},{"from":1747.49,"to":1749.91,"location":2,"content":"It's always computed based on these, uh,"},{"from":1749.91,"to":1751.65,"location":2,"content":"linear transformations and the,"},{"from":1751.65,"to":1753.11,"location":2,"content":"you know, the non-linearity."},{"from":1753.11,"to":1755.01,"location":2,"content":"So, it's not all that easy to"},{"from":1755.01,"to":1757.96,"location":2,"content":"preserve the information from one hidden state to the other,"},{"from":1757.96,"to":1761.14,"location":2,"content":"in particular, because we are putting it through this non-linearity function."},{"from":1761.14,"to":1766.98,"location":2,"content":"So, this motivates us to ask what about an RNN with some kind of separate memory?"},{"from":1766.98,"to":1771.51,"location":2,"content":"If we have some kind of separate place to store information that we want to use later,"},{"from":1771.51,"to":1774.63,"location":2,"content":"then would this make it easier for our RNN"},{"from":1774.63,"to":1778.29,"location":2,"content":"to learn to preserve information over many timesteps?"},{"from":1778.29,"to":1785.2,"location":2,"content":"So, this is the motivating idea behind LSTMs or Long Short-Term Memory RNNs."},{"from":1785.2,"to":1791.55,"location":2,"content":"So, the idea here is that an LSTM is a type of RNN and it was proposed back in, uh, 1997."},{"from":1791.55,"to":1793.34,"location":2,"content":"And the idea is that this is, uh,"},{"from":1793.34,"to":1796.8,"location":2,"content":"this was proposed as an explicit solution to the vanishing gradients problem."},{"from":1796.8,"to":1800.28,"location":2,"content":"[NOISE] So, one of the main differences here is"},{"from":1800.28,"to":1803.88,"location":2,"content":"that on each step T instead of just having a hidden state h_t,"},{"from":1803.88,"to":1808.32,"location":2,"content":"we have both the hidden state h_t and the cell state which we denote c_t."},{"from":1808.32,"to":1812.09,"location":2,"content":"And both of these are vectors of some same length,"},{"from":1812.09,"to":1815.43,"location":2,"content":"n, and the idea there is that the cell is meant to"},{"from":1815.43,"to":1820.11,"location":2,"content":"sto- store our long-term information that, that's on memory units."},{"from":1820.11,"to":1823.61,"location":2,"content":"Another super important thing is that the LSTM can"},{"from":1823.61,"to":1826.98,"location":2,"content":"erase and write [NOISE] and read information from the cell."},{"from":1826.98,"to":1829.99,"location":2,"content":"So, you kind of think of this a bit like memory in a computer,"},{"from":1829.99,"to":1833.71,"location":2,"content":"in that you can do these operations, reading and writing and erasing,"},{"from":1833.71,"to":1837.09,"location":2,"content":"um, and that's how you're gonna keep your information."},{"from":1837.09,"to":1839.22,"location":2,"content":"[NOISE]."},{"from":1839.22,"to":1843.25,"location":2,"content":"Another super important thing is that the way the LSTM decides,"},{"from":1843.25,"to":1845.49,"location":2,"content":"whether it wants to erase, write, read,"},{"from":1845.49,"to":1848.57,"location":2,"content":"information and decide how much and which information,"},{"from":1848.57,"to":1851.43,"location":2,"content":"uh, that's all controlled by these [NOISE] gates."},{"from":1851.43,"to":1856.56,"location":2,"content":"So, the idea is [NOISE] that the gates are themselves also vectors of length n,"},{"from":1856.56,"to":1859.59,"location":2,"content":"and the idea there is that on each timestep,"},{"from":1859.59,"to":1864.96,"location":2,"content":"each element of these gates which are vectors are somewhere between zero and one."},{"from":1864.96,"to":1870.16,"location":2,"content":"So here, uh, one represents an open gate and zero represents a closed gate,"},{"from":1870.16,"to":1872.71,"location":2,"content":"and you can have values anywhere in between."},{"from":1872.71,"to":1875.47,"location":2,"content":"So, the overall idea, which we're gonna firm up on the next slide,"},{"from":1875.47,"to":1877.77,"location":2,"content":"but the overall idea is that if the gate is open,"},{"from":1877.77,"to":1880.59,"location":2,"content":"that represents some kind of information being passed through,"},{"from":1880.59,"to":1881.67,"location":2,"content":"and if the gate is closed,"},{"from":1881.67,"to":1884.36,"location":2,"content":"it [NOISE] means that information does not pass through."},{"from":1884.36,"to":1888.7,"location":2,"content":"Okay. So, the last really important thing is that the gates are dynamic."},{"from":1888.7,"to":1892.95,"location":2,"content":"They're not just set at some constant value for the whole sequence."},{"from":1892.95,"to":1894.33,"location":2,"content":"[NOISE] Um, they're dynamic,"},{"from":1894.33,"to":1896.79,"location":2,"content":"which means that they're different on each timestep T,"},{"from":1896.79,"to":1901.2,"location":2,"content":"and the value that is the decision of whether they're open or closed and in which ways,"},{"from":1901.2,"to":1905.1,"location":2,"content":"[NOISE] um, that is computed based on the current context."},{"from":1905.1,"to":1906.94,"location":2,"content":"Okay. So here's, um,"},{"from":1906.94,"to":1910.13,"location":2,"content":"here's the- the equations for the LSTM which might make it clearer."},{"from":1910.13,"to":1914.16,"location":2,"content":"So, uh, suppose we have some sequence of i- inputs x_t and we"},{"from":1914.16,"to":1918.43,"location":2,"content":"want to compute a sequence of hidden state h_t and cell states c_t."},{"from":1918.43,"to":1922.74,"location":2,"content":"So, this is what happens on timestep t. Uh,"},{"from":1922.74,"to":1927.21,"location":2,"content":"this process equation shows you the three gates that I talked about before."},{"from":1927.21,"to":1929.91,"location":2,"content":"So, the first one is called the Forget Gates."},{"from":1929.91,"to":1934.55,"location":2,"content":"And the idea is that this one is controlling what is kept versus what is forgotten,"},{"from":1934.55,"to":1938.13,"location":2,"content":"um, from the previous cell state, the previous memory."},{"from":1938.13,"to":1942.51,"location":2,"content":"And you can see that this forget gate is computed based on, uh,"},{"from":1942.51,"to":1946.46,"location":2,"content":"the previous hidden state h_t minus one and the current input x_t."},{"from":1946.46,"to":1949.11,"location":2,"content":"Um, so that's what I meant when I said that it's"},{"from":1949.11,"to":1951.93,"location":2,"content":"dynamic and it's computed based on the- the current context."},{"from":1951.93,"to":1956.19,"location":2,"content":"[NOISE] Um, you can also see that it's computed using,"},{"from":1956.19,"to":1957.39,"location":2,"content":"uh, the sigmoid function,"},{"from":1957.39,"to":1959.92,"location":2,"content":"which means that it is somewhere between zero and one."},{"from":1959.92,"to":1963.05,"location":2,"content":"Okay. The next gate is called the input gate,"},{"from":1963.05,"to":1968.64,"location":2,"content":"and this one controls what parts of the new cell contents are written to the cell."},{"from":1968.64,"to":1972.02,"location":2,"content":"So, the idea there is that you have this- this memory cell and this is kind of, um,"},{"from":1972.02,"to":1977.23,"location":2,"content":"controlling like ho- how and what you get to write to the memory cell."},{"from":1977.23,"to":1979.99,"location":2,"content":"Okay. And the last one is called the upper gate."},{"from":1979.99,"to":1981.78,"location":2,"content":"So, this one is controlling, uh,"},{"from":1981.78,"to":1984.93,"location":2,"content":"what parts of the cell are outputs to the hidden state,"},{"from":1984.93,"to":1988.62,"location":2,"content":"[NOISE] so you could view this as kind of like the read function, right?"},{"from":1988.62,"to":1990.09,"location":2,"content":"We're going to read some information from"},{"from":1990.09,"to":1992.68,"location":2,"content":"our memory cell and that's gonna get put into our hidden states,"},{"from":1992.68,"to":1994.05,"location":2,"content":"and this gate is gonna control that."},{"from":1994.05,"to":1996.99,"location":2,"content":"[NOISE] Okay."},{"from":1996.99,"to":2002.06,"location":2,"content":"[NOISE] Uh, yeah, that's just the sigmoid function as we noted before."},{"from":2002.06,"to":2005.87,"location":2,"content":"All right. So, the next set of equation shows how we use these gates."},{"from":2005.87,"to":2008.12,"location":2,"content":"[NOISE] So, the first line, uh,"},{"from":2008.12,"to":2009.34,"location":2,"content":"you could regard this, uh,"},{"from":2009.34,"to":2012.1,"location":2,"content":"c_tilde as the new [NOISE] cell content."},{"from":2012.1,"to":2014.96,"location":2,"content":"So, uh, this is the new content that you want to write to the cell,"},{"from":2014.96,"to":2017.54,"location":2,"content":"[NOISE] and this is also computed based on, uh,"},{"from":2017.54,"to":2019.49,"location":2,"content":"your previous hidden state and your current inputs,"},{"from":2019.49,"to":2021.92,"location":2,"content":"and this goes through your tan h non-linearity."},{"from":2021.92,"to":2025.25,"location":2,"content":"So, uh, this is kind of the- the main contents that"},{"from":2025.25,"to":2029.57,"location":2,"content":"you are computing based on the context and you want to write this into memory."},{"from":2029.57,"to":2034.55,"location":2,"content":"So, on the next line what's happening is that we're going to use"},{"from":2034.55,"to":2040.07,"location":2,"content":"the forget gate to selectively forget some of the information from the previous,"},{"from":2040.07,"to":2041.93,"location":2,"content":"[NOISE] uh, memory cell."},{"from":2041.93,"to":2044.78,"location":2,"content":"And you can see that we're doing these element-wise products,"},{"from":2044.78,"to":2046.34,"location":2,"content":"that's what the little circle is."},{"from":2046.34,"to":2048.95,"location":2,"content":"So, the idea is that if you remember that f_t is"},{"from":2048.95,"to":2051.98,"location":2,"content":"a vector full of values between zero and one,"},{"from":2051.98,"to":2054.38,"location":2,"content":"when you do an element-wise product between f_t and"},{"from":2054.38,"to":2056.89,"location":2,"content":"the previous cell state c_t minus one,"},{"from":2056.89,"to":2059,"location":2,"content":"then what you're essentially doing is you're kind of masking"},{"from":2059,"to":2061.86,"location":2,"content":"out some of the information from the previous hidden state."},{"from":2061.86,"to":2063.68,"location":2,"content":"Sorry, no. Previous cell state."},{"from":2063.68,"to":2066.16,"location":2,"content":"So, when f is one,"},{"from":2066.16,"to":2067.8,"location":2,"content":"then you're copying over the information,"},{"from":2067.8,"to":2070.34,"location":2,"content":"but when f is zero, then you're getting rid of that information,"},{"from":2070.34,"to":2073.93,"location":2,"content":"you are erasing it or forgetting it."},{"from":2073.93,"to":2077.03,"location":2,"content":"Okay. And then the other half of this equation,"},{"from":2077.03,"to":2079.55,"location":2,"content":"um, i_t times c tilde t, uh,"},{"from":2079.55,"to":2081.5,"location":2,"content":"that's the input gate controlling"},{"from":2081.5,"to":2084.22,"location":2,"content":"which parts of the new cell contents are gonna get written,"},{"from":2084.22,"to":2087.13,"location":2,"content":"written to the, to the cell."},{"from":2087.13,"to":2090.09,"location":2,"content":"Okay. And then the last thing we do is we, uh,"},{"from":2090.09,"to":2092.9,"location":2,"content":"pass the cell through a tan h,"},{"from":2092.9,"to":2095.39,"location":2,"content":"that's just adding another non-linearity,"},{"from":2095.39,"to":2096.53,"location":2,"content":"and then you pass that through"},{"from":2096.53,"to":2099.38,"location":2,"content":"the output gates and that gives you [NOISE] the hidden state."},{"from":2099.38,"to":2102.95,"location":2,"content":"So, in LSTMs, we often think of the hidden states as being,"},{"from":2102.95,"to":2105.09,"location":2,"content":"uh, like the outputs of the RNN."},{"from":2105.09,"to":2107.57,"location":2,"content":"And the reason for this is that you kind of view"},{"from":2107.57,"to":2109.91,"location":2,"content":"the cell states as being this kind of"},{"from":2109.91,"to":2112.99,"location":2,"content":"internal memory that's not generally accessible to the outside,"},{"from":2112.99,"to":2115.28,"location":2,"content":"but the hidden states are the parts that you're"},{"from":2115.28,"to":2117.91,"location":2,"content":"gonna pa- pass on to the next part of the model."},{"from":2117.91,"to":2120.62,"location":2,"content":"So, that's why we view it as kind of like the output of the model."},{"from":2120.62,"to":2124.52,"location":2,"content":"[NOISE] Uh, and this is, yeah,"},{"from":2124.52,"to":2126.41,"location":2,"content":"x just to remind the- there is- circles are"},{"from":2126.41,"to":2129.09,"location":2,"content":"element-wise products and that's how we apply the gates."},{"from":2129.09,"to":2130.91,"location":2,"content":"Uh, did anyone have any questions about this?"},{"from":2130.91,"to":2140.99,"location":2,"content":"[NOISE]."},{"from":2140.99,"to":2143.96,"location":2,"content":"Okay. [NOISE] Um, so as a reminder,"},{"from":2143.96,"to":2146.42,"location":2,"content":"all of these are vectors of some same length n."},{"from":2146.42,"to":2149.51,"location":2,"content":"[NOISE] Okay."},{"from":2149.51,"to":2152.91,"location":2,"content":"So, some people learn better from diagrams than equations,"},{"from":2152.91,"to":2155.66,"location":2,"content":"and here's a diagram presentation of the same idea."},{"from":2155.66,"to":2157.89,"location":2,"content":"So, this is a really nice diagram from a blog post,"},{"from":2157.89,"to":2159.66,"location":2,"content":"uh, by Chris Olah about LSTMs,"},{"from":2159.66,"to":2161.42,"location":2,"content":"and that was a good place to start if you want to"},{"from":2161.42,"to":2164.45,"location":2,"content":"get an intuitive understanding of what LSTMs are."},{"from":2164.45,"to":2166.51,"location":2,"content":"So, in this diagram, uh,"},{"from":2166.51,"to":2169.05,"location":2,"content":"the green boxes represent timesteps,"},{"from":2169.05,"to":2172.55,"location":2,"content":"um, and let's zoom in on the middle one and see what's happening here."},{"from":2172.55,"to":2174.86,"location":2,"content":"So, within one timestep,"},{"from":2174.86,"to":2177.65,"location":2,"content":"you can see that this diagram is showing exactly the same thing as"},{"from":2177.65,"to":2180.76,"location":2,"content":"those six equations showed on the previous slide."},{"from":2180.76,"to":2185.81,"location":2,"content":"So, uh, the first thing we do is we use the, uh, the current input x_t,"},{"from":2185.81,"to":2189.35,"location":2,"content":"which is at the bottom and the previous hidden state h_t minus the one on the left,"},{"from":2189.35,"to":2191.42,"location":2,"content":"and we can use that to compute the forget gate."},{"from":2191.42,"to":2194.72,"location":2,"content":"[NOISE] And you can see f_t is on that arrow there."},{"from":2194.72,"to":2199.39,"location":2,"content":"And then you apply the forget gate to the previous, uh, cell,"},{"from":2199.39,"to":2202.97,"location":2,"content":"and that's the same thing as forgetting some of the- the cell content from last time."},{"from":2202.97,"to":2204.7,"location":2,"content":"[NOISE] Okay."},{"from":2204.7,"to":2207.29,"location":2,"content":"And then after that, you can compute the input gate, uh,"},{"from":2207.29,"to":2210.16,"location":2,"content":"and that's computed in much the same way as the forget gate."},{"from":2210.16,"to":2215.24,"location":2,"content":"And then you use the input gate to decide which parts of this,"},{"from":2215.24,"to":2218.76,"location":2,"content":"uh, new cell content get written to the cell,"},{"from":2218.76,"to":2220.57,"location":2,"content":"and that gives you the cell c_t."},{"from":2220.57,"to":2224.24,"location":2,"content":"So, here you can see that you computed the impu ga- input gates and"},{"from":2224.24,"to":2228.77,"location":2,"content":"the new content and then you use that to gate that and write it to the cell."},{"from":2228.77,"to":2230.6,"location":2,"content":"So, now we've got our new cell c_t,"},{"from":2230.6,"to":2235.37,"location":2,"content":"and then the last things we need to do is to compute our new output gate, that's o_t."},{"from":2235.37,"to":2239.78,"location":2,"content":"And then lastly, use the output gate to select which parts of"},{"from":2239.78,"to":2245.09,"location":2,"content":"the cell contents you're gonna read and put in the new hidden state h_t."},{"from":2245.09,"to":2247.31,"location":2,"content":"So, that's, that's, uh, that's"},{"from":2247.31,"to":2252.4,"location":2,"content":"the same thing as the equations we saw on the previous slide."},{"from":2252.4,"to":2255.03,"location":2,"content":"Okay. So, that's LSTMs."},{"from":2255.03,"to":2257.51,"location":2,"content":"Um, is there a question?"},{"from":2257.51,"to":2270.11,"location":2,"content":"What's the importance [NOISE]  [inaudible]"},{"from":2270.11,"to":2272.84,"location":2,"content":"The question is, why are we applying a tan h"},{"from":2272.84,"to":2275.72,"location":2,"content":"on the very last equation on this, on this slide?"},{"from":2275.72,"to":2281.12,"location":2,"content":"Why we're planning a tan h to the cell before applying the output gate?"},{"from":2281.12,"to":2285.8,"location":2,"content":"Let's see. Um."},{"from":2285.8,"to":2290.33,"location":2,"content":"Yeah. So, your question is, the- the cell,"},{"from":2290.33,"to":2299.33,"location":2,"content":"the new cell content already went through a tan h. Um, I'm not sure."},{"from":2299.33,"to":2301.58,"location":2,"content":"So, I suppose a- a- a general answer is that it must"},{"from":2301.58,"to":2303.98,"location":2,"content":"be giving some kind of more expressivity in some way,"},{"from":2303.98,"to":2306.41,"location":2,"content":"and that it's not just applying"},{"from":2306.41,"to":2311.42,"location":2,"content":"tan h's sequentially because you do have the gates in between."},{"from":2311.42,"to":2314.24,"location":2,"content":"Um, so I suppose there must be a reason,"},{"from":2314.24,"to":2316.45,"location":2,"content":"kind of similarly to when you apply- apply"},{"from":2316.45,"to":2319.64,"location":2,"content":"a linear layer you won't have a non-linearity before the next linear layer."},{"from":2319.64,"to":2323.07,"location":2,"content":"I suppose maybe we're viewing these cases as a kind of linear layer?"},{"from":2323.07,"to":2324.92,"location":2,"content":"I'm not sure. I'll look it up."},{"from":2324.92,"to":2329.45,"location":2,"content":"[NOISE] Okay."},{"from":2329.45,"to":2332.07,"location":2,"content":"So, uh, that's LSTMs."},{"from":2332.07,"to":2334.22,"location":2,"content":"And, um, re- if you recall,"},{"from":2334.22,"to":2335.8,"location":2,"content":"we were- oh, question?"},{"from":2335.8,"to":2338.61,"location":2,"content":"Yeah. Why is it that in the forget gate,"},{"from":2338.61,"to":2342.18,"location":2,"content":"you don't look at the previous cell state but you just look at the new hidden state?"},{"from":2342.18,"to":2344.18,"location":2,"content":"Like it seems like if you're this- instead of"},{"from":2344.18,"to":2347.09,"location":2,"content":"deciding what to forget from the cell state, you should look at it."},{"from":2347.09,"to":2349.67,"location":2,"content":"So the question is, why is the forget gate"},{"from":2349.67,"to":2352.61,"location":2,"content":"computed only for the previous hidden state and the current input,"},{"from":2352.61,"to":2356.84,"location":2,"content":"why is it not computed based on ct minus one itself, right?"},{"from":2356.84,"to":2358.82,"location":2,"content":"Because surely you want to look at the thing to figure"},{"from":2358.82,"to":2361.88,"location":2,"content":"out whether you want to forget it or not?"},{"from":2361.88,"to":2364.3,"location":2,"content":"Um, that's a pretty good question."},{"from":2364.3,"to":2369.74,"location":2,"content":"Uh, so, I suppose one reason why you might think that this- this works fine is that"},{"from":2369.74,"to":2373.04,"location":2,"content":"the LSTM might be learning a general algorithm"},{"from":2373.04,"to":2376.73,"location":2,"content":"for where it stores different types of information in the cell, right?"},{"from":2376.73,"to":2379.37,"location":2,"content":"So, maybe it's learning that in this particular position in the cell,"},{"from":2379.37,"to":2384.23,"location":2,"content":"I learn information about this particular semantic thing and then in this situation,"},{"from":2384.23,"to":2388.3,"location":2,"content":"I want to use that or not use that, forget it or keep it."},{"from":2388.3,"to":2391.58,"location":2,"content":"But, yeah, I haven't entirely convinced myself why you don't want to"},{"from":2391.58,"to":2395.09,"location":2,"content":"look at the contents of the cell itself in order to decide."},{"from":2395.09,"to":2402.53,"location":2,"content":"I suppose another thing to notice is that ht minus one was read from ct minus one."},{"from":2402.53,"to":2406.01,"location":2,"content":"So, I suppose there is some information there but not necessarily all of the information."},{"from":2406.01,"to":2411.09,"location":2,"content":"Ah, yeah."},{"from":2411.09,"to":2413.42,"location":2,"content":"I'm not sure, that's another thing I need to look up I guess."},{"from":2413.42,"to":2421.06,"location":2,"content":"[NOISE] Any other questions?"},{"from":2421.06,"to":2427.78,"location":2,"content":"Okay. Ah, so, that's LSTMs and,"},{"from":2427.78,"to":2431.82,"location":2,"content":"um, LSTMs were introduced to try to solve the vanishing gradient problem."},{"from":2431.82,"to":2433.64,"location":2,"content":"So, the question is, ah,"},{"from":2433.64,"to":2437.84,"location":2,"content":"how exactly is this architecture making the vanishing gradient problem any better?"},{"from":2437.84,"to":2441.71,"location":2,"content":"So, you could, ah, see that the LSTM architecture"},{"from":2441.71,"to":2445.93,"location":2,"content":"actually makes it easier for RNNs to preserve information over many time steps."},{"from":2445.93,"to":2448.34,"location":2,"content":"So, while it w as kind of difficult for"},{"from":2448.34,"to":2452.28,"location":2,"content":"the vanilla RNN to preserve the information over all of the hidden states,"},{"from":2452.28,"to":2454.73,"location":2,"content":"there's actually a fairly easy strategy that makes"},{"from":2454.73,"to":2457.26,"location":2,"content":"it simple for the LSTM to preserve the information."},{"from":2457.26,"to":2462.29,"location":2,"content":"So, namely, if the forget gate is set to remember everything on every step, um,"},{"from":2462.29,"to":2465.29,"location":2,"content":"that's a fairly simple strategy that will ensure that"},{"from":2465.29,"to":2469.82,"location":2,"content":"the information in the cell is going to be preserved indefinitely over many time steps."},{"from":2469.82,"to":2473.11,"location":2,"content":"So, I don't know if that's actually a good strategy for whatever task you're trying to do,"},{"from":2473.11,"to":2475.99,"location":2,"content":"but my point is that there is at least, um,"},{"from":2475.99,"to":2480.68,"location":2,"content":"a fairly straightforward way for the LSTM to keep the information over many steps."},{"from":2480.68,"to":2485.18,"location":2,"content":"And as we noted that's relatively harder for the vanilla RNN to do."},{"from":2485.18,"to":2489.64,"location":2,"content":"So, you can think of this as the key reason why LSTMs are more able,"},{"from":2489.64,"to":2491.86,"location":2,"content":"ah, to preserve the information"},{"from":2491.86,"to":2494.82,"location":2,"content":"and thus are more robust to the vanishing gradient problem."},{"from":2494.82,"to":2498.59,"location":2,"content":"Ah, however, I think you should still know that LSTMs don't"},{"from":2498.59,"to":2502.18,"location":2,"content":"necessarily guarantee that we don't have a vanishing or exploding gradient problem."},{"from":2502.18,"to":2503.61,"location":2,"content":"You could still have that problem,"},{"from":2503.61,"to":2508.08,"location":2,"content":"but the thing to remember is that it's easier to avoid it anyway."},{"from":2508.08,"to":2511.42,"location":2,"content":"Okay. So, um, LSTMs, ah,"},{"from":2511.42,"to":2514.82,"location":2,"content":"have been shown to be more robust to the vanishing gradient problem,"},{"from":2514.82,"to":2517.18,"location":2,"content":"ah but I'm going to tell you a little about how they've"},{"from":2517.18,"to":2520.11,"location":2,"content":"actually been more successful in real life. You have a question?"},{"from":2520.11,"to":2541.6,"location":2,"content":"Yeah,  [inaudible]"},{"from":2541.6,"to":2546.11,"location":2,"content":"Okay. So it's a great question."},{"from":2546.11,"to":2548.48,"location":2,"content":"The question is, why is it that just because you"},{"from":2548.48,"to":2551.39,"location":2,"content":"have these LSTM defined forward equations,"},{"from":2551.39,"to":2553.39,"location":2,"content":"why do you not have the vanishing gradient problem?"},{"from":2553.39,"to":2556.28,"location":2,"content":"Why does the- the logic about, ah,"},{"from":2556.28,"to":2560.06,"location":2,"content":"the chain rule kind of getting smaller and smaller or bigger and bigger not apply?"},{"from":2560.06,"to":2563.8,"location":2,"content":"So, I think the key here is that, um,"},{"from":2563.8,"to":2565.72,"location":2,"content":"in the vanilla RNN,"},{"from":2565.72,"to":2567.92,"location":2,"content":"the hidden states are kind of like a bottleneck, right?"},{"from":2567.92,"to":2570.62,"location":2,"content":"Like all gradients must pass through them."},{"from":2570.62,"to":2572.36,"location":2,"content":"So, if that gradient is small then,"},{"from":2572.36,"to":2574.59,"location":2,"content":"all downstream gradients will be small,"},{"from":2574.59,"to":2578.41,"location":2,"content":"whereas here you could regard the cell as being kind of like"},{"from":2578.41,"to":2580.82,"location":2,"content":"a shortcut connection at least in"},{"from":2580.82,"to":2584.3,"location":2,"content":"the case where the forget gate is set to remember things,"},{"from":2584.3,"to":2587.33,"location":2,"content":"um, then that's kind of like a shortcut connection where"},{"from":2587.33,"to":2591.02,"location":2,"content":"the cell will stay the same if you have the forget gate set to remember things."},{"from":2591.02,"to":2594.08,"location":2,"content":"So, if the cell is staying mostly the same,"},{"from":2594.08,"to":2597.53,"location":2,"content":"then you are not going to be,"},{"from":2597.53,"to":2600.2,"location":2,"content":"ah, having the vanishing gradient via the cell."},{"from":2600.2,"to":2602.51,"location":2,"content":"So, that means that to get a connection from"},{"from":2602.51,"to":2605.47,"location":2,"content":"the gradient of something in the future with respect to something in the past,"},{"from":2605.47,"to":2607.76,"location":2,"content":"there is a potential route for the gradient to"},{"from":2607.76,"to":2610.86,"location":2,"content":"go via the cell that doesn't necessarily vanish."},{"from":2610.86,"to":2612.86,"location":2,"content":"So in that, I have one more question."},{"from":2612.86,"to":2613.12,"location":2,"content":"Um-uh."},{"from":2613.12,"to":2629.83,"location":2,"content":"Since we have a shortcut [inaudible]"},{"from":2629.83,"to":2633.68,"location":2,"content":"So I think the question was how do you check that your gradients are correct given that"},{"from":2633.68,"to":2637.89,"location":2,"content":"there are now multiple routes for information to travel?"},{"from":2637.89,"to":2638.45,"location":2,"content":"Right."},{"from":2638.45,"to":2641.78,"location":2,"content":"So, I suppose this somewhat relates to what we talked about last time with"},{"from":2641.78,"to":2644.66,"location":2,"content":"the multivariable chain rule about what is"},{"from":2644.66,"to":2648.82,"location":2,"content":"the derivative of the loss with respect to a repeated weight matrix and we saw that,"},{"from":2648.82,"to":2650.68,"location":2,"content":"if there are multiple routes then"},{"from":2650.68,"to":2653.33,"location":2,"content":"the multivariable chain rule says that you add up the gradients."},{"from":2653.33,"to":2656.68,"location":2,"content":"So, if your question is how do you do the calculus correctly and make sure it's correct,"},{"from":2656.68,"to":2658.19,"location":2,"content":"I guess you just kind of apply"},{"from":2658.19,"to":2659.66,"location":2,"content":"the multi-variable chain rule and it's more"},{"from":2659.66,"to":2661.46,"location":2,"content":"complicated than assessing with the LSTMs."},{"from":2661.46,"to":2664.09,"location":2,"content":"Ah if you're using PyTorch 14 you do not have to do that yourself,"},{"from":2664.09,"to":2665.78,"location":2,"content":"if you're going to implement it yourself then,"},{"from":2665.78,"to":2667.53,"location":2,"content":"you might have a more difficult time."},{"from":2667.53,"to":2670.78,"location":2,"content":"Um, yeah. So, I guess, yeah."},{"from":2670.78,"to":2678.56,"location":2,"content":"Okay. All right, so, what do we get to. All right."},{"from":2678.56,"to":2681.39,"location":2,"content":"So, let's talk about LSTMs and how they work in the- in the real world."},{"from":2681.39,"to":2684.95,"location":2,"content":"So, in the pretty recent past,"},{"from":2684.95,"to":2688.49,"location":2,"content":"2013-2015 um LSTM started achieving a lot of state of"},{"from":2688.49,"to":2692.32,"location":2,"content":"the art results on a variety of different tasks including for example,"},{"from":2692.32,"to":2694.37,"location":2,"content":"handwriting recognition, speech recognition,"},{"from":2694.37,"to":2697.8,"location":2,"content":"machine translation, parsing, image captioning."},{"from":2697.8,"to":2699.38,"location":2,"content":"So, over this period,"},{"from":2699.38,"to":2702.02,"location":2,"content":"LSTMs became the dominant approach in a lot of"},{"from":2702.02,"to":2708.23,"location":2,"content":"these application areas because they worked convincingly a lot better than vanilla RNNs."},{"from":2708.23,"to":2710.7,"location":2,"content":"However, today in 2019,"},{"from":2710.7,"to":2713.47,"location":2,"content":"things changed pretty fast in deep learning."},{"from":2713.47,"to":2716.24,"location":2,"content":"So, other approaches for example,"},{"from":2716.24,"to":2718.4,"location":2,"content":"transformers which you're going to learn about later in the class."},{"from":2718.4,"to":2720.41,"location":2,"content":"Ah, in some of these application areas,"},{"from":2720.41,"to":2721.72,"location":2,"content":"they seem to have become,"},{"from":2721.72,"to":2723.93,"location":2,"content":"ah, the dominant approach."},{"from":2723.93,"to":2725.48,"location":2,"content":"So, to look into this,"},{"from":2725.48,"to":2729.35,"location":2,"content":"I had a look at WMT which is a machine translation conference and"},{"from":2729.35,"to":2733.86,"location":2,"content":"also competition where people submit their MT systems to be evaluated."},{"from":2733.86,"to":2735.62,"location":2,"content":"And I looked at the report,"},{"from":2735.62,"to":2739.76,"location":2,"content":"the summary report for WMT 2016 and in this report,"},{"from":2739.76,"to":2741.13,"location":2,"content":"I did a quick Ctrl+F,"},{"from":2741.13,"to":2744.01,"location":2,"content":"and I found the word RNN appeared 44 times."},{"from":2744.01,"to":2746.99,"location":2,"content":"So, it seems that most people entering this competition were building"},{"from":2746.99,"to":2750.8,"location":2,"content":"their MT systems based on RNNs and in particular LSTMs."},{"from":2750.8,"to":2752.95,"location":2,"content":"And then I looked at the report from 2018,"},{"from":2752.95,"to":2755.21,"location":2,"content":"just two years later and I found that the RNN,"},{"from":2755.21,"to":2759.78,"location":2,"content":"the word RNN only appeared nine times and the word transformer appeared 63 times,"},{"from":2759.78,"to":2762.16,"location":2,"content":"and in fact the organizers noted that everyone,"},{"from":2762.16,"to":2764.33,"location":2,"content":"well, most people seem to using transformers now."},{"from":2764.33,"to":2767.93,"location":2,"content":"So um, this shows that things change pretty fast in deep learning."},{"from":2767.93,"to":2771.35,"location":2,"content":"The thing that was hot and new just a few years ago um,"},{"from":2771.35,"to":2775.66,"location":2,"content":"is- is now being passed by perhaps by other kinds of approaches."},{"from":2775.66,"to":2777.26,"location":2,"content":"So, you're going to learn more about transformers"},{"from":2777.26,"to":2779.32,"location":2,"content":"later but I guess that gives you a kind of"},{"from":2779.32,"to":2784.39,"location":2,"content":"idea of where LSTMs are currently in applications."},{"from":2784.39,"to":2789.84,"location":2,"content":"Okay. So, the second kind of RNN we're going to learn about is gated recurrent units."},{"from":2789.84,"to":2792.61,"location":2,"content":"So, these fortunately are simpler than LSTMs,"},{"from":2792.61,"to":2796.07,"location":2,"content":"in fact that was the motivation for them being proposed."},{"from":2796.07,"to":2800.15,"location":2,"content":"They were proposed in 2014 as a way to try to retain"},{"from":2800.15,"to":2805.05,"location":2,"content":"the strengths of LSTMs by getting rid of any unnecessary complexities."},{"from":2805.05,"to":2806.82,"location":2,"content":"So, in a GRU,"},{"from":2806.82,"to":2808.52,"location":2,"content":"we don't have a cell state."},{"from":2808.52,"to":2810.47,"location":2,"content":"We again just have a hidden state."},{"from":2810.47,"to":2814.07,"location":2,"content":"But the thing it has in ah in common with LSTMs is that we're going to be"},{"from":2814.07,"to":2817.46,"location":2,"content":"using gates to control the flow of information."},{"from":2817.46,"to":2820.41,"location":2,"content":"So, here are the equations for GRU."},{"from":2820.41,"to":2822.89,"location":2,"content":"We start off with two gates."},{"from":2822.89,"to":2826.43,"location":2,"content":"So the first gate is called the update gate and this"},{"from":2826.43,"to":2831.05,"location":2,"content":"controls what parts of the hidden states are going to be updated versus preserved."},{"from":2831.05,"to":2833.75,"location":2,"content":"So, you can kind of view this as playing"},{"from":2833.75,"to":2837.17,"location":2,"content":"the role of both the forget gate and the input gate in"},{"from":2837.17,"to":2844.58,"location":2,"content":"the LSTM and it's computed in much the same way as the gates in the LSTM were."},{"from":2844.58,"to":2847.57,"location":2,"content":"The second gate is called the reset gate rt,"},{"from":2847.57,"to":2850.55,"location":2,"content":"and this gate is controlling which parts of"},{"from":2850.55,"to":2854.91,"location":2,"content":"the previous hidden state are going to be used to compute new contents."},{"from":2854.91,"to":2857.74,"location":2,"content":"So, you can think of the- the reset gate as kind of selecting"},{"from":2857.74,"to":2861.3,"location":2,"content":"which parts of the previous hidden states are useful versus not useful."},{"from":2861.3,"to":2865.01,"location":2,"content":"So, it's going to discard some things and select some other things."},{"from":2865.01,"to":2868.05,"location":2,"content":"Okay. So, here's how those gates get used."},{"from":2868.05,"to":2869.69,"location":2,"content":"Um, h tilde here."},{"from":2869.69,"to":2874.4,"location":2,"content":"This is you can think of it as the new hidden state contents and what's"},{"from":2874.4,"to":2876.44,"location":2,"content":"going on in that equation is that we are applying"},{"from":2876.44,"to":2879.41,"location":2,"content":"the reset gate to the previous hidden state ht minus"},{"from":2879.41,"to":2884.15,"location":2,"content":"one um and then putting all of that through some linear transformations and"},{"from":2884.15,"to":2887.42,"location":2,"content":"a tan H and then this gives us the new content"},{"from":2887.42,"to":2891.3,"location":2,"content":"which we want to write to the hidden cell."},{"from":2891.3,"to":2895.66,"location":2,"content":"And then lastly our new hidden cell is going to be a combination"},{"from":2895.66,"to":2900.07,"location":2,"content":"of ah this new content and the previous hidden state."},{"from":2900.07,"to":2904.79,"location":2,"content":"So, the important thing to notice here is that we have this one minus u and u term."},{"from":2904.79,"to":2907.37,"location":2,"content":"So um, it's kind of like a balance right?"},{"from":2907.37,"to":2911.08,"location":2,"content":"U is ah is setting the balance between"},{"from":2911.08,"to":2915.09,"location":2,"content":"preserving things from the previous hidden state versus writing new stuff."},{"from":2915.09,"to":2916.36,"location":2,"content":"So, whereas in the LSTM,"},{"from":2916.36,"to":2919.2,"location":2,"content":"those were two completely separate gates that could be whatever value."},{"from":2919.2,"to":2922.24,"location":2,"content":"Here we have this constraint that U is being uh, balanced."},{"from":2922.24,"to":2924.76,"location":2,"content":"So, if you have more of one, you have to have less of the other."},{"from":2924.76,"to":2931.16,"location":2,"content":"So, this is one way in which the creators of the GRU sought to make LSTMs more simple."},{"from":2931.16,"to":2934.52,"location":2,"content":"Was by having a single gate play both of these roles."},{"from":2934.52,"to":2939.41,"location":2,"content":"Okay. So, that's GRUs and I think it's a little less obvious just looking at it."},{"from":2939.41,"to":2944.87,"location":2,"content":"Why GRUs help the vanishing gradients problem because there is no explicit ah memory"},{"from":2944.87,"to":2946.52,"location":2,"content":"cell, like there is in LSTMs."},{"from":2946.52,"to":2950.36,"location":2,"content":"So, I think the way to look at this here is um GRUs,"},{"from":2950.36,"to":2952.16,"location":2,"content":"you can view this as also being a solution to"},{"from":2952.16,"to":2955.22,"location":2,"content":"the vanishing gradient problem because like LSTMs,"},{"from":2955.22,"to":2959.43,"location":2,"content":"GRUs make it easier to retain information ah long-term."},{"from":2959.43,"to":2961.24,"location":2,"content":"So, for example here,"},{"from":2961.24,"to":2965.09,"location":2,"content":"if the update gate ut is set to zero,"},{"from":2965.09,"to":2970.39,"location":2,"content":"then we're going to be ah keeping the hidden state the same on every step."},{"from":2970.39,"to":2973.64,"location":2,"content":"And again that's maybe not a good idea but at least that is a strategy you can easily"},{"from":2973.64,"to":2977.32,"location":2,"content":"do in order to retain information over long distances."},{"from":2977.32,"to":2980.42,"location":2,"content":"So that's kind of like- like the same explanation of how GRUs make it"},{"from":2980.42,"to":2986.41,"location":2,"content":"potentially easier for RNNs to retain information long-term."},{"from":2986.41,"to":2991.49,"location":2,"content":"Okay. So, we've learned about these two different types of RNNs. Yes."},{"from":2991.49,"to":3008.23,"location":2,"content":"[inaudible]"},{"from":3008.23,"to":3010.11,"location":2,"content":"I think the question was,"},{"from":3010.11,"to":3012.79,"location":2,"content":"if we view the two gates in the GRU, as being, uh,"},{"from":3012.79,"to":3020.44,"location":2,"content":"a precise, um, analogy to the gates in the LSTM or are they more of a fuzzy analogy."},{"from":3020.44,"to":3022.79,"location":2,"content":"I'd say probably more of a fuzzy analogy"},{"from":3022.79,"to":3026.08,"location":2,"content":"because there are other changes going on in here, like,"},{"from":3026.08,"to":3028.51,"location":2,"content":"for example, the fact that there's no separate, um,"},{"from":3028.51,"to":3032.26,"location":2,"content":"memory cell, it means they're not performing exactly the same functions."},{"from":3032.26,"to":3039.89,"location":2,"content":"Yeah. Okay. So, we've learned about LSTMs and GRUs which are both,"},{"from":3039.89,"to":3041.65,"location":2,"content":"um, more complicated forms of RNNs,"},{"from":3041.65,"to":3043.7,"location":2,"content":"more complicated than Vanilla RNNs."},{"from":3043.7,"to":3045.61,"location":2,"content":"And they are both,"},{"from":3045.61,"to":3048.86,"location":2,"content":"uh, more robust to the vanishing gradient problem."},{"from":3048.86,"to":3053.95,"location":2,"content":"So, um, it would be useful to know which of these should we be using in practice?"},{"from":3053.95,"to":3055.12,"location":2,"content":"Which one is more successful,"},{"from":3055.12,"to":3056.77,"location":2,"content":"the LSTM or GRU?"},{"from":3056.77,"to":3059.77,"location":2,"content":"Uh, so, I- I did a little reading and it looks like researchers have"},{"from":3059.77,"to":3062.89,"location":2,"content":"proposed a lot of different types of gated RNNs."},{"from":3062.89,"to":3064.45,"location":2,"content":"So, it's not just GRUs and LSTMs,"},{"from":3064.45,"to":3067.59,"location":2,"content":"there's many other papers with lots of other different variants."},{"from":3067.59,"to":3071.84,"location":2,"content":"Uh, but these are definitely the two that are most widely used."},{"from":3071.84,"to":3075.34,"location":2,"content":"And, ah, you can probably say that the biggest difference between the two, um,"},{"from":3075.34,"to":3077.92,"location":2,"content":"for sure is the fact that GRUs are simpler"},{"from":3077.92,"to":3081.11,"location":2,"content":"and quicker to compute and they have fewer parameters."},{"from":3081.11,"to":3083.5,"location":2,"content":"So, this makes an actual practical difference to you as, uh,"},{"from":3083.5,"to":3087.97,"location":2,"content":"a deep learning practitioner because if you build your net based on GRUs,"},{"from":3087.97,"to":3089.97,"location":2,"content":"then it's gonna be faster to run forwards and,"},{"from":3089.97,"to":3092.17,"location":2,"content":"you know, faster to train and so on."},{"from":3092.17,"to":3094.54,"location":2,"content":"So, other than that, there appears to be"},{"from":3094.54,"to":3098.68,"location":2,"content":"no very conclusive evidence that one of these LSTM or GRUs,"},{"from":3098.68,"to":3102.79,"location":2,"content":"uh, is consistently outperforming the other on lots of different tasks."},{"from":3102.79,"to":3105.95,"location":2,"content":"Uh, it seems that often, uh,"},{"from":3105.95,"to":3108.25,"location":2,"content":"sometimes GRUs do perform as well as LSTMs,"},{"from":3108.25,"to":3111.52,"location":2,"content":"but there are cases where one of them performs better than the other."},{"from":3111.52,"to":3113.44,"location":2,"content":"So, as a rule of thumb,"},{"from":3113.44,"to":3117.19,"location":2,"content":"it seems like LSTM is often a good default choice to start with, uh,"},{"from":3117.19,"to":3118.84,"location":2,"content":"especially if your data has"},{"from":3118.84,"to":3121.15,"location":2,"content":"particularly long dependencies because there's evidence to think"},{"from":3121.15,"to":3125.5,"location":2,"content":"that LSTMs might be slightly better at keeping information over very long distances."},{"from":3125.5,"to":3127.57,"location":2,"content":"And also, if you have a lot of training data,"},{"from":3127.57,"to":3129.67,"location":2,"content":"you might think that LSTMs are a better choice because they"},{"from":3129.67,"to":3132.34,"location":2,"content":"have more parameters which means that,"},{"from":3132.34,"to":3137.94,"location":2,"content":"um, maybe you need more train data to learn them."},{"from":3137.94,"to":3141.7,"location":2,"content":"So, a rule of thumb is that maybe you want to start with LSTMs"},{"from":3141.7,"to":3143.5,"location":2,"content":"and if you're happy with their performance and you're"},{"from":3143.5,"to":3145.68,"location":2,"content":"happy with how long it takes to train, then you stick with that."},{"from":3145.68,"to":3147.61,"location":2,"content":"But if you feel like you need it to be more efficient,"},{"from":3147.61,"to":3150.85,"location":2,"content":"then maybe you should switch to GRUs and see how that goes with the performance"},{"from":3150.85,"to":3154.69,"location":2,"content":"and if it's faster. All right."},{"from":3154.69,"to":3156.97,"location":2,"content":"So, um, we've talked so far about how"},{"from":3156.97,"to":3160.72,"location":2,"content":"the vanishing/exploding gradients are a problem that occur a lot in RNNs."},{"from":3160.72,"to":3162.41,"location":2,"content":"But, um, the question is,"},{"from":3162.41,"to":3163.74,"location":2,"content":"is it only an RNN problem?"},{"from":3163.74,"to":3166.33,"location":2,"content":"Does this occur in other kinds of neural networks as well?"},{"from":3166.33,"to":3168.11,"location":2,"content":"And the answer is,"},{"from":3168.11,"to":3170.17,"location":2,"content":"uh, no, it's not just an RNN problem."},{"from":3170.17,"to":3172.39,"location":2,"content":"In fact, vanishing and exploding gradients are a"},{"from":3172.39,"to":3175.15,"location":2,"content":"pretty significant problem for"},{"from":3175.15,"to":3178.42,"location":2,"content":"most neural architecture such as feed-forward and convolutional,"},{"from":3178.42,"to":3179.74,"location":2,"content":"especially when they're deep."},{"from":3179.74,"to":3182.41,"location":2,"content":"And this is a really serious problem because there's no point having"},{"from":3182.41,"to":3186.66,"location":2,"content":"a really cool neural architecture if you can't learn it efficiently because of the,"},{"from":3186.66,"to":3188.5,"location":2,"content":"uh, vanishing gradient problem."},{"from":3188.5,"to":3193.03,"location":2,"content":"So, in particular, uh, in these feed-forward and convolutional networks, uh,"},{"from":3193.03,"to":3195.28,"location":2,"content":"you often have a gradient becoming vanishingly"},{"from":3195.28,"to":3198.52,"location":2,"content":"small over back-propagation, uh, because of the Chain Rule,"},{"from":3198.52,"to":3200.2,"location":2,"content":"because of this multiplying by"},{"from":3200.2,"to":3202.39,"location":2,"content":"all these different intermediate gradients or"},{"from":3202.39,"to":3205.12,"location":2,"content":"sometimes due to your choice of non-linearity function."},{"from":3205.12,"to":3209.26,"location":2,"content":"So, if this happens, this means that your- the lower layers of your, let's say,"},{"from":3209.26,"to":3211.28,"location":2,"content":"convolutional or feed-forward network,"},{"from":3211.28,"to":3212.95,"location":2,"content":"they have a much smaller,"},{"from":3212.95,"to":3215.93,"location":2,"content":"uh, gradient than the high levels."},{"from":3215.93,"to":3219.97,"location":2,"content":"And this means that they get changed very slowly during SGD."},{"from":3219.97,"to":3221.29,"location":2,"content":"So, this means that, overall,"},{"from":3221.29,"to":3223.99,"location":2,"content":"your network is very slow to train because when you take updates,"},{"from":3223.99,"to":3227.08,"location":2,"content":"then your lower layers are changing very slowly."},{"from":3227.08,"to":3229.3,"location":2,"content":"So, one solution, uh,"},{"from":3229.3,"to":3231.28,"location":2,"content":"the kind of like a family of solutions that we've seen in"},{"from":3231.28,"to":3233.62,"location":2,"content":"recent years is that there's been lots of"},{"from":3233.62,"to":3238.72,"location":2,"content":"proposals for new types of deep feed-forward or convolutional architectures."},{"from":3238.72,"to":3242.74,"location":2,"content":"And what they do is, they add more direct connections in the network."},{"from":3242.74,"to":3244.33,"location":2,"content":"And the- the idea,"},{"from":3244.33,"to":3245.76,"location":2,"content":"kind of as we talked about before,"},{"from":3245.76,"to":3248.53,"location":2,"content":"is that if you add all of these direct connections between layers,"},{"from":3248.53,"to":3252.09,"location":2,"content":"like maybe not just adjacent layers but further apart layers,"},{"from":3252.09,"to":3254.68,"location":2,"content":"then it makes it much easier for the gradients to flow,"},{"from":3254.68,"to":3257.64,"location":2,"content":"and you're going to find it easier to train your network overall."},{"from":3257.64,"to":3259.66,"location":2,"content":"So, I'm going to show you some examples of these in"},{"from":3259.66,"to":3261.67,"location":2,"content":"particular because it's fairly likely you're going to"},{"from":3261.67,"to":3265.7,"location":2,"content":"run into these kinds of architectures when you're doing your projects and reading papers."},{"from":3265.7,"to":3269.41,"location":2,"content":"So, one example is something called residual connections or,"},{"from":3269.41,"to":3272.51,"location":2,"content":"uh, the network itself is sometimes referred to as ResNet."},{"from":3272.51,"to":3276.35,"location":2,"content":"And here we've got a figure from the related paper."},{"from":3276.35,"to":3281.07,"location":2,"content":"So, what's going on in this diagram is that you have, uh,"},{"from":3281.07,"to":3282.93,"location":2,"content":"the usual kind of you've got weight layer and"},{"from":3282.93,"to":3285.48,"location":2,"content":"a non-linearity which is ReLU, and another weight layer."},{"from":3285.48,"to":3289.18,"location":2,"content":"So, if you regard that function as being f of x, ah,"},{"from":3289.18,"to":3290.7,"location":2,"content":"what they're doing is instead of just, ah,"},{"from":3290.7,"to":3292.54,"location":2,"content":"transforming x to f of x,"},{"from":3292.54,"to":3295.15,"location":2,"content":"the- they're taking f of x plus x."},{"from":3295.15,"to":3298.43,"location":2,"content":"So they're adding this identity skip connection where"},{"from":3298.43,"to":3301.99,"location":2,"content":"the input x is skipped over those two layers and then,"},{"from":3301.99,"to":3305.47,"location":2,"content":"um, added to the output of the two layers."},{"from":3305.47,"to":3307.51,"location":2,"content":"So, the reason why this is a good idea,"},{"from":3307.51,"to":3310.15,"location":2,"content":"uh, also known as skip connections,"},{"from":3310.15,"to":3315.28,"location":2,"content":"is that the identity connection is going to preserve information by default, right?"},{"from":3315.28,"to":3318.01,"location":2,"content":"So, if you imagine perhaps if you, um,"},{"from":3318.01,"to":3320.11,"location":2,"content":"initialize your network and you"},{"from":3320.11,"to":3322.6,"location":2,"content":"initialize your weight layers to have small random values,"},{"from":3322.6,"to":3324.84,"location":2,"content":"then if they're small and kind of close to zero,"},{"from":3324.84,"to":3328.89,"location":2,"content":"then you're going to have something like a noisy identity function, right?"},{"from":3328.89,"to":3332.24,"location":2,"content":"So you're going to be preserving information by default through all of your layers."},{"from":3332.24,"to":3333.52,"location":2,"content":"And if you have a very deep network,"},{"from":3333.52,"to":3335.11,"location":2,"content":"that means that even often many,"},{"from":3335.11,"to":3340.32,"location":2,"content":"um, many layers, you're still gonna have something like your original input."},{"from":3340.32,"to":3344.22,"location":2,"content":"So, uh, the- the people who wrote this paper, they show that, uh,"},{"from":3344.22,"to":3346.66,"location":2,"content":"if you don't have something like skip connections then"},{"from":3346.66,"to":3349.57,"location":2,"content":"actually you can find that deep layers- uh,"},{"from":3349.57,"to":3353.24,"location":2,"content":"deep networks perform worse on some tasks than shallow networks."},{"from":3353.24,"to":3354.95,"location":2,"content":"Not because they're not expressive enough,"},{"from":3354.95,"to":3356.66,"location":2,"content":"but because they're too difficult to learn."},{"from":3356.66,"to":3358.28,"location":2,"content":"So, when you attempt to learn deep networks,"},{"from":3358.28,"to":3359.95,"location":2,"content":"it just doesn't learn effectively and you end up"},{"from":3359.95,"to":3361.8,"location":2,"content":"getting worse performance in the shallow network."},{"from":3361.8,"to":3363.07,"location":2,"content":"So, the people who wrote this paper,"},{"from":3363.07,"to":3365.05,"location":2,"content":"they show that when they add these skip connections,"},{"from":3365.05,"to":3367.11,"location":2,"content":"then they made the deep networks, uh,"},{"from":3367.11,"to":3372,"location":2,"content":"much more effective and they managed to get good performance."},{"from":3372,"to":3375.28,"location":2,"content":"Uh, so another example which kinda take this- this idea"},{"from":3375.28,"to":3378.22,"location":2,"content":"further is something called dense connections or DenseNet."},{"from":3378.22,"to":3379.59,"location":2,"content":"And again, this was, uh,"},{"from":3379.59,"to":3383.97,"location":2,"content":"something proposed I think in a feed-forward or or convolutional setting."},{"from":3383.97,"to":3386.77,"location":2,"content":"And, ah, it's just kind of the same as skip connections but except ,"},{"from":3386.77,"to":3388.03,"location":2,"content":"um, connects everything to everything."},{"from":3388.03,"to":3390.07,"location":2,"content":"So, add more of these skip connections kind of"},{"from":3390.07,"to":3392.68,"location":2,"content":"from all layers to all layers and they showed that this,"},{"from":3392.68,"to":3394.48,"location":2,"content":"uh, performs even better."},{"from":3394.48,"to":3397.45,"location":2,"content":"And, uh, the last one I want to talk about which I don't have a picture"},{"from":3397.45,"to":3400.21,"location":2,"content":"for is something called highway connections."},{"from":3400.21,"to":3403.18,"location":2,"content":"So, this is similar to the residual or skip connections."},{"from":3403.18,"to":3406.59,"location":2,"content":"Ah, but the idea is that instead of just adding your x,"},{"from":3406.59,"to":3408.64,"location":2,"content":"adding your identity, uh, connection,"},{"from":3408.64,"to":3412.06,"location":2,"content":"the idea is that you're gonna have a gate that controls the balance between, um,"},{"from":3412.06,"to":3415.96,"location":2,"content":"adding the identity and computing, ah, the transformation."},{"from":3415.96,"to":3418.41,"location":2,"content":"So, instead of f of x plus x, you're gonna have, you know,"},{"from":3418.41,"to":3419.99,"location":2,"content":"gate times f of x plus, you know,"},{"from":3419.99,"to":3422.11,"location":2,"content":"one minus gate times x, something like that."},{"from":3422.11,"to":3425.56,"location":2,"content":"Um, so, this work was actually inspired by LSTMs,"},{"from":3425.56,"to":3427.51,"location":2,"content":"but instead of applying it to a recurrent setting,"},{"from":3427.51,"to":3433.86,"location":2,"content":"they were seeking to apply it to a feed-forward setting."},{"from":3433.86,"to":3436.12,"location":2,"content":"Okay. I'm gonna keep going for now."},{"from":3436.12,"to":3439.39,"location":2,"content":"Um. So, overall the question was,"},{"from":3439.39,"to":3440.62,"location":2,"content":"you know, how much uh,"},{"from":3440.62,"to":3443.65,"location":2,"content":"vanishing and exploding gradients a problem outside of the setting of RNNs?"},{"from":3443.65,"to":3446.71,"location":2,"content":"And I think uh, the important takeaway is that it is a big problem"},{"from":3446.71,"to":3450.59,"location":2,"content":"but you should notice that it is particularly a problem for RNNs."},{"from":3450.59,"to":3454,"location":2,"content":"So, um, RNNs are particularly unstable and"},{"from":3454,"to":3457.53,"location":2,"content":"this is essentially due to the repeated multiplication by the same weight matrix."},{"from":3457.53,"to":3459.18,"location":2,"content":"If you remember from last time, um,"},{"from":3459.18,"to":3461.89,"location":2,"content":"the characteristic thing about RNNs that makes them recurrent is"},{"from":3461.89,"to":3464.77,"location":2,"content":"the fact that you are applying the same weight matrix over and over again."},{"from":3464.77,"to":3467.09,"location":2,"content":"So, this is actually the core reason"},{"from":3467.09,"to":3469.86,"location":2,"content":"why they are so prone to the vanishing and exploding gradients,"},{"from":3469.86,"to":3473.67,"location":2,"content":"and ah, you can see some more information about that in the paper."},{"from":3473.67,"to":3477.73,"location":2,"content":"Okay. So, I know there's been a lot of dense information today,"},{"from":3477.73,"to":3479.82,"location":2,"content":"a lot of um, lot of notation."},{"from":3479.82,"to":3481.69,"location":2,"content":"So, here's a recap, if I've lost you at any point."},{"from":3481.69,"to":3483.28,"location":2,"content":"Now's a good time to jump back in because it's gonna"},{"from":3483.28,"to":3485.51,"location":2,"content":"get a little easier to understand perhaps."},{"from":3485.51,"to":3487.6,"location":2,"content":"So, okay, recap. What have we learned about today?"},{"from":3487.6,"to":3490.43,"location":2,"content":"Um, the first thing we learned about was the vanishing gradient problem."},{"from":3490.43,"to":3491.83,"location":2,"content":"We learned uh, what it is."},{"from":3491.83,"to":3495.58,"location":2,"content":"We learned why it happens and we saw why it's bad for RNNs,"},{"from":3495.58,"to":3498.09,"location":2,"content":"for example, RNN language models."},{"from":3498.09,"to":3501.64,"location":2,"content":"Ah, and we also learned about LSTMs and GRUs which are"},{"from":3501.64,"to":3505.48,"location":2,"content":"more complicated RNNs and they use gates to control the flow of information."},{"from":3505.48,"to":3509.39,"location":2,"content":"And by doing that, they are more resilient to the vanishing gradient problem."},{"from":3509.39,"to":3511.39,"location":2,"content":"Okay. So, if the remainder of this lecture,"},{"from":3511.39,"to":3512.74,"location":2,"content":"I think we've got about 20 minutes left,"},{"from":3512.74,"to":3516.45,"location":2,"content":"ah, we're going to be learning about two more advanced type of RNNs."},{"from":3516.45,"to":3519.04,"location":2,"content":"So, the first one is bidirectional RNNs and that's all"},{"from":3519.04,"to":3523.11,"location":2,"content":"about information flowing left to right and right to left."},{"from":3523.11,"to":3524.91,"location":2,"content":"And then we're also going to learn about"},{"from":3524.91,"to":3529.78,"location":2,"content":"multi-layer RNNs which is when you apply multiple RNNs on top of each other."},{"from":3529.78,"to":3534.1,"location":2,"content":"So, I'd say that both of these are pretty simple conceptually."},{"from":3534.1,"to":3536.91,"location":2,"content":"Um, so it shouldn't be too hard to understand."},{"from":3536.91,"to":3540.16,"location":2,"content":"All right, so let's start with bidirectional RNNs."},{"from":3540.16,"to":3544.22,"location":2,"content":"Um, this is a picture which you saw at the end of last lecture."},{"from":3544.22,"to":3545.3,"location":2,"content":"So, if you remember,"},{"from":3545.3,"to":3547.69,"location":2,"content":"sentiment classification is the task when you have"},{"from":3547.69,"to":3550.15,"location":2,"content":"some kind of input sentence such as the movie was"},{"from":3550.15,"to":3555.46,"location":2,"content":"terribly exciting and you want to classify this as a positive or negative sentiment."},{"from":3555.46,"to":3559.68,"location":2,"content":"So, in this example, it should be seen as positive sentiment."},{"from":3559.68,"to":3563.65,"location":2,"content":"So, um, this is an example of how you might try to"},{"from":3563.65,"to":3566.86,"location":2,"content":"solve sentiment classification using a fairly simple RNN model."},{"from":3566.86,"to":3569.83,"location":2,"content":"Ah, here we're using the RNN as a kind of encoder of"},{"from":3569.83,"to":3572.89,"location":2,"content":"the sentence and the hidden states represent the sentence."},{"from":3572.89,"to":3575.74,"location":2,"content":"And we'll do some kind of combination of the hidden states to compute uh,"},{"from":3575.74,"to":3577.76,"location":2,"content":"what we think the sentiment is."},{"from":3577.76,"to":3580.16,"location":2,"content":"So, my question is, if we look at let's say,"},{"from":3580.16,"to":3584.35,"location":2,"content":"the hidden state that corresponds to the word terribly and we're regarding"},{"from":3584.35,"to":3586.42,"location":2,"content":"this hidden state as a representation of the word"},{"from":3586.42,"to":3589.51,"location":2,"content":"terribly in the context of the sentence."},{"from":3589.51,"to":3593.45,"location":2,"content":"So, for this reason we- we sometimes call hidden states in this kind of situation"},{"from":3593.45,"to":3595.93,"location":2,"content":"a contextual representation because the idea is that it's"},{"from":3595.93,"to":3599.78,"location":2,"content":"a representation of the word terribly in the context of the sentence."},{"from":3599.78,"to":3604.15,"location":2,"content":"So, thing to think about here is that this contextual representation,"},{"from":3604.15,"to":3607.16,"location":2,"content":"it only contains information about the left context."},{"from":3607.16,"to":3610.15,"location":2,"content":"So, for terribly, the left context is the words um,"},{"from":3610.15,"to":3613.12,"location":2,"content":"the movie was and this hidden state the one that's got"},{"from":3613.12,"to":3616.43,"location":2,"content":"a blue box around it has only seen information to the left."},{"from":3616.43,"to":3620.49,"location":2,"content":"It hasn't seen the information of the words exciting or exclamation mark."},{"from":3620.49,"to":3624.72,"location":2,"content":"So, what we're asking is what about the right context?"},{"from":3624.72,"to":3628.69,"location":2,"content":"The right context of terribly is- is what exciting and the exclamation mark."},{"from":3628.69,"to":3633.04,"location":2,"content":"And do we think that the right context is useful here?"},{"from":3633.04,"to":3635.23,"location":2,"content":"Do we think that this is something we want to know about?"},{"from":3635.23,"to":3637.41,"location":2,"content":"And I would argue that in this example,"},{"from":3637.41,"to":3641.7,"location":2,"content":"it is actually kind of important because we've got the phrase terribly exciting."},{"from":3641.7,"to":3644.83,"location":2,"content":"And if you look at the word terribly in isolation,"},{"from":3644.83,"to":3647.01,"location":2,"content":"terrible or terribly usually means something bad, right?"},{"from":3647.01,"to":3650.65,"location":2,"content":"But terribly exciting, you can mean something good because it just means very exciting."},{"from":3650.65,"to":3653.23,"location":2,"content":"So, if you know about the right context,"},{"from":3653.23,"to":3656.68,"location":2,"content":"the word exciting then this might quite significantly"},{"from":3656.68,"to":3658.9,"location":2,"content":"modify your perception of the meaning of the word"},{"from":3658.9,"to":3661.21,"location":2,"content":"terribly in the context of the sentence."},{"from":3661.21,"to":3663.79,"location":2,"content":"And especially given that we're trying to do sentiment classification,"},{"from":3663.79,"to":3665.82,"location":2,"content":"this is- this is kind of important."},{"from":3665.82,"to":3670.15,"location":2,"content":"So this motivates why you might want to have information"},{"from":3670.15,"to":3673.91,"location":2,"content":"from both the left and the right when you're making your representations."},{"from":3673.91,"to":3676,"location":2,"content":"Ah, if when you were a kid,"},{"from":3676,"to":3678.53,"location":2,"content":"your parents told you to look both ways before you cross the street."},{"from":3678.53,"to":3680.62,"location":2,"content":"You might regard it as the same kind of idea that there's"},{"from":3680.62,"to":3682.51,"location":2,"content":"useful information to the left and the right that"},{"from":3682.51,"to":3684.97,"location":2,"content":"you'd like to know about ah, before you do anything."},{"from":3684.97,"to":3687.93,"location":2,"content":"Okay. So that's the motivation and um,"},{"from":3687.93,"to":3691.9,"location":2,"content":"here is how a bidirectional RNN might work in practice."},{"from":3691.9,"to":3695.07,"location":2,"content":"I have a kind of accidentally festive color scheme here."},{"from":3695.07,"to":3698.76,"location":2,"content":"And so the idea is that you have two RNNs going on."},{"from":3698.76,"to":3702.88,"location":2,"content":"You have the forward RNN as before that encodes the sentence left to right."},{"from":3702.88,"to":3706,"location":2,"content":"And then separately, you also have a backwards RNN."},{"from":3706,"to":3709.14,"location":2,"content":"And this has completely separate weights to the forward RNN."},{"from":3709.14,"to":3712.66,"location":2,"content":"So, the backward RNN is just doing the same thing"},{"from":3712.66,"to":3716.11,"location":2,"content":"except that it's encoding the sequence from right to left."},{"from":3716.11,"to":3719.98,"location":2,"content":"So, each of the hidden states is computed based on the one to the right."},{"from":3719.98,"to":3722.5,"location":2,"content":"And then finally, you just take the hidden states from"},{"from":3722.5,"to":3726.7,"location":2,"content":"the two RNNs and then you concatenate them together and you've got your uh,"},{"from":3726.7,"to":3729.39,"location":2,"content":"your final kind of representations."},{"from":3729.39,"to":3732.03,"location":2,"content":"So, in particular, if we now think about"},{"from":3732.03,"to":3736.33,"location":2,"content":"this contextual representation of the word terribly in the context,"},{"from":3736.33,"to":3742.18,"location":2,"content":"um, this- this vector has information from both the left and the right, right?"},{"from":3742.18,"to":3744.24,"location":2,"content":"Because you had the forwards and backwards RNNs that"},{"from":3744.24,"to":3747.3,"location":2,"content":"respectively had information from both left and right."},{"from":3747.3,"to":3750.19,"location":2,"content":"So the idea is that these concatenated hidden states,"},{"from":3750.19,"to":3754.74,"location":2,"content":"those can be regarded as kind of like the outputs of the bidirectional RNN."},{"from":3754.74,"to":3756.46,"location":2,"content":"Like if you're going to use these hidden states for"},{"from":3756.46,"to":3758.68,"location":2,"content":"any kind of further computation, then ah,"},{"from":3758.68,"to":3760.78,"location":2,"content":"it's these concatenated hidden states that you are going to be"},{"from":3760.78,"to":3764.25,"location":2,"content":"passing on to the next part of the network."},{"from":3764.25,"to":3768.2,"location":2,"content":"Um, here- here are the equations that just say the same thing."},{"from":3768.2,"to":3771.79,"location":2,"content":"So, you have your forward RNN and here we've got ah,"},{"from":3771.79,"to":3773.99,"location":2,"content":"a notation that you might not have seen before"},{"from":3773.99,"to":3777.01,"location":2,"content":"this kind of notation where it says RNN and then in brackets,"},{"from":3777.01,"to":3780.78,"location":2,"content":"the previous hidden state and the input that's simply saying that you know,"},{"from":3780.78,"to":3784.18,"location":2,"content":"HT is computed from the previous hidden state and the input."},{"from":3784.18,"to":3788.59,"location":2,"content":"And RNN forward could be a vanilla or a GRU or an LSTM."},{"from":3788.59,"to":3790.81,"location":2,"content":"It doesn't really matter, we're looking at it abstractly."},{"from":3790.81,"to":3796.1,"location":2,"content":"So, you have these two separate RNNs,"},{"from":3796.1,"to":3799.54,"location":2,"content":"RNN forwards and RNN backwards and generally, these have separate weights."},{"from":3799.54,"to":3801.91,"location":2,"content":"Although I have seen some papers where they have shared weights."},{"from":3801.91,"to":3803.88,"location":2,"content":"So, it seems that sometimes that does work better,"},{"from":3803.88,"to":3806.79,"location":2,"content":"perhaps maybe when you have enough training data."},{"from":3806.79,"to":3812.02,"location":2,"content":"And then finally, we regard these concatenated hidden states which you might just"},{"from":3812.02,"to":3818.55,"location":2,"content":"notice ht as being like the hidden state of the bidirectional RNN."},{"from":3818.55,"to":3822.55,"location":2,"content":"So, um, the previous diagram is pretty unwieldy."},{"from":3822.55,"to":3824.39,"location":2,"content":"So here's a simplified diagram."},{"from":3824.39,"to":3826.24,"location":2,"content":"And this is probably the only kind of diagram you're going to"},{"from":3826.24,"to":3828.7,"location":2,"content":"see from now on to denote bidirectional RNNs."},{"from":3828.7,"to":3830.77,"location":2,"content":"Um, so, what we've done here is you've just"},{"from":3830.77,"to":3833.57,"location":2,"content":"made all of the horizontal arrows go left and right ah,"},{"from":3833.57,"to":3836.26,"location":2,"content":"to represent that this is a bidirectional RNN."},{"from":3836.26,"to":3840.37,"location":2,"content":"So, the other thing you should assume is that the hidden states depicted here, you know,"},{"from":3840.37,"to":3844.24,"location":2,"content":"these red- red trying- red rectangles with the dots."},{"from":3844.24,"to":3846.58,"location":2,"content":"You can assume that those are the concatenated forwards,"},{"from":3846.58,"to":3848.59,"location":2,"content":"backwards hidden states from the bidirectional RNN."},{"from":3848.59,"to":3856,"location":2,"content":"[inaudible]"},{"from":3856,"to":3858.41,"location":2,"content":"Okay. So the question is, um,"},{"from":3858.41,"to":3862.06,"location":2,"content":"would you train your forwards and backwards RNNs kind of separately,"},{"from":3862.06,"to":3863.89,"location":2,"content":"um, on some kind of task and then"},{"from":3863.89,"to":3866.66,"location":2,"content":"maybe concatenate them together once they're separately trained networks,"},{"from":3866.66,"to":3868.28,"location":2,"content":"or would you train them all together?"},{"from":3868.28,"to":3872.2,"location":2,"content":"Um, it seems to me that it's much more common to train them together,"},{"from":3872.2,"to":3875.23,"location":2,"content":"but I don- I don't think I've heard of anyone training them separately."},{"from":3875.23,"to":3877.27,"location":2,"content":"Uh, so yeah, it seems like the standard practice is usually"},{"from":3877.27,"to":3879.16,"location":2,"content":"to train them together. Does that make sense?"},{"from":3879.16,"to":3893.29,"location":2,"content":"[inaudible]."},{"from":3893.29,"to":3895.69,"location":2,"content":"So, let's suppose that we were trying to build"},{"from":3895.69,"to":3899.44,"location":2,"content":"a sentiment classification system using the bidirectional RNN."},{"from":3899.44,"to":3903.46,"location":2,"content":"Then what you do, which maybe I should have pictured but I didn't have space, is uh,"},{"from":3903.46,"to":3907.42,"location":2,"content":"you would do the same thing that you were doing with the unidirectional RNN, uh,"},{"from":3907.42,"to":3910.05,"location":2,"content":"which was, let's say an element y is min or max,"},{"from":3910.05,"to":3911.66,"location":2,"content":"um, to get your sentence encoding."},{"from":3911.66,"to":3916.95,"location":2,"content":"Maybe you just do that but over the concatenated, um, n states."},{"from":3916.95,"to":3920.56,"location":2,"content":"Okay. So, an important thing to note is that, uh,"},{"from":3920.56,"to":3923.02,"location":2,"content":"when talking about applying bidirectional RNNs,"},{"from":3923.02,"to":3926.93,"location":2,"content":"we've assumed that we actually have access to the entire input sequence."},{"from":3926.93,"to":3928.78,"location":2,"content":"So, we assume that we have the full sentence,"},{"from":3928.78,"to":3931.57,"location":2,"content":"uh, the movie was very exciting, and,"},{"from":3931.57,"to":3934.74,"location":2,"content":"uh, that, that was a necessary assumption in order to"},{"from":3934.74,"to":3937.93,"location":2,"content":"be able to run the forwards and the backwards RNN, right?"},{"from":3937.93,"to":3940.93,"location":2,"content":"Um, so there are some situations where you can't assume this."},{"from":3940.93,"to":3943.18,"location":2,"content":"Like, for example, in Language Modeling,"},{"from":3943.18,"to":3947.36,"location":2,"content":"you only have access to the left context kind of by definition of the task."},{"from":3947.36,"to":3948.99,"location":2,"content":"You only know the words that have come so far."},{"from":3948.99,"to":3950.41,"location":2,"content":"You don't know what's coming next."},{"from":3950.41,"to":3954.07,"location":2,"content":"So, you can't use a bidirectional RNN, uh,"},{"from":3954.07,"to":3955.53,"location":2,"content":"to do Language Modeling, uh,"},{"from":3955.53,"to":3957.76,"location":2,"content":"in the way that we've depicted here because uh,"},{"from":3957.76,"to":3959.82,"location":2,"content":"you don't have the full sequence."},{"from":3959.82,"to":3963.11,"location":2,"content":"However, if you do have access to the entire sequence."},{"from":3963.11,"to":3965.23,"location":2,"content":"Uh, so, for example, if you're doing any kind of encoding"},{"from":3965.23,"to":3967.49,"location":2,"content":"similar to the sentiment example,"},{"from":3967.49,"to":3971.68,"location":2,"content":"uh, then bidirectionally- bidirectionality is pretty powerful."},{"from":3971.68,"to":3974.82,"location":2,"content":"And you should probably regard it as a good thing to do by default uh,"},{"from":3974.82,"to":3976.87,"location":2,"content":"because it turns out that getting this information from"},{"from":3976.87,"to":3978.8,"location":2,"content":"both the left and the right, uh,"},{"from":3978.8,"to":3983.72,"location":2,"content":"makes it a lot easier to learn these more useful contextual representations."},{"from":3983.72,"to":3985.87,"location":2,"content":"So, in particular, as a preview of"},{"from":3985.87,"to":3988.03,"location":2,"content":"something you're going to learn about later in the class, uh,"},{"from":3988.03,"to":3990.61,"location":2,"content":"there's a model called BERT, B-E-R-T,"},{"from":3990.61,"to":3994.33,"location":2,"content":"and that stands for Bidirectional Encoder Representations from Transformers."},{"from":3994.33,"to":3996.01,"location":2,"content":"And this is a pretty recently."},{"from":3996.01,"to":3999.07,"location":2,"content":"Like, a few months ago, uh, proposed system,"},{"from":3999.07,"to":4002.46,"location":2,"content":"and it's this pre-trained contextual representation system."},{"from":4002.46,"to":4006.45,"location":2,"content":"Um, and it's heavily reliant on the idea of bidirectionality."},{"from":4006.45,"to":4008.76,"location":2,"content":"It turns out that the bidirectional, uh,"},{"from":4008.76,"to":4011.57,"location":2,"content":"nature of BERT is pretty important to its success."},{"from":4011.57,"to":4013.29,"location":2,"content":"So, you're gonna learn more about that later,"},{"from":4013.29,"to":4015.99,"location":2,"content":"but that's just an example of how bidirectionality can give you much"},{"from":4015.99,"to":4019.88,"location":2,"content":"more uh, powerful contextual representations."},{"from":4019.88,"to":4024.39,"location":2,"content":"Okay. So the last thing we're going to talk about today is multi-layer RNNs."},{"from":4024.39,"to":4028.8,"location":2,"content":"Uh, so you could regard RNNs as already being deep"},{"from":4028.8,"to":4034.2,"location":2,"content":"in some sense because you've already unrolled them over potentially very many timesteps,"},{"from":4034.2,"to":4036.63,"location":2,"content":"and you could regard that as a kind of depth, right?"},{"from":4036.63,"to":4039.39,"location":2,"content":"But there's another way that RNNs could be deep."},{"from":4039.39,"to":4045.21,"location":2,"content":"So, for example, if you applied multiple RNNs kind of one after another,"},{"from":4045.21,"to":4048.55,"location":2,"content":"then this would be a different way to make your RNN deep,"},{"from":4048.55,"to":4050.49,"location":2,"content":"and this is the idea between, uh,"},{"from":4050.49,"to":4053.78,"location":2,"content":"behind a multi-layer RNN."},{"from":4053.78,"to":4057.32,"location":2,"content":"So, the reason why you would want to do this is because uh,"},{"from":4057.32,"to":4060.66,"location":2,"content":"this might allow the network to compute more complex representations."},{"from":4060.66,"to":4063.84,"location":2,"content":"So, this is the logic betwe- behind deep networks in general."},{"from":4063.84,"to":4065.28,"location":2,"content":"So, if you're familiar with the idea of why"},{"from":4065.28,"to":4067.62,"location":2,"content":"deeper is better for let's say convolutional networks,"},{"from":4067.62,"to":4069.2,"location":2,"content":"then this is kind of the same logic."},{"from":4069.2,"to":4074.76,"location":2,"content":"It's saying that, uh, your lower RNNs might be computing lower-level features like,"},{"from":4074.76,"to":4076.77,"location":2,"content":"let's suppose maybe it's keeping track of syntax,"},{"from":4076.77,"to":4082.1,"location":2,"content":"and your higher  level RNN's gonna compute higher-level features like maybe semantics."},{"from":4082.1,"to":4086.78,"location":2,"content":"And a note on terminology, these are sometimes called stacked RNNs."},{"from":4086.78,"to":4089.64,"location":2,"content":"So, this works much as you'd imagine."},{"from":4089.64,"to":4093.63,"location":2,"content":"So here's an example of how a multi-layer RNN might work."},{"from":4093.63,"to":4095.61,"location":2,"content":"Uh, if it's three layers."},{"from":4095.61,"to":4098.11,"location":2,"content":"So this is a unidirectional RNN,"},{"from":4098.11,"to":4100.53,"location":2,"content":"but it could be bidirectional,"},{"from":4100.53,"to":4103.68,"location":2,"content":"um, If you have access to the entire input sequence."},{"from":4103.68,"to":4109.29,"location":2,"content":"So, I guess the, the main thing is that the hidden states from one RNN layer are going to"},{"from":4109.29,"to":4115.4,"location":2,"content":"be used as the inputs to the RNN layer that's coming next."},{"from":4115.4,"to":4118.8,"location":2,"content":"Um, any questions on this?"},{"from":4118.8,"to":4125.27,"location":2,"content":"Yeah."},{"from":4125.27,"to":4126.45,"location":2,"content":"[inaudible]."},{"from":4126.45,"to":4129.45,"location":2,"content":"That's a great question. So the question I think it's about the order of computation."},{"from":4129.45,"to":4132.39,"location":2,"content":"What order will you compute all of these hidden states in?"},{"from":4132.39,"to":4136.1,"location":2,"content":"I suppose there's some flexibility, right?"},{"from":4136.1,"to":4139.64,"location":2,"content":"But you could compute all of the step one ones,"},{"from":4139.64,"to":4142.3,"location":2,"content":"like all of the V ones and then all of the movie ones,"},{"from":4142.3,"to":4145.97,"location":2,"content":"or you could do all of RNN layer one and then all of RNN layer two."},{"from":4145.97,"to":4148.88,"location":2,"content":"So, it's- I think that, um, when you- you know,"},{"from":4148.88,"to":4151.06,"location":2,"content":"call the PyTorch function to do a multi-layer RNN,"},{"from":4151.06,"to":4153.38,"location":2,"content":"it will do all of RNN layer one, then two, then three."},{"from":4153.38,"to":4154.58,"location":2,"content":"That's what I think happens."},{"from":4154.58,"to":4156.27,"location":2,"content":"But it seems like logically,"},{"from":4156.27,"to":4159,"location":2,"content":"there's no reason why you couldn't do it the other way."},{"from":4159,"to":4170.19,"location":2,"content":"Yep?  [inaudible]."},{"from":4170.19,"to":4172.11,"location":2,"content":"Yes, yes. That's a great point as well."},{"from":4172.11,"to":4175.95,"location":2,"content":"Um, so uh, someone pointed out that if they were bidirectional,"},{"from":4175.95,"to":4177.48,"location":2,"content":"then you no longer have that flexibility."},{"from":4177.48,"to":4179.95,"location":2,"content":"You would have to do all of layer one before layer two."},{"from":4179.95,"to":4187.04,"location":2,"content":"Yeah, good point. Anyone else?"},{"from":4187.04,"to":4192.48,"location":2,"content":"Okay. Uh, so mostly RNNs in practice,"},{"from":4192.48,"to":4196.06,"location":2,"content":"um, this tends to perform pretty well,"},{"from":4196.06,"to":4198.09,"location":2,"content":"uh, in that when I look at, um,"},{"from":4198.09,"to":4200.97,"location":2,"content":"RNN-based systems that are doing very well on some kind of task,"},{"from":4200.97,"to":4204.33,"location":2,"content":"they usually are some kind of multi-layer RNN, um,"},{"from":4204.33,"to":4206.4,"location":2,"content":"but they certainly aren't as deep as"},{"from":4206.4,"to":4209.43,"location":2,"content":"the deep convolutional or feed-forward networks you might have seen in,"},{"from":4209.43,"to":4210.8,"location":2,"content":"for example, image tasks."},{"from":4210.8,"to":4212.55,"location":2,"content":"So whereas, you know, very deep convolutional networks,"},{"from":4212.55,"to":4214.47,"location":2,"content":"I think hundreds of layers now, um,"},{"from":4214.47,"to":4216.8,"location":2,"content":"you certainly aren't getting RNNs that are that deep."},{"from":4216.8,"to":4218.79,"location":2,"content":"So, for example, um,"},{"from":4218.79,"to":4222.3,"location":2,"content":"in this paper from, uh, Google, uh,"},{"from":4222.3,"to":4225.16,"location":2,"content":"they're doing this kind of large hyperparameter search for"},{"from":4225.16,"to":4229.74,"location":2,"content":"neural machine translation to find which kinds of hyperparameters work well for NMT."},{"from":4229.74,"to":4231.72,"location":2,"content":"And in this paper, they found that um,"},{"from":4231.72,"to":4234.16,"location":2,"content":"two to four layers was best for the encoder RNN,"},{"from":4234.16,"to":4236.28,"location":2,"content":"and four layers was best for the decoder RNN."},{"from":4236.28,"to":4239.43,"location":2,"content":"Uh, you'll find out more about what encoder and decoder mean next time."},{"from":4239.43,"to":4241.27,"location":2,"content":"Um, but those are fairly small numbers."},{"from":4241.27,"to":4243.33,"location":2,"content":"Although they did find that if you add these skip"},{"from":4243.33,"to":4245.35,"location":2,"content":"connections or these dense connections, um,"},{"from":4245.35,"to":4249.75,"location":2,"content":"then it makes it much easier to learn some even deeper RNNs more effectively,"},{"from":4249.75,"to":4251.06,"location":2,"content":"like, maybe up to eight layers,"},{"from":4251.06,"to":4253.6,"location":2,"content":"but these certainly aren'tx  hundreds of layers deep."},{"from":4253.6,"to":4255.75,"location":2,"content":"And one of the reasons why, uh,"},{"from":4255.75,"to":4259.1,"location":2,"content":"RNNs don't tend to be nearly as deep as these other kinds of networks,"},{"from":4259.1,"to":4261.68,"location":2,"content":"is that because as we commented before,"},{"from":4261.68,"to":4263.2,"location":2,"content":"RNNs have to be computed, uh,"},{"from":4263.2,"to":4265.38,"location":2,"content":"sequentially; they can't be computed in parallel."},{"from":4265.38,"to":4267.33,"location":2,"content":"This means that they're pretty expensive to compute."},{"from":4267.33,"to":4269.52,"location":2,"content":"If you have this depth in like, two-dimensions,"},{"from":4269.52,"to":4273.68,"location":2,"content":"you have the depth over the timesteps and then the depth over the RNN layer is two,"},{"from":4273.68,"to":4275.16,"location":2,"content":"then it beco- it becomes very,"},{"from":4275.16,"to":4277.83,"location":2,"content":"very expensive to compute these, these RNNs."},{"from":4277.83,"to":4279.89,"location":2,"content":"So, that's another reason why they don't get very deep."},{"from":4279.89,"to":4283.29,"location":2,"content":"Uh, so again, we just mentioned transformers."},{"from":4283.29,"to":4285.16,"location":2,"content":"Uh, you gonna learn about transformers later."},{"from":4285.16,"to":4287.28,"location":2,"content":"But these, it seems, um,"},{"from":4287.28,"to":4290.4,"location":2,"content":"can be deeper fro- from what I can tell of,"},{"from":4290.4,"to":4291.9,"location":2,"content":"of what people are using these days."},{"from":4291.9,"to":4293.58,"location":2,"content":"Transformer-based networks can be pretty deep."},{"from":4293.58,"to":4295.53,"location":2,"content":"So, uh, but for example,"},{"from":4295.53,"to":4298.43,"location":2,"content":"there's a 24-layer version and a 12-layer version, um,"},{"from":4298.43,"to":4299.82,"location":2,"content":"and admittedly, that was trained by Google,"},{"from":4299.82,"to":4301.86,"location":2,"content":"and they have a lot of computational power."},{"from":4301.86,"to":4303.6,"location":2,"content":"Um, but I think part of the reason why"},{"from":4303.6,"to":4305.69,"location":2,"content":"these transformer-based networks can be quite deep,"},{"from":4305.69,"to":4308.22,"location":2,"content":"is that they have a lot of these skipping like connections."},{"from":4308.22,"to":4309.75,"location":2,"content":"In fact, the whole um,"},{"from":4309.75,"to":4312.57,"location":2,"content":"innovation of transformers is that they're built on a lot of, kind of,"},{"from":4312.57,"to":4317.52,"location":2,"content":"skip connections. Okay, any questions?"},{"from":4317.52,"to":4321.45,"location":2,"content":"We're almost done. Okay. All right."},{"from":4321.45,"to":4324.03,"location":2,"content":"So, uh, here's a summary of what we've learned today."},{"from":4324.03,"to":4325.97,"location":2,"content":"I know it's been a lot of information."},{"from":4325.97,"to":4331.53,"location":2,"content":"Um, but I think here are four practical takeaways from today that, uh,"},{"from":4331.53,"to":4333.8,"location":2,"content":"are probably useful to you in your projects,"},{"from":4333.8,"to":4334.92,"location":2,"content":"even if you, um,"},{"from":4334.92,"to":4337.8,"location":2,"content":"uh, even if you"},{"from":4337.8,"to":4341.13,"location":2,"content":"didn't find them very interesting in themselves they're probably pretty useful."},{"from":4341.13,"to":4344.19,"location":2,"content":"So, the first one is that LSTMs are very powerful."},{"from":4344.19,"to":4345.99,"location":2,"content":"They're certainly a lot powerful than,"},{"from":4345.99,"to":4347.91,"location":2,"content":"uh, more powerful than Vanila RNNs."},{"from":4347.91,"to":4351.45,"location":2,"content":"Um, GRUs are also more powerful than, uh, Vanila RNNs."},{"from":4351.45,"to":4354.21,"location":2,"content":"Uh, and the only difference that is consistently the"},{"from":4354.21,"to":4357.48,"location":2,"content":"same is that GRUs are faster than LSTMs."},{"from":4357.48,"to":4360.47,"location":2,"content":"The next one is that you should probably clip your gradients,"},{"from":4360.47,"to":4362.06,"location":2,"content":"because if you don't clip your gradients,"},{"from":4362.06,"to":4367.05,"location":2,"content":"you're in danger of walking off cliffs and then ending up with NaNs in your model."},{"from":4367.05,"to":4372.1,"location":2,"content":"Uh, the next tip is that bidirectionality is useful if you can apply it."},{"from":4372.1,"to":4376.13,"location":2,"content":"And, basically, anytime when you have access to the entire input sequence,"},{"from":4376.13,"to":4377.85,"location":2,"content":"you can apply bidirectionality,"},{"from":4377.85,"to":4379.79,"location":2,"content":"so you should probably do that by default."},{"from":4379.79,"to":4384.14,"location":2,"content":"And then the last tip is that multi-layer RNNs are pretty powerful."},{"from":4384.14,"to":4386.33,"location":2,"content":"And again, you should probably do that if you,"},{"from":4386.33,"to":4388.35,"location":2,"content":"uh, have enough computational power to do so."},{"from":4388.35,"to":4391.4,"location":2,"content":"But if you're going to make your multi-layer RNN pretty deep,"},{"from":4391.4,"to":4393.26,"location":2,"content":"then you might need skip connections."},{"from":4393.26,"to":4402.84,"location":2,"content":"All right. Thanks [NOISE]."}]}