{"font_size":0.4,"font_color":"#FFFFFF","background_alpha":0.5,"background_color":"#9C27B0","Stroke":"none","body":[{"from":4.57,"to":10.11,"location":2,"content":"okay hi everyone okay let's get started"},{"from":10.11,"to":12.88,"location":2,"content":"great to see you all here welcome back"},{"from":12.88,"to":20.05,"location":2,"content":"for week 2 our CS 224 in so so this is a"},{"from":20.05,"to":22.54,"location":2,"content":"little preview of what's coming up in"},{"from":22.54,"to":25.71,"location":2,"content":"the class for this week and next week"},{"from":25.71,"to":28.36,"location":2,"content":"you know this week is perhaps the worst"},{"from":28.36,"to":31.75,"location":2,"content":"week of this class so in week two of the"},{"from":31.75,"to":36.73,"location":2,"content":"class our hope is to actually kind of go"},{"from":36.73,"to":39.07,"location":2,"content":"through some of the nitty-gritty of"},{"from":39.07,"to":42.7,"location":2,"content":"neural networks and how they're trained"},{"from":42.7,"to":46.42,"location":2,"content":"and how we can learn good neural"},{"from":46.42,"to":49.3,"location":2,"content":"networks by backpropagation which means"},{"from":49.3,"to":51.55,"location":2,"content":"in particular we're going to be sort of"},{"from":51.55,"to":53.32,"location":2,"content":"talking about the training algorithms"},{"from":53.32,"to":56.59,"location":2,"content":"and doing calculus to work out gradients"},{"from":56.59,"to":60.37,"location":2,"content":"for improving them so we'll look a bit a"},{"from":60.37,"to":65.32,"location":2,"content":"little bit word window classification"},{"from":65.32,"to":67.51,"location":2,"content":"named entity recognition so there's a"},{"from":67.51,"to":69.79,"location":2,"content":"teeny bit of natural language processing"},{"from":69.79,"to":73.57,"location":2,"content":"in there but basically sort of week two"},{"from":73.57,"to":78.07,"location":2,"content":"is sort of math of deep learning and"},{"from":78.07,"to":80.62,"location":2,"content":"neural network models and sort of really"},{"from":80.62,"to":83.74,"location":2,"content":"knew all Network fundamentals but the"},{"from":83.74,"to":86.17,"location":2,"content":"hope is that that will give you kind of"},{"from":86.17,"to":88.69,"location":2,"content":"good understanding of how these things"},{"from":88.69,"to":90.79,"location":2,"content":"really work and will give you all the"},{"from":90.79,"to":93.37,"location":2,"content":"information you need to do the coming-up"},{"from":93.37,"to":97.15,"location":2,"content":"homework and so then in week 3 we kind"},{"from":97.15,"to":100.48,"location":2,"content":"of flip so then week three is going to"},{"from":100.48,"to":102.73,"location":2,"content":"be mainly about natural language"},{"from":102.73,"to":104.53,"location":2,"content":"processing so we're then going to talk"},{"from":104.53,"to":106.72,"location":2,"content":"about how to port syntactic structures"},{"from":106.72,"to":109.57,"location":2,"content":"over sentences for building dependency"},{"from":109.57,"to":111.7,"location":2,"content":"parsers of sentences which is then"},{"from":111.7,"to":114.4,"location":2,"content":"actually what's used in homework 3 so"},{"from":114.4,"to":116.38,"location":2,"content":"we're chugging along rapidly and then"},{"from":116.38,"to":118.36,"location":2,"content":"we'll talk about this idea of the"},{"from":118.36,"to":120.31,"location":2,"content":"probability of a sentence which leads"},{"from":120.31,"to":124.78,"location":2,"content":"into neural language models so on the"},{"from":124.78,"to":127.32,"location":2,"content":"homeworks homework 1 was due"},{"from":127.32,"to":130.36,"location":2,"content":"approximately two minutes ago so I hope"},{"from":130.36,"to":132.97,"location":2,"content":"everyone has submitted their homework"},{"from":132.97,"to":138.73,"location":2,"content":"one I mean as one just sort of add"},{"from":138.73,"to":141.37,"location":2,"content":"in general I thought you know homework 1"},{"from":141.37,"to":144.01,"location":2,"content":"we hope you found was a good warm-up and"},{"from":144.01,"to":146.56,"location":2,"content":"not too too hard and so it really be"},{"from":146.56,"to":149.26,"location":2,"content":"best to get homework 1 in quickly rather"},{"from":149.26,"to":151.3,"location":2,"content":"than to burn lots of your late days"},{"from":151.3,"to":154.33,"location":2,"content":"doing homework 1 and now right now out"},{"from":154.33,"to":157.75,"location":2,"content":"on the website there's homework too so"},{"from":157.75,"to":159.82,"location":2,"content":"we're chugging along"},{"from":159.82,"to":162.49,"location":2,"content":"so homework 2 kind of corresponds to"},{"from":162.49,"to":164.65,"location":2,"content":"this week's lectures so on the first"},{"from":164.65,"to":165.49,"location":2,"content":"part of it"},{"from":165.49,"to":168.19,"location":2,"content":"we are expecting you to grind through"},{"from":168.19,"to":170.17,"location":2,"content":"some math problems of working out"},{"from":170.17,"to":173.56,"location":2,"content":"gradient derivations and then the second"},{"from":173.56,"to":176.23,"location":2,"content":"part of it is then implementing your own"},{"from":176.23,"to":178.39,"location":2,"content":"version of word Tyvek making use of"},{"from":178.39,"to":181.39,"location":2,"content":"numpy and so this time sort of writing a"},{"from":181.39,"to":183.25,"location":2,"content":"Python program it's no longer an I"},{"from":183.25,"to":186.34,"location":2,"content":"Python notebook encourage you to get"},{"from":186.34,"to":191.83,"location":2,"content":"early look at the materials on the web I"},{"from":191.83,"to":193.66,"location":2,"content":"mean in particular corresponding to"},{"from":193.66,"to":196.39,"location":2,"content":"today's lecture there's some quite good"},{"from":196.39,"to":199.03,"location":2,"content":"tutorial materials that are available on"},{"from":199.03,"to":201.13,"location":2,"content":"the website and so also encourage you to"},{"from":201.13,"to":204.94,"location":2,"content":"look at those more generally just to"},{"from":204.94,"to":207.85,"location":2,"content":"make a couple more comments on things I"},{"from":207.85,"to":211,"location":2,"content":"mean I guess this is true of a lot of"},{"from":211,"to":213.94,"location":2,"content":"classes at Stanford but you know when we"},{"from":213.94,"to":216.76,"location":2,"content":"get the course reviews for this class we"},{"from":216.76,"to":219.19,"location":2,"content":"always get the full spectrum from people"},{"from":219.19,"to":221.35,"location":2,"content":"who say the class is terrible and it's"},{"from":221.35,"to":223.96,"location":2,"content":"way too much work to people who say it's"},{"from":223.96,"to":225.37,"location":2,"content":"a really great class one of their"},{"from":225.37,"to":226.87,"location":2,"content":"favorite classes at Stanford"},{"from":226.87,"to":229.78,"location":2,"content":"obviously instructors care etc and I"},{"from":229.78,"to":232.09,"location":2,"content":"mean probably this reflects that we get"},{"from":232.09,"to":235.18,"location":2,"content":"this very wide range of people coming to"},{"from":235.18,"to":237.7,"location":2,"content":"take this class or on the one hand on"},{"from":237.7,"to":239.71,"location":2,"content":"the right-hand margin perhaps we have"},{"from":239.71,"to":242.11,"location":2,"content":"the physics PhDs and on the left-hand"},{"from":242.11,"to":244.51,"location":2,"content":"margin we have some frosh who think this"},{"from":244.51,"to":246.28,"location":2,"content":"will be fun to do anyway"},{"from":246.28,"to":249.46,"location":2,"content":"we welcome it we welcome everybody but"},{"from":249.46,"to":251.83,"location":2,"content":"in principle this is a graduate level"},{"from":251.83,"to":254.35,"location":2,"content":"class you know that doesn't mean we want"},{"from":254.35,"to":256.51,"location":2,"content":"to fail people out we'd like everyone to"},{"from":256.51,"to":259.54,"location":2,"content":"succeed but also like graduate level"},{"from":259.54,"to":262.45,"location":2,"content":"class we'd like you to here take some"},{"from":262.45,"to":265.21,"location":2,"content":"initiative in your success meaning if"},{"from":265.21,"to":266.65,"location":2,"content":"there are things that you need to know"},{"from":266.65,"to":268.63,"location":2,"content":"to do the assignments and you don't know"},{"from":268.63,"to":271.54,"location":2,"content":"them then you should be taking some"},{"from":271.54,"to":271.87,"location":2,"content":"initial"},{"from":271.87,"to":274.36,"location":2,"content":"to find some tutorials come to office"},{"from":274.36,"to":277.21,"location":2,"content":"hours and talk to people and get any"},{"from":277.21,"to":280.24,"location":2,"content":"help you need and learn to sort of for"},{"from":280.24,"to":282.28,"location":2,"content":"any holes in your knowledge okay so"},{"from":282.28,"to":285.73,"location":2,"content":"here's the plan for today so that was"},{"from":285.73,"to":287.89,"location":2,"content":"the course information update so you"},{"from":287.89,"to":290.56,"location":2,"content":"know this is sort of in some sense you"},{"from":290.56,"to":292.6,"location":2,"content":"know machine learning neural nets intro"},{"from":292.6,"to":295,"location":2,"content":"just to try and make sure everyone is up"},{"from":295,"to":296.95,"location":2,"content":"to speed on all this stuff so talk a"},{"from":296.95,"to":299.16,"location":2,"content":"little bit about classification"},{"from":299.16,"to":302.62,"location":2,"content":"introduced neural networks little detour"},{"from":302.62,"to":305.11,"location":2,"content":"and named entity recognition in sort of"},{"from":305.11,"to":309.49,"location":2,"content":"show a model of doing window would"},{"from":309.49,"to":312.19,"location":2,"content":"window classification and then at the"},{"from":312.19,"to":315.07,"location":2,"content":"end part we sort of then dive deeper"},{"from":315.07,"to":318.22,"location":2,"content":"into what kind of tools we need to learn"},{"from":318.22,"to":321.58,"location":2,"content":"neural networks and so today we're going"},{"from":321.58,"to":324.94,"location":2,"content":"to go through somewhere between review"},{"from":324.94,"to":328.66,"location":2,"content":"and primer of matrix calculus and then"},{"from":328.66,"to":331.21,"location":2,"content":"that will lead into next times lecture"},{"from":331.21,"to":333.79,"location":2,"content":"where it's talking more about back"},{"from":333.79,"to":337.69,"location":2,"content":"propagation and computation graphs so"},{"from":337.69,"to":341.77,"location":2,"content":"yeah so this material especially the"},{"from":341.77,"to":343.56,"location":2,"content":"part at the end you know for some people"},{"from":343.56,"to":347.05,"location":2,"content":"it'll seem really babyish if it's the"},{"from":347.05,"to":349.51,"location":2,"content":"kind of stuff you do every week for"},{"from":349.51,"to":352.36,"location":2,"content":"other people it might seem impossibly"},{"from":352.36,"to":354.64,"location":2,"content":"difficult but hopefully for a large"},{"from":354.64,"to":356.44,"location":2,"content":"percentage of you in the middle this"},{"from":356.44,"to":359.71,"location":2,"content":"will be kind of a useful review of doing"},{"from":359.71,"to":361.84,"location":2,"content":"this kind of matrix calculus and the"},{"from":361.84,"to":363.4,"location":2,"content":"kind of things that we hope that you can"},{"from":363.4,"to":370.51,"location":2,"content":"do on homework 2 ok so yeah so sorry if"},{"from":370.51,"to":372.43,"location":2,"content":"I'm boring some people if you set"},{"from":372.43,"to":376.72,"location":2,"content":"through 229 last quarter you saw what a"},{"from":376.72,"to":379.39,"location":2,"content":"classifier was like and hopefully this"},{"from":379.39,"to":381.79,"location":2,"content":"will seem familiar but I'm just sort of"},{"from":381.79,"to":384.1,"location":2,"content":"hoping to try and have everyone in week"},{"from":384.1,"to":386.2,"location":2,"content":"2 sort of up to speed and I'm roughly"},{"from":386.2,"to":387.91,"location":2,"content":"the same page so here's our"},{"from":387.91,"to":390.25,"location":2,"content":"classification setup so we have assumed"},{"from":390.25,"to":393.28,"location":2,"content":"we have training data set where we have"},{"from":393.28,"to":397.93,"location":2,"content":"these vector X of our X points and then"},{"from":397.93,"to":400.71,"location":2,"content":"for each of one of them we have a class"},{"from":400.71,"to":404.23,"location":2,"content":"so the inputs might be words or"},{"from":404.23,"to":405.67,"location":2,"content":"sentences documents or"},{"from":405.67,"to":407.89,"location":2,"content":"something there a d-dimensional vector"},{"from":407.89,"to":411.67,"location":2,"content":"the weii are the labels or classes that"},{"from":411.67,"to":414.37,"location":2,"content":"we want to classify - and we've got a"},{"from":414.37,"to":416.32,"location":2,"content":"set of c classes that we're trying to"},{"from":416.32,"to":418.93,"location":2,"content":"predict and so those might be something"},{"from":418.93,"to":420.85,"location":2,"content":"like the topic of a document the"},{"from":420.85,"to":423.85,"location":2,"content":"sentiment positive or negative of a"},{"from":423.85,"to":426.04,"location":2,"content":"document or later we'll look a bit more"},{"from":426.04,"to":432.06,"location":2,"content":"named entities okay so if we have that"},{"from":432.06,"to":436.36,"location":2,"content":"for this sort of intuition is we've got"},{"from":436.36,"to":438.46,"location":2,"content":"this vector space of which we again have"},{"from":438.46,"to":441.52,"location":2,"content":"a 2d picture and we have points in that"},{"from":441.52,"to":443.95,"location":2,"content":"vector space which correspond to our X"},{"from":443.95,"to":447.76,"location":2,"content":"items and what we'd want to do is look"},{"from":447.76,"to":450.43,"location":2,"content":"at the ones and our training sample and"},{"from":450.43,"to":452.38,"location":2,"content":"see which ones are green and red for our"},{"from":452.38,"to":454.6,"location":2,"content":"two classes here and then we want to"},{"from":454.6,"to":457.6,"location":2,"content":"sort of learn a line that could divide"},{"from":457.6,"to":460.66,"location":2,"content":"between the green and the red ones as"},{"from":460.66,"to":463.09,"location":2,"content":"best as possible and that learn'd line"},{"from":463.09,"to":467.17,"location":2,"content":"is our classifier so on traditional"},{"from":467.17,"to":469.6,"location":2,"content":"machine learning or statistics we have"},{"from":469.6,"to":471.73,"location":2,"content":"the sort of X I vectors or our data"},{"from":471.73,"to":474.94,"location":2,"content":"items that are purely fixed but we're"},{"from":474.94,"to":480.75,"location":2,"content":"going to then multiply those X I by some"},{"from":480.75,"to":483.46,"location":2,"content":"estimated weight vector and that"},{"from":483.46,"to":485.92,"location":2,"content":"estimated weight vector will then go"},{"from":485.92,"to":488.5,"location":2,"content":"into a classification decision and the"},{"from":488.5,"to":490.75,"location":2,"content":"classifier that I'm showing here is a"},{"from":490.75,"to":493.54,"location":2,"content":"soft max classifier which is almost"},{"from":493.54,"to":495.55,"location":2,"content":"identical but not quite to a logistic"},{"from":495.55,"to":497.68,"location":2,"content":"regression classifier which you should"},{"from":497.68,"to":500.77,"location":2,"content":"have seen in CS 109 or a stats class or"},{"from":500.77,"to":502.78,"location":2,"content":"something like that which is giving a"},{"from":502.78,"to":506.95,"location":2,"content":"probability of different classes okay"},{"from":506.95,"to":510.16,"location":2,"content":"and in particular if you've got a soft"},{"from":510.16,"to":513.67,"location":2,"content":"max classifier or a logistic distich"},{"from":513.67,"to":515.8,"location":2,"content":"regression classifier these are what"},{"from":515.8,"to":518.2,"location":2,"content":"accordin linear classifiers so the"},{"from":518.2,"to":520.96,"location":2,"content":"decision boundary between two classes"},{"from":520.96,"to":524.8,"location":2,"content":"here is a line in some suitably high"},{"from":524.8,"to":526.78,"location":2,"content":"dimensional space so it's a plane or a"},{"from":526.78,"to":529.33,"location":2,"content":"hyperplane once you've got a bigger X"},{"from":529.33,"to":533.62,"location":2,"content":"vector okay so here's our soft max"},{"from":533.62,"to":536.53,"location":2,"content":"classifier and there are sort of two"},{"from":536.53,"to":539.47,"location":2,"content":"parts to that so in the"},{"from":539.47,"to":543.63,"location":2,"content":"in the weight matrix W we have a row"},{"from":543.63,"to":547.51,"location":2,"content":"corresponding to each class and then for"},{"from":547.51,"to":550.78,"location":2,"content":"that row we're sort of dot producting it"},{"from":550.78,"to":554.26,"location":2,"content":"with our data point vector X I and"},{"from":554.26,"to":557.14,"location":2,"content":"that's giving us a kind of a score for"},{"from":557.14,"to":559.42,"location":2,"content":"how likely it is that the example"},{"from":559.42,"to":561.94,"location":2,"content":"belongs to that class and then we're"},{"from":561.94,"to":564.16,"location":2,"content":"running that through a softmax function"},{"from":564.16,"to":567.28,"location":2,"content":"and just as we saw in week one this"},{"from":567.28,"to":569.98,"location":2,"content":"softmax takes a bunch of numbers and"},{"from":569.98,"to":571.3,"location":2,"content":"turns them into a probability"},{"from":571.3,"to":574,"location":2,"content":"distribution does that make sense to"},{"from":574,"to":575.74,"location":2,"content":"people people remember that from last"},{"from":575.74,"to":579.04,"location":2,"content":"week good so far okay"},{"from":579.04,"to":581.47,"location":2,"content":"I'm not going to go through this in"},{"from":581.47,"to":587.2,"location":2,"content":"detail but I mean essentially this is"},{"from":587.2,"to":589.89,"location":2,"content":"what a logistic regression does as well"},{"from":589.89,"to":593.95,"location":2,"content":"the difference is that here in this"},{"from":593.95,"to":599.62,"location":2,"content":"setup we have a weight vector for each"},{"from":599.62,"to":603.97,"location":2,"content":"class whereas what the statisticians do"},{"from":603.97,"to":607.09,"location":2,"content":"in logistic regression is they say wait"},{"from":607.09,"to":610.45,"location":2,"content":"that gives us one more number of weight"},{"from":610.45,"to":613,"location":2,"content":"vectors than we really need we can get"},{"from":613,"to":616.24,"location":2,"content":"away but for C classes we can get away"},{"from":616.24,"to":618.91,"location":2,"content":"with C minus one weight vectors so in"},{"from":618.91,"to":619.99,"location":2,"content":"particular if you're doing binary"},{"from":619.99,"to":622.36,"location":2,"content":"logistic regression you only need one"},{"from":622.36,"to":624.4,"location":2,"content":"weight vector whereas this softmax"},{"from":624.4,"to":626.44,"location":2,"content":"regression formulation you've actually"},{"from":626.44,"to":628.21,"location":2,"content":"got two weight vectors one for each"},{"from":628.21,"to":630.28,"location":2,"content":"class so there's that some little"},{"from":630.28,"to":632.08,"location":2,"content":"difference there which we could get into"},{"from":632.08,"to":634.45,"location":2,"content":"but basically the same let's just say"},{"from":634.45,"to":636.79,"location":2,"content":"it's we either doing softmax or logistic"},{"from":636.79,"to":640.33,"location":2,"content":"regression doesn't matter so when we're"},{"from":640.33,"to":644.38,"location":2,"content":"training what we want to do is we want"},{"from":644.38,"to":648.13,"location":2,"content":"to be able to predict the correct class"},{"from":648.13,"to":651.43,"location":2,"content":"and so the way we're going to do that is"},{"from":651.43,"to":653.23,"location":2,"content":"we're going to want to train our model"},{"from":653.23,"to":655.63,"location":2,"content":"so it gives us highest probability as"},{"from":655.63,"to":658.39,"location":2,"content":"possible to the correct class and"},{"from":658.39,"to":661.12,"location":2,"content":"therefore it'll give us lo a probability"},{"from":661.12,"to":665.35,"location":2,"content":"play as possible to the wrong classes"},{"from":665.35,"to":669.4,"location":2,"content":"and so our criterion for doing that is"},{"from":669.4,"to":672.31,"location":2,"content":"we're going to create this negative log"},{"from":672.31,"to":673.21,"location":2,"content":"probability"},{"from":673.21,"to":676.27,"location":2,"content":"the of our assignments and then we're"},{"from":676.27,"to":678.16,"location":2,"content":"going to want to minimize the negative"},{"from":678.16,"to":680.77,"location":2,"content":"log probability which corresponds to"},{"from":680.77,"to":683.38,"location":2,"content":"maximizing the log probability which"},{"from":683.38,"to":685.06,"location":2,"content":"corresponds to maximizing the"},{"from":685.06,"to":693.37,"location":2,"content":"probability and but sort of pretty soon"},{"from":693.37,"to":695.2,"location":2,"content":"now we're going to start doing more"},{"from":695.2,"to":697.54,"location":2,"content":"stuff with deep learning frameworks in"},{"from":697.54,"to":700.42,"location":2,"content":"particular PI torch and you can discover"},{"from":700.42,"to":702.37,"location":2,"content":"in that that there's actually a thing"},{"from":702.37,"to":704.98,"location":2,"content":"called nll loss which stands for"},{"from":704.98,"to":707.68,"location":2,"content":"negative log likelihood loss but"},{"from":707.68,"to":709.6,"location":2,"content":"basically no one uses that because the"},{"from":709.6,"to":711.79,"location":2,"content":"more convenient thing to use is what's"},{"from":711.79,"to":714.28,"location":2,"content":"called the cross entropy loss and so"},{"from":714.28,"to":715.99,"location":2,"content":"you'll hear everywhere that we're"},{"from":715.99,"to":718.18,"location":2,"content":"training with cross entropy loss so I"},{"from":718.18,"to":720.37,"location":2,"content":"just wanted to briefly mention that and"},{"from":720.37,"to":723.88,"location":2,"content":"explain what's going on there so the"},{"from":723.88,"to":726.73,"location":2,"content":"concept of cross entropy comes from baby"},{"from":726.73,"to":728.86,"location":2,"content":"information theory which is about the"},{"from":728.86,"to":731.38,"location":2,"content":"amount of information theory I know so"},{"from":731.38,"to":733.66,"location":2,"content":"we're assuming that there's some true"},{"from":733.66,"to":737.62,"location":2,"content":"probability distribution P and our model"},{"from":737.62,"to":739.45,"location":2,"content":"we've built some probability"},{"from":739.45,"to":741.73,"location":2,"content":"distribution Q that's what we've built"},{"from":741.73,"to":744.25,"location":2,"content":"with our softmax regression and we want"},{"from":744.25,"to":747.15,"location":2,"content":"to have a measure of whether our"},{"from":747.15,"to":749.74,"location":2,"content":"estimated probability distribution is"},{"from":749.74,"to":751.81,"location":2,"content":"the good one and the way we do it and"},{"from":751.81,"to":754.54,"location":2,"content":"cross entropy is we go through the"},{"from":754.54,"to":756.7,"location":2,"content":"classes and we say what's the"},{"from":756.7,"to":758.62,"location":2,"content":"probability of the class according to"},{"from":758.62,"to":761.89,"location":2,"content":"the true model using that weighting we"},{"from":761.89,"to":764.95,"location":2,"content":"then work out the log of the probability"},{"from":764.95,"to":768.34,"location":2,"content":"according to our estimated model and we"},{"from":768.34,"to":770.89,"location":2,"content":"sum those up and negate it and that is"},{"from":770.89,"to":776.65,"location":2,"content":"our cross entropy measure okay but so"},{"from":776.65,"to":781,"location":2,"content":"this in general gives you a measure of"},{"from":781,"to":785.22,"location":2,"content":"sort of information between"},{"from":785.22,"to":788.73,"location":2,"content":"distributions but in our particular case"},{"from":788.73,"to":791.95,"location":2,"content":"remember that for each example we've"},{"from":791.95,"to":794.05,"location":2,"content":"sort of assuming that this is a piece of"},{"from":794.05,"to":796.69,"location":2,"content":"label training data so we're saying for"},{"from":796.69,"to":799.48,"location":2,"content":"that example the right answer is class"},{"from":799.48,"to":802.9,"location":2,"content":"seven so therefore our true distribution"},{"from":802.9,"to":806.92,"location":2,"content":"our P is for this example"},{"from":806.92,"to":810.01,"location":2,"content":"class seven with probability one and its"},{"from":810.01,"to":811.21,"location":2,"content":"class"},{"from":811.21,"to":814.27,"location":2,"content":"anything else with probability zero so"},{"from":814.27,"to":816.58,"location":2,"content":"if you think about then what happens"},{"from":816.58,"to":818.44,"location":2,"content":"with this formula you've got this"},{"from":818.44,"to":821.2,"location":2,"content":"summation over all the classes but P of"},{"from":821.2,"to":823.6,"location":2,"content":"C is gonna be either one or zero and"},{"from":823.6,"to":826.12,"location":2,"content":"it's going to be one only for the true"},{"from":826.12,"to":828.94,"location":2,"content":"class here and so what you're left with"},{"from":828.94,"to":831.61,"location":2,"content":"is this is going to equal minus the log"},{"from":831.61,"to":836.5,"location":2,"content":"of you see for the true class which is"},{"from":836.5,"to":839.29,"location":2,"content":"sort of what we were then computing in"},{"from":839.29,"to":843.81,"location":2,"content":"the previous slide okay so that's what"},{"from":843.81,"to":846.37,"location":2,"content":"yeah so that's basically where you get"},{"from":846.37,"to":847.27,"location":2,"content":"with cross-entropy"},{"from":847.27,"to":852.28,"location":2,"content":"loss but one other concept to mention so"},{"from":852.28,"to":854.59,"location":2,"content":"when you have a full data set of a whole"},{"from":854.59,"to":857.53,"location":2,"content":"bunch of examples the cross-entropy loss"},{"from":857.53,"to":860.86,"location":2,"content":"is then taking the per example average"},{"from":860.86,"to":862.15,"location":2,"content":"so I guess that's what information"},{"from":862.15,"to":864.04,"location":2,"content":"theory people sometimes call the cross"},{"from":864.04,"to":866.71,"location":2,"content":"entropy rate so additionally factored in"},{"from":866.71,"to":868.51,"location":2,"content":"there if you're training it on in"},{"from":868.51,"to":871.57,"location":2,"content":"examples is that one on in factor that's"},{"from":871.57,"to":877.84,"location":2,"content":"coming in there okay okay so that's"},{"from":877.84,"to":882.21,"location":2,"content":"cross entropy loss that okay yeah"},{"from":882.21,"to":884.53,"location":2,"content":"there's some mixture of the actual"},{"from":884.53,"to":888.79,"location":2,"content":"levels in the ground sure right so the"},{"from":888.79,"to":892.54,"location":2,"content":"simplest case is that your gold data"},{"from":892.54,"to":895.72,"location":2,"content":"someone has Hanway water and they've"},{"from":895.72,"to":899.5,"location":2,"content":"labeled one and the rest of zero there"},{"from":899.5,"to":901.66,"location":2,"content":"you can think of cases where that isn't"},{"from":901.66,"to":904.03,"location":2,"content":"the case I mean one case is you could"},{"from":904.03,"to":905.95,"location":2,"content":"believe that human beings sometimes"},{"from":905.95,"to":908.74,"location":2,"content":"don't know the right answer so if human"},{"from":908.74,"to":910.57,"location":2,"content":"beings serum I'm not sure whether this"},{"from":910.57,"to":913.18,"location":2,"content":"should be class three or four you can"},{"from":913.18,"to":915.64,"location":2,"content":"imagine that we can make training data"},{"from":915.64,"to":917.95,"location":2,"content":"where we put probability 1/2 on both of"},{"from":917.95,"to":920.62,"location":2,"content":"them and that wouldn't be a crazy thing"},{"from":920.62,"to":923.8,"location":2,"content":"to do and so then you have a true cross"},{"from":923.8,"to":925.45,"location":2,"content":"entropy loss using more of a"},{"from":925.45,"to":929.83,"location":2,"content":"distribution the case where it's much"},{"from":929.83,"to":933.58,"location":2,"content":"more commonly used in actual practice is"},{"from":933.58,"to":936.67,"location":2,"content":"there are many circumstances in which"},{"from":936.67,"to":938.47,"location":2,"content":"people want to do semi-supervised"},{"from":938.47,"to":940.42,"location":2,"content":"learning so I guess"},{"from":940.42,"to":942.43,"location":2,"content":"a topic that both my group and Chris"},{"from":942.43,"to":944.38,"location":2,"content":"raised group have worked on quite a lot"},{"from":944.38,"to":946.66,"location":2,"content":"where we don't actually have fully"},{"from":946.66,"to":949.6,"location":2,"content":"labeled data but we've got some means of"},{"from":949.6,"to":952.06,"location":2,"content":"guessing what the labels of the data are"},{"from":952.06,"to":954.64,"location":2,"content":"and if we try to guess labels of data"},{"from":954.64,"to":957.16,"location":2,"content":"well then quite often we'll say here's"},{"from":957.16,"to":959.92,"location":2,"content":"this data item it's two-thirds chance as"},{"from":959.92,"to":961.45,"location":2,"content":"this label but it could be these other"},{"from":961.45,"to":963.97,"location":2,"content":"four labels and we'd use a probability"},{"from":963.97,"to":966.01,"location":2,"content":"distribution and yeah then it's more"},{"from":966.01,"to":972.03,"location":2,"content":"general cross-entropy loss okay right so"},{"from":972.03,"to":975.64,"location":2,"content":"that's cross-entropy loss pretty good"},{"from":975.64,"to":978.04,"location":2,"content":"with this bottom bits a little bit"},{"from":978.04,"to":980.56,"location":2,"content":"different which is the say whoa Nelly"},{"from":980.56,"to":982.63,"location":2,"content":"this is the sort of the full data set"},{"from":982.63,"to":986.2,"location":2,"content":"the other thing to notice when we have a"},{"from":986.2,"to":991.78,"location":2,"content":"fool that we can have a full data set of"},{"from":991.78,"to":996.4,"location":2,"content":"X's and then we have a full set of"},{"from":996.4,"to":1000.63,"location":2,"content":"weights we're here we're working a row"},{"from":1000.63,"to":1003.21,"location":2,"content":"vector for the weights for one class but"},{"from":1003.21,"to":1004.38,"location":2,"content":"we're going to work it out for all"},{"from":1004.38,"to":1007.47,"location":2,"content":"classes so we can sort of simplify what"},{"from":1007.47,"to":1009.15,"location":2,"content":"we're writing here and we can sort of"},{"from":1009.15,"to":1011.64,"location":2,"content":"use matrix notation and just work"},{"from":1011.64,"to":1016.74,"location":2,"content":"directly in terms of the matrix W okay"},{"from":1016.74,"to":1021.24,"location":2,"content":"so for traditional ML optimization our"},{"from":1021.24,"to":1025.98,"location":2,"content":"parameters are these sets of weights for"},{"from":1025.98,"to":1028.08,"location":2,"content":"the different classes so for each of the"},{"from":1028.08,"to":1032.55,"location":2,"content":"classes we have a d-dimensional row"},{"from":1032.55,"to":1034.56,"location":2,"content":"vector of weights because we're going to"},{"from":1034.56,"to":1036.56,"location":2,"content":"sort of dot product with our d"},{"from":1036.56,"to":1040.62,"location":2,"content":"dimensional input vector so we have C"},{"from":1040.62,"to":1046.07,"location":2,"content":"times D items and our W matrix and those"},{"from":1046.07,"to":1049.68,"location":2,"content":"the parameters of our model and so if we"},{"from":1049.68,"to":1052.59,"location":2,"content":"want to learn that model using the ideas"},{"from":1052.59,"to":1056.07,"location":2,"content":"of gradient descents it's the casted"},{"from":1056.07,"to":1058.17,"location":2,"content":"gradient descent we're going to do sort"},{"from":1058.17,"to":1059.85,"location":2,"content":"of what we started to talk about last"},{"from":1059.85,"to":1063.45,"location":2,"content":"time we have these set of parameters we"},{"from":1063.45,"to":1067.62,"location":2,"content":"work out the gradient the partial"},{"from":1067.62,"to":1071.91,"location":2,"content":"derivatives of all of these of the loss"},{"from":1071.91,"to":1073.96,"location":2,"content":"with respect to all of these parameter"},{"from":1073.96,"to":1076.78,"location":2,"content":"and we use that to get a gradient update"},{"from":1076.78,"to":1079.84,"location":2,"content":"on our loss function and we move around"},{"from":1079.84,"to":1082.84,"location":2,"content":"the W's and moving around the W's"},{"from":1082.84,"to":1086.08,"location":2,"content":"corresponds to sort of moving this line"},{"from":1086.08,"to":1088.42,"location":2,"content":"that separates between the classes and"},{"from":1088.42,"to":1091.21,"location":2,"content":"we fiddle that around so as to minimize"},{"from":1091.21,"to":1094.45,"location":2,"content":"our loss which corresponds to choosing a"},{"from":1094.45,"to":1097.78,"location":2,"content":"line that best separates between the"},{"from":1097.78,"to":1102.91,"location":2,"content":"items of the classes in some sense okay"},{"from":1102.91,"to":1105.85,"location":2,"content":"so that's a basic classifier so the"},{"from":1105.85,"to":1110.35,"location":2,"content":"first question is well how are things"},{"from":1110.35,"to":1113.29,"location":2,"content":"going to be different with a neural"},{"from":1113.29,"to":1118.03,"location":2,"content":"network classifier and so the essential"},{"from":1118.03,"to":1121.99,"location":2,"content":"observation is that sort of most of the"},{"from":1121.99,"to":1125.17,"location":2,"content":"classic classifiers that people used a"},{"from":1125.17,"to":1127.3,"location":2,"content":"lot of the time so that includes things"},{"from":1127.3,"to":1131.23,"location":2,"content":"like naive Bayes models basic support"},{"from":1131.23,"to":1134.2,"location":2,"content":"vector machines softmax or logistic"},{"from":1134.2,"to":1138.04,"location":2,"content":"regressions they're sort of fairly"},{"from":1138.04,"to":1141.94,"location":2,"content":"simple classifiers in particular those"},{"from":1141.94,"to":1144.31,"location":2,"content":"are all linear classifiers which are"},{"from":1144.31,"to":1147.1,"location":2,"content":"going to classify by drawing a line or"},{"from":1147.1,"to":1148.93,"location":2,"content":"in the high dimensional space by drawing"},{"from":1148.93,"to":1150.94,"location":2,"content":"some kind of plane that separates"},{"from":1150.94,"to":1153.82,"location":2,"content":"examples and having a simple classifier"},{"from":1153.82,"to":1156.84,"location":2,"content":"like that can be useful in certain"},{"from":1156.84,"to":1159.37,"location":2,"content":"circumstances I mean that gives you what"},{"from":1159.37,"to":1161.14,"location":2,"content":"in machine learning as a high biased"},{"from":1161.14,"to":1163.6,"location":2,"content":"classifiers there's lots of talk of in"},{"from":1163.6,"to":1167.02,"location":2,"content":"CS 229 but if you have a data set that's"},{"from":1167.02,"to":1170.41,"location":2,"content":"like this you can't do a very good job"},{"from":1170.41,"to":1173.23,"location":2,"content":"at classifying all the points correctly"},{"from":1173.23,"to":1176.02,"location":2,"content":"if you have a high bias classifier"},{"from":1176.02,"to":1178.27,"location":2,"content":"because you're gonna only draw a line so"},{"from":1178.27,"to":1180.13,"location":2,"content":"you'd like to have a more powerful"},{"from":1180.13,"to":1184.15,"location":2,"content":"classifier and essentially what's been"},{"from":1184.15,"to":1186.55,"location":2,"content":"powering a lot of the use of deep"},{"from":1186.55,"to":1189.82,"location":2,"content":"learning is that in a lot of cases when"},{"from":1189.82,"to":1192.13,"location":2,"content":"you have natural signals so those are"},{"from":1192.13,"to":1195.43,"location":2,"content":"things like speech language images and"},{"from":1195.43,"to":1198.28,"location":2,"content":"things like that you have a ton of data"},{"from":1198.28,"to":1200.79,"location":2,"content":"so you could learn a quite sophisticated"},{"from":1200.79,"to":1206.17,"location":2,"content":"classifier but representing the classes"},{"from":1206.17,"to":1207.79,"location":2,"content":"in terms of the input"},{"from":1207.79,"to":1210.4,"location":2,"content":"data is sort of very complex you could"},{"from":1210.4,"to":1212.05,"location":2,"content":"never do it by just drawing a line"},{"from":1212.05,"to":1215.2,"location":2,"content":"between the two classes and so you'd"},{"from":1215.2,"to":1217.63,"location":2,"content":"like to use some more complicated kind"},{"from":1217.63,"to":1221.23,"location":2,"content":"of classifier and so neural networks the"},{"from":1221.23,"to":1223,"location":2,"content":"multi-layer neural networks that were"},{"from":1223,"to":1224.58,"location":2,"content":"going to be staying to get into now"},{"from":1224.58,"to":1228.19,"location":2,"content":"precisely what they do is provide your"},{"from":1228.19,"to":1231.73,"location":2,"content":"way to learn very complex you know"},{"from":1231.73,"to":1235.78,"location":2,"content":"almost limitless in fact classifiers so"},{"from":1235.78,"to":1238.06,"location":2,"content":"that if you look at the decisions that"},{"from":1238.06,"to":1240.07,"location":2,"content":"they're making in terms of the original"},{"from":1240.07,"to":1243.1,"location":2,"content":"space they can be learning cases like"},{"from":1243.1,"to":1248.47,"location":2,"content":"this I put the I put the pointer on a"},{"from":1248.47,"to":1252.31,"location":2,"content":"couple of the slides here this this is a"},{"from":1252.31,"to":1254.68,"location":2,"content":"visualization that was done by andraka"},{"from":1254.68,"to":1257.2,"location":2,"content":"pothi he was a PhD student here until a"},{"from":1257.2,"to":1259.24,"location":2,"content":"couple of years ago so this is a little"},{"from":1259.24,"to":1262.18,"location":2,"content":"JavaScript app that you can find off his"},{"from":1262.18,"to":1264.22,"location":2,"content":"website and it's actually a lot of fun"},{"from":1264.22,"to":1266.31,"location":2,"content":"to play with to see what kind of"},{"from":1266.31,"to":1268.99,"location":2,"content":"decision boundaries you can get a neural"},{"from":1268.99,"to":1275.88,"location":2,"content":"net to come up with okay so for getting"},{"from":1275.88,"to":1279.64,"location":2,"content":"for getting more advanced classification"},{"from":1279.64,"to":1283.57,"location":2,"content":"out of a neural net used for natural"},{"from":1283.57,"to":1287.08,"location":2,"content":"language there are sort of two things"},{"from":1287.08,"to":1289.93,"location":2,"content":"going that you can do that I want to"},{"from":1289.93,"to":1292.3,"location":2,"content":"talk about which are in in some sense"},{"from":1292.3,"to":1295.06,"location":2,"content":"the same thing when it comes down to it"},{"from":1295.06,"to":1297.19,"location":2,"content":"but I'll sort of mention them separately"},{"from":1297.19,"to":1300.66,"location":2,"content":"at the beginning that one of them is"},{"from":1300.66,"to":1305.29,"location":2,"content":"that we have these word vectors and then"},{"from":1305.29,"to":1307.36,"location":2,"content":"the second one is that we're going to"},{"from":1307.36,"to":1310.36,"location":2,"content":"build deeper multi-layer networks okay"},{"from":1310.36,"to":1313.42,"location":2,"content":"so at first crucial difference that we"},{"from":1313.42,"to":1317.02,"location":2,"content":"already started to see with what we were"},{"from":1317.02,"to":1320.05,"location":2,"content":"doing last week is rather than sort of"},{"from":1320.05,"to":1322.21,"location":2,"content":"having a word being this is the word"},{"from":1322.21,"to":1326.71,"location":2,"content":"house we instead say house is a vector"},{"from":1326.71,"to":1330.33,"location":2,"content":"of real numbers and what we can do is"},{"from":1330.33,"to":1333.37,"location":2,"content":"change the vector that corresponds to"},{"from":1333.37,"to":1337.57,"location":2,"content":"house in such a way as we can build"},{"from":1337.57,"to":1339.67,"location":2,"content":"better classifiers which means that"},{"from":1339.67,"to":1341.56,"location":2,"content":"we're going to be sort of moving how"},{"from":1341.56,"to":1344.05,"location":2,"content":"as representation around the space to"},{"from":1344.05,"to":1345.76,"location":2,"content":"capture things that were interested in"},{"from":1345.76,"to":1348.1,"location":2,"content":"like word similarity analogies and"},{"from":1348.1,"to":1350.92,"location":2,"content":"things like that so this is actually you"},{"from":1350.92,"to":1353.37,"location":2,"content":"know kind of a weird idea compared to"},{"from":1353.37,"to":1356.68,"location":2,"content":"conventional steps or m/l so rather than"},{"from":1356.68,"to":1360.25,"location":2,"content":"saying we just have the parameters W we"},{"from":1360.25,"to":1363.33,"location":2,"content":"also say that all of these word"},{"from":1363.33,"to":1366.31,"location":2,"content":"representations are also parameters of"},{"from":1366.31,"to":1368.52,"location":2,"content":"our model so we're actually going to"},{"from":1368.52,"to":1372.31,"location":2,"content":"change the representations of words to"},{"from":1372.31,"to":1374.62,"location":2,"content":"allow our classifiers to do better"},{"from":1374.62,"to":1376.57,"location":2,"content":"so we're simultaneously changing the"},{"from":1376.57,"to":1378.46,"location":2,"content":"weights and we're changing the"},{"from":1378.46,"to":1380.56,"location":2,"content":"representation of words and we're"},{"from":1380.56,"to":1382.75,"location":2,"content":"optimizing both of them at once to try"},{"from":1382.75,"to":1385.57,"location":2,"content":"and make our model as good as possible"},{"from":1385.57,"to":1389.29,"location":2,"content":"and so this is the sense in which people"},{"from":1389.29,"to":1390.67,"location":2,"content":"often talk about for deep learning"},{"from":1390.67,"to":1393.55,"location":2,"content":"models that we're doing representation"},{"from":1393.55,"to":1397.21,"location":2,"content":"learning all right I sort of said there"},{"from":1397.21,"to":1399.34,"location":2,"content":"are two ways I was going to mention two"},{"from":1399.34,"to":1402.13,"location":2,"content":"things ones this sort of word vector"},{"from":1402.13,"to":1404.02,"location":2,"content":"representation learning and then the"},{"from":1404.02,"to":1406.12,"location":2,"content":"second one is that we're going to start"},{"from":1406.12,"to":1407.89,"location":2,"content":"looking at deeper multi-layer neural"},{"from":1407.89,"to":1411.94,"location":2,"content":"networks sort of hidden over here on the"},{"from":1411.94,"to":1415.24,"location":2,"content":"slide is the observation that really you"},{"from":1415.24,"to":1418.66,"location":2,"content":"can think of would word vector embedding"},{"from":1418.66,"to":1422.05,"location":2,"content":"as just putting your having a model with"},{"from":1422.05,"to":1424.84,"location":2,"content":"one more neural network layer so if you"},{"from":1424.84,"to":1429.1,"location":2,"content":"imagine that each word was a one hot"},{"from":1429.1,"to":1432.49,"location":2,"content":"vector with four the different word"},{"from":1432.49,"to":1434.92,"location":2,"content":"types in your model so you had a you"},{"from":1434.92,"to":1437.74,"location":2,"content":"know 150,000 dimensional vector with"},{"from":1437.74,"to":1439.69,"location":2,"content":"this or one hot encoding of different"},{"from":1439.69,"to":1442.63,"location":2,"content":"words then you could say you have a map"},{"from":1442.63,"to":1446.71,"location":2,"content":"a matrix L which is sort of your lexicon"},{"from":1446.71,"to":1451.06,"location":2,"content":"matrix and you will pass your one hot"},{"from":1451.06,"to":1454.36,"location":2,"content":"vector for a word through a layer of"},{"from":1454.36,"to":1457.12,"location":2,"content":"neural net which multiplies the one hot"},{"from":1457.12,"to":1460.9,"location":2,"content":"vector or L and the one hot vector and"},{"from":1460.9,"to":1463.36,"location":2,"content":"since this is a one hot vector what that"},{"from":1463.36,"to":1466.39,"location":2,"content":"will have the effect of doing is taking"},{"from":1466.39,"to":1471.73,"location":2,"content":"out a column of L and so really we've"},{"from":1471.73,"to":1475.06,"location":2,"content":"got an extra layer of matrix in our new"},{"from":1475.06,"to":1475.45,"location":2,"content":"own"},{"from":1475.45,"to":1478.21,"location":2,"content":"and we're learning the parameters of"},{"from":1478.21,"to":1480.67,"location":2,"content":"that matrix in the same way as we're"},{"from":1480.67,"to":1483.7,"location":2,"content":"learning a deep neural network for other"},{"from":1483.7,"to":1486.31,"location":2,"content":"purposes so mathematically that"},{"from":1486.31,"to":1488.35,"location":2,"content":"completely makes sense and that's sort"},{"from":1488.35,"to":1491.95,"location":2,"content":"of a sensible way to think about what"},{"from":1491.95,"to":1495.01,"location":2,"content":"you're doing with word embeddings and"},{"from":1495.01,"to":1497.86,"location":2,"content":"neural networks and implementation wise"},{"from":1497.86,"to":1500.71,"location":2,"content":"this makes no sense at all and no one"},{"from":1500.71,"to":1502.18,"location":2,"content":"does this because it just doesn't make"},{"from":1502.18,"to":1504.73,"location":2,"content":"sense to do a matrix multiply when the"},{"from":1504.73,"to":1507.1,"location":2,"content":"result of the matrix multiply will be"},{"from":1507.1,"to":1512.2,"location":2,"content":"okay this is word ID seventeen sort of"},{"from":1512.2,"to":1514.33,"location":2,"content":"then constructing a one hot vector of"},{"from":1514.33,"to":1516.97,"location":2,"content":"length 150,000 with the one in position"},{"from":1516.97,"to":1519.13,"location":2,"content":"17 and then doing a matrix multiply it"},{"from":1519.13,"to":1522.04,"location":2,"content":"makes no sense you just take out the"},{"from":1522.04,"to":1524.77,"location":2,"content":"column or or the row as we've discussed"},{"from":1524.77,"to":1527.38,"location":2,"content":"seventeen of your matrix and that's what"},{"from":1527.38,"to":1530.29,"location":2,"content":"everyone actually does okay"},{"from":1530.29,"to":1533.71,"location":2,"content":"here's my one obligatory picture of"},{"from":1533.71,"to":1536.62,"location":2,"content":"neurons for the class so don't miss it"},{"from":1536.62,"to":1537.94,"location":2,"content":"I'm not gonna show it again in all class"},{"from":1537.94,"to":1541.83,"location":2,"content":"okay so the origins of neural networks"},{"from":1541.83,"to":1546.61,"location":2,"content":"was in some sense to try and construct"},{"from":1546.61,"to":1550.45,"location":2,"content":"an artificial neuron that seemed to in"},{"from":1550.45,"to":1553.27,"location":2,"content":"some sense kind of capture the kind of"},{"from":1553.27,"to":1557.05,"location":2,"content":"computations that go on in human brains"},{"from":1557.05,"to":1560.8,"location":2,"content":"and it's a very loose analogy for what"},{"from":1560.8,"to":1563.2,"location":2,"content":"was produced but you know our model here"},{"from":1563.2,"to":1566.26,"location":2,"content":"is these are our this is a teeny part of"},{"from":1566.26,"to":1569.05,"location":2,"content":"our human brain so here are neurons this"},{"from":1569.05,"to":1572.53,"location":2,"content":"is a neuron cell here and so what does a"},{"from":1572.53,"to":1576.64,"location":2,"content":"neuron consist of so up the back it's"},{"from":1576.64,"to":1578.86,"location":2,"content":"got these dendrites lots of dendrites"},{"from":1578.86,"to":1581.77,"location":2,"content":"then it's got a cell body and if there's"},{"from":1581.77,"to":1585.25,"location":2,"content":"stuff coming in on the dendrites the"},{"from":1585.25,"to":1588.04,"location":2,"content":"cell body will become active and then"},{"from":1588.04,"to":1590.32,"location":2,"content":"it'll all start spiking down this long"},{"from":1590.32,"to":1593.32,"location":2,"content":"thing which is called the axon and so"},{"from":1593.32,"to":1596.2,"location":2,"content":"then these axons lead to the dendrites"},{"from":1596.2,"to":1598.48,"location":2,"content":"of a different cell or lots of different"},{"from":1598.48,"to":1600.28,"location":2,"content":"cells right this one"},{"from":1600.28,"to":1602.5,"location":2,"content":"that's right shown but some of these are"},{"from":1602.5,"to":1605.29,"location":2,"content":"kind of going to different cells and so"},{"from":1605.29,"to":1608.62,"location":2,"content":"you then have these sort of terminal"},{"from":1608.62,"to":1609.31,"location":2,"content":"buttons"},{"from":1609.31,"to":1611.23,"location":2,"content":"the Exxon which are kind of close to the"},{"from":1611.23,"to":1613.15,"location":2,"content":"dendrites but have a little gap in them"},{"from":1613.15,"to":1615.97,"location":2,"content":"and some minute miracles of biochemistry"},{"from":1615.97,"to":1618.25,"location":2,"content":"happen there and so that's the synapse"},{"from":1618.25,"to":1620.44,"location":2,"content":"across which you'll then have sort of"},{"from":1620.44,"to":1622.81,"location":2,"content":"activation flowing which goes into the"},{"from":1622.81,"to":1626.25,"location":2,"content":"next neuron so that was the starting off"},{"from":1626.25,"to":1628.54,"location":2,"content":"model that people wanted to try and"},{"from":1628.54,"to":1631.45,"location":2,"content":"simulate in computation so people came"},{"from":1631.45,"to":1634.03,"location":2,"content":"up with this model of an artificial"},{"from":1634.03,"to":1637.81,"location":2,"content":"neuron so that we have things coming in"},{"from":1637.81,"to":1640.77,"location":2,"content":"from other neurons at some level of"},{"from":1640.77,"to":1644.17,"location":2,"content":"activation so that's a number X 0 X 1 X"},{"from":1644.17,"to":1649.59,"location":2,"content":"2 then sine APS's vary depending on how"},{"from":1649.59,"to":1652.36,"location":2,"content":"excitable they are as to how easily"},{"from":1652.36,"to":1654.7,"location":2,"content":"they'll let signal cross across the"},{"from":1654.7,"to":1657.54,"location":2,"content":"synapse and so that's being modeled by"},{"from":1657.54,"to":1661.72,"location":2,"content":"multiplying them by a weight W 0 W 1 W 2"},{"from":1661.72,"to":1666.01,"location":2,"content":"and then the cell body sort of correctly"},{"from":1666.01,"to":1668.91,"location":2,"content":"is sort of summing this amount of"},{"from":1668.91,"to":1670.78,"location":2,"content":"excitation it's getting from the"},{"from":1670.78,"to":1675.55,"location":2,"content":"different dendrites and then it can have"},{"from":1675.55,"to":1677.74,"location":2,"content":"its own bias as to how likely it is to"},{"from":1677.74,"to":1681.28,"location":2,"content":"fire that's the B so we get that and"},{"from":1681.28,"to":1683.29,"location":2,"content":"then it has some overall kind of"},{"from":1683.29,"to":1686.5,"location":2,"content":"threshold or propensity for firing so we"},{"from":1686.5,"to":1687.97,"location":2,"content":"sort of stick it through an activation"},{"from":1687.97,"to":1691.87,"location":2,"content":"function which is sort of will determine"},{"from":1691.87,"to":1694.48,"location":2,"content":"a firing rate and that will be the"},{"from":1694.48,"to":1696.37,"location":2,"content":"signal that's going out on the output"},{"from":1696.37,"to":1698.77,"location":2,"content":"axon so that was sort of the starting"},{"from":1698.77,"to":1702.79,"location":2,"content":"point of that but you know really for"},{"from":1702.79,"to":1705.01,"location":2,"content":"what we've ended up computing we just"},{"from":1705.01,"to":1706.75,"location":2,"content":"have a little bit of baby math here"},{"from":1706.75,"to":1710.53,"location":2,"content":"which actually looks very familiar to"},{"from":1710.53,"to":1713.77,"location":2,"content":"the kind of baby math you see in linear"},{"from":1713.77,"to":1716.53,"location":2,"content":"algebra and statistics and so it's"},{"from":1716.53,"to":1720.18,"location":2,"content":"really no different so in particular a"},{"from":1720.18,"to":1724.39,"location":2,"content":"neuron can very easily be a binary"},{"from":1724.39,"to":1728.68,"location":2,"content":"logistic regression unit so that this is"},{"from":1728.68,"to":1731.02,"location":2,"content":"sort of for a logistic regression you're"},{"from":1731.02,"to":1732.88,"location":2,"content":"taking for your input X you're"},{"from":1732.88,"to":1735.01,"location":2,"content":"multiplying it by weight vector you're"},{"from":1735.01,"to":1740.23,"location":2,"content":"adding your bias term and then you're"},{"from":1740.23,"to":1742.81,"location":2,"content":"putting it through and"},{"from":1742.81,"to":1746.77,"location":2,"content":"linearity like the logistic function and"},{"from":1746.77,"to":1749.5,"location":2,"content":"then so you're calculating a logistic"},{"from":1749.5,"to":1753.58,"location":2,"content":"regression inside this sort of neuron"},{"from":1753.58,"to":1758.17,"location":2,"content":"model and so this is the this is the"},{"from":1758.17,"to":1759.91,"location":2,"content":"difference between the softmax and the"},{"from":1759.91,"to":1761.95,"location":2,"content":"logistic regression as I saying that"},{"from":1761.95,"to":1764.83,"location":2,"content":"whereas the softmax for two classes has"},{"from":1764.83,"to":1767.41,"location":2,"content":"two sets of parameters this sort of just"},{"from":1767.41,"to":1769.9,"location":2,"content":"has one set of parameters z and you're"},{"from":1769.9,"to":1771.91,"location":2,"content":"modeling the two classes by giving the"},{"from":1771.91,"to":1774.91,"location":2,"content":"probability of one class from zero to"},{"from":1774.91,"to":1777.52,"location":2,"content":"one depending on where the input to the"},{"from":1777.52,"to":1779.23,"location":2,"content":"logistic regression is highly negative"},{"from":1779.23,"to":1783.73,"location":2,"content":"or highly positive okay so really we can"},{"from":1783.73,"to":1786.1,"location":2,"content":"just say these artificial neurons are"},{"from":1786.1,"to":1789.34,"location":2,"content":"sort of like binary logistic regression"},{"from":1789.34,"to":1791.98,"location":2,"content":"units or we can make variants of binary"},{"from":1791.98,"to":1794.95,"location":2,"content":"logistic regression units by using some"},{"from":1794.95,"to":1797.35,"location":2,"content":"different F function and we'll come back"},{"from":1797.35,"to":1802.72,"location":2,"content":"to that again pretty soon okay well so"},{"from":1802.72,"to":1805.36,"location":2,"content":"that gives us one neuron so one neuron"},{"from":1805.36,"to":1808.09,"location":2,"content":"is a logistic regression unit for"},{"from":1808.09,"to":1810.64,"location":2,"content":"current purposes so crucially what we're"},{"from":1810.64,"to":1812.35,"location":2,"content":"wanting to do with neural networks to"},{"from":1812.35,"to":1815.56,"location":2,"content":"say well why only run one logistic"},{"from":1815.56,"to":1818.89,"location":2,"content":"regression why don't we run a whole"},{"from":1818.89,"to":1820.78,"location":2,"content":"bunch of logistic regressions at the"},{"from":1820.78,"to":1824.05,"location":2,"content":"same time so you know here our inputs"},{"from":1824.05,"to":1826.15,"location":2,"content":"and here's our little logistic"},{"from":1826.15,"to":1829,"location":2,"content":"regression unit but we could run three"},{"from":1829,"to":1831.88,"location":2,"content":"logistic regressions at the same time or"},{"from":1831.88,"to":1835.42,"location":2,"content":"we can run any number of them well"},{"from":1835.42,"to":1838.6,"location":2,"content":"that's good but sort of for conventional"},{"from":1838.6,"to":1841.63,"location":2,"content":"training of a statistical model we'd"},{"from":1841.63,"to":1844.27,"location":2,"content":"sort of have to determine for those"},{"from":1844.27,"to":1846.82,"location":2,"content":"orange outputs of the logistic"},{"from":1846.82,"to":1849.16,"location":2,"content":"regression you know what we're training"},{"from":1849.16,"to":1851.68,"location":2,"content":"each of them to try and capture with"},{"from":1851.68,"to":1854.32,"location":2,"content":"have to have data to predict what"},{"from":1854.32,"to":1857.04,"location":2,"content":"they're going to try and capture and so"},{"from":1857.04,"to":1859.75,"location":2,"content":"the secret of sort of then building"},{"from":1859.75,"to":1863.23,"location":2,"content":"began neural networks is to say we don't"},{"from":1863.23,"to":1865.42,"location":2,"content":"actually want to decide ahead of time"},{"from":1865.42,"to":1869.14,"location":2,"content":"what those little orange logistic"},{"from":1869.14,"to":1871.99,"location":2,"content":"regressions are trying to capture we"},{"from":1871.99,"to":1874.81,"location":2,"content":"want the neural network to self organize"},{"from":1874.81,"to":1875.99,"location":2,"content":"so that"},{"from":1875.99,"to":1879.71,"location":2,"content":"those orange logistic regression units"},{"from":1879.71,"to":1883.43,"location":2,"content":"learn something useful and well what is"},{"from":1883.43,"to":1884.9,"location":2,"content":"something useful"},{"from":1884.9,"to":1887.99,"location":2,"content":"well our idea is to say we do actually"},{"from":1887.99,"to":1891.77,"location":2,"content":"have some tasks that we want to do so we"},{"from":1891.77,"to":1894.83,"location":2,"content":"we have some tasks that we want to do so"},{"from":1894.83,"to":1897.41,"location":2,"content":"maybe we want to sort of decide whether"},{"from":1897.41,"to":1899.39,"location":2,"content":"a movie review is positive or negative"},{"from":1899.39,"to":1901.01,"location":2,"content":"something like sentiment analysis or"},{"from":1901.01,"to":1902.48,"location":2,"content":"something like that there is something"},{"from":1902.48,"to":1905.09,"location":2,"content":"we want to do at the end of the day and"},{"from":1905.09,"to":1908.6,"location":2,"content":"we're going to have a logistic"},{"from":1908.6,"to":1910.58,"location":2,"content":"regression classifier they're telling us"},{"from":1910.58,"to":1914.27,"location":2,"content":"positive or negative but the inputs to"},{"from":1914.27,"to":1916.22,"location":2,"content":"that aren't going to directly be"},{"from":1916.22,"to":1918.05,"location":2,"content":"something like words in the document"},{"from":1918.05,"to":1920.36,"location":2,"content":"they're going to be this intermediate"},{"from":1920.36,"to":1923.39,"location":2,"content":"layer of logistic regression units and"},{"from":1923.39,"to":1926.42,"location":2,"content":"we're going to train this whole thing to"},{"from":1926.42,"to":1929.38,"location":2,"content":"minimize our cross entropy lost and"},{"from":1929.38,"to":1931.79,"location":2,"content":"essentially what we're going to want to"},{"from":1931.79,"to":1933.05,"location":2,"content":"have happen and the backpropagation"},{"from":1933.05,"to":1936.59,"location":2,"content":"algorithm will do for us is to say you"},{"from":1936.59,"to":1939.17,"location":2,"content":"things in the middle it's your job to"},{"from":1939.17,"to":1942.98,"location":2,"content":"find some useful way to calculate values"},{"from":1942.98,"to":1945.47,"location":2,"content":"from the underlying data such that it'll"},{"from":1945.47,"to":1948.59,"location":2,"content":"help our final classifier make a good"},{"from":1948.59,"to":1951.38,"location":2,"content":"decision and I mean in particular you"},{"from":1951.38,"to":1954.77,"location":2,"content":"know back to this picture you know the"},{"from":1954.77,"to":1957.29,"location":2,"content":"final classifier it's just a linear"},{"from":1957.29,"to":1959.93,"location":2,"content":"classifier a soft max or a logistic"},{"from":1959.93,"to":1961.73,"location":2,"content":"regression it's going to have a line"},{"from":1961.73,"to":1964.24,"location":2,"content":"like this but if the intermediate"},{"from":1964.24,"to":1967.28,"location":2,"content":"classifiers they are like a word"},{"from":1967.28,"to":1968.99,"location":2,"content":"embedding they can kind of sort of"},{"from":1968.99,"to":1971.3,"location":2,"content":"re-rent the space and shift things"},{"from":1971.3,"to":1974.66,"location":2,"content":"around so they can learn to shift things"},{"from":1974.66,"to":1977.51,"location":2,"content":"around in such a way as you're learning"},{"from":1977.51,"to":1980.21,"location":2,"content":"a highly nonlinear function of the"},{"from":1980.21,"to":1990.08,"location":2,"content":"original input space okay and so at that"},{"from":1990.08,"to":1992.3,"location":2,"content":"point it's simply a matter of saying"},{"from":1992.3,"to":1995.09,"location":2,"content":"well why stop there maybe you'd get even"},{"from":1995.09,"to":1998.57,"location":2,"content":"better if we put in more layers and this"},{"from":1998.57,"to":2001.18,"location":2,"content":"sort of gets us into the area of deep"},{"from":2001.18,"to":2005.82,"location":2,"content":"learning and sort of precisely this is"},{"from":2005.82,"to":2008.98,"location":2,"content":"that sort of there was they've sort of"},{"from":2008.98,"to":2009.73,"location":2,"content":"been three"},{"from":2009.73,"to":2011.77,"location":2,"content":"cummings of neural networks as the first"},{"from":2011.77,"to":2014.56,"location":2,"content":"work in the 50s which is essentially"},{"from":2014.56,"to":2017.74,"location":2,"content":"when people had a model of a single"},{"from":2017.74,"to":2020.38,"location":2,"content":"neuron like this and then only gradually"},{"from":2020.38,"to":2022.9,"location":2,"content":"worked out how it related to more"},{"from":2022.9,"to":2026.55,"location":2,"content":"conventional statistics and there was"},{"from":2026.55,"to":2030.19,"location":2,"content":"the second version of neural networks"},{"from":2030.19,"to":2033.34,"location":2,"content":"which saw the 80s and early 90s where"},{"from":2033.34,"to":2036.13,"location":2,"content":"people built neural networks like this"},{"from":2036.13,"to":2039.12,"location":2,"content":"that had this one hidden layer where a"},{"from":2039.12,"to":2041.08,"location":2,"content":"representation could be learned in the"},{"from":2041.08,"to":2044.77,"location":2,"content":"middle but at that time it really wasn't"},{"from":2044.77,"to":2048.1,"location":2,"content":"effective of or people weren't able to"},{"from":2048.1,"to":2051.79,"location":2,"content":"build deeper networks and get them to do"},{"from":2051.79,"to":2053.8,"location":2,"content":"anything useful so you sort of had these"},{"from":2053.8,"to":2056.2,"location":2,"content":"neural networks at one hidden layers and"},{"from":2056.2,"to":2059.35,"location":2,"content":"so precisely with research that started"},{"from":2059.35,"to":2062.83,"location":2,"content":"in into deep learning that precisely the"},{"from":2062.83,"to":2066.7,"location":2,"content":"motivating question is we believe we'll"},{"from":2066.7,"to":2070.05,"location":2,"content":"be able to do even more sophisticated"},{"from":2070.05,"to":2072.88,"location":2,"content":"classification for more complex tasks"},{"from":2072.88,"to":2074.92,"location":2,"content":"things like speech recognition and image"},{"from":2074.92,"to":2077.95,"location":2,"content":"recognition if we could have a deeper"},{"from":2077.95,"to":2080.83,"location":2,"content":"network which will be able to more"},{"from":2080.83,"to":2083.5,"location":2,"content":"effectively learn more sophisticated"},{"from":2083.5,"to":2085.45,"location":2,"content":"functions of the input which will allow"},{"from":2085.45,"to":2088.48,"location":2,"content":"us to do things like recognize sounds of"},{"from":2088.48,"to":2090.88,"location":2,"content":"a language how could we possibly train"},{"from":2090.88,"to":2094.06,"location":2,"content":"such a network so it all works"},{"from":2094.06,"to":2096.4,"location":2,"content":"effectively and that's the kind of thing"},{"from":2096.4,"to":2100.3,"location":2,"content":"we'll go on to more so starting this"},{"from":2100.3,"to":2103.48,"location":2,"content":"lecture more so in the next lecture but"},{"from":2103.48,"to":2107.05,"location":2,"content":"before we get to there just to underline"},{"from":2107.05,"to":2110.2,"location":2,"content":"it again so once we have something like"},{"from":2110.2,"to":2113.92,"location":2,"content":"this as our layer of a neural network we"},{"from":2113.92,"to":2117.4,"location":2,"content":"have a vector of inputs we have a vector"},{"from":2117.4,"to":2121.44,"location":2,"content":"of outputs and everything is connected"},{"from":2121.44,"to":2124.75,"location":2,"content":"so that we've got this sort of weights"},{"from":2124.75,"to":2127.99,"location":2,"content":"along every one of these black lines and"},{"from":2127.99,"to":2131.68,"location":2,"content":"so we can say a1 is you're taking"},{"from":2131.68,"to":2134.95,"location":2,"content":"weights times each component of x1 and"},{"from":2134.95,"to":2139.27,"location":2,"content":"adding a biased and then you're going to"},{"from":2139.27,"to":2141.82,"location":2,"content":"be running which is sort of this part"},{"from":2141.82,"to":2143.62,"location":2,"content":"and then running it through"},{"from":2143.62,"to":2146.53,"location":2,"content":"our non-linearity and that will give us"},{"from":2146.53,"to":2148.54,"location":2,"content":"an output and we're going to do that for"},{"from":2148.54,"to":2153.73,"location":2,"content":"each of a 1 a 2 and a 3 so again we can"},{"from":2153.73,"to":2156.67,"location":2,"content":"kind of regard a is a vector and we can"},{"from":2156.67,"to":2159.07,"location":2,"content":"kind of collapse it into this matrix"},{"from":2159.07,"to":2161.65,"location":2,"content":"notation for working out the fix of"},{"from":2161.65,"to":2163.87,"location":2,"content":"layers so fully connected layers are"},{"from":2163.87,"to":2167.65,"location":2,"content":"effectively matrices of weights and"},{"from":2167.65,"to":2169.96,"location":2,"content":"commonly we write them like this where"},{"from":2169.96,"to":2172.21,"location":2,"content":"we have the bias term as a vector of"},{"from":2172.21,"to":2174.64,"location":2,"content":"bias terms there's sort of a choice"},{"from":2174.64,"to":2176.92,"location":2,"content":"there you can either have an always-on"},{"from":2176.92,"to":2179.95,"location":2,"content":"input and then the bias terms become"},{"from":2179.95,"to":2181.78,"location":2,"content":"part of the weights of our slightly"},{"from":2181.78,"to":2186.28,"location":2,"content":"bigger matrix of one extra one extra"},{"from":2186.28,"to":2196.56,"location":2,"content":"either column or row one extra row right"},{"from":2196.56,"to":2198.94,"location":2,"content":"or you can just sort of have them"},{"from":2198.94,"to":2203.77,"location":2,"content":"separately written as bees okay and then"},{"from":2203.77,"to":2207.21,"location":2,"content":"the final note here right so once we've"},{"from":2207.21,"to":2210.67,"location":2,"content":"calculated this part we always put"},{"from":2210.67,"to":2214.12,"location":2,"content":"things through non-linearity which is"},{"from":2214.12,"to":2215.56,"location":2,"content":"referred to as the activation function"},{"from":2215.56,"to":2218.74,"location":2,"content":"and so something like the logistic"},{"from":2218.74,"to":2220.81,"location":2,"content":"transform I showed earlier is an"},{"from":2220.81,"to":2223.27,"location":2,"content":"activation function and this is written"},{"from":2223.27,"to":2228.58,"location":2,"content":"as sort of vector import activation"},{"from":2228.58,"to":2231.7,"location":2,"content":"function giving a vector output and what"},{"from":2231.7,"to":2233.83,"location":2,"content":"this always means is that we apply this"},{"from":2233.83,"to":2237.34,"location":2,"content":"function element-wise so we applying the"},{"from":2237.34,"to":2239.67,"location":2,"content":"logistic function which is sort of a"},{"from":2239.67,"to":2243.43,"location":2,"content":"naturally a 1 input 1 output function"},{"from":2243.43,"to":2246.01,"location":2,"content":"like the little graph I showed before so"},{"from":2246.01,"to":2248.68,"location":2,"content":"when we apply that to a vector we apply"},{"from":2248.68,"to":2251.23,"location":2,"content":"it to each element of the vector element"},{"from":2251.23,"to":2258.73,"location":2,"content":"wise ok we will come back very soon to"},{"from":2258.73,"to":2262.18,"location":2,"content":"sort of saying more about nonlinearities"},{"from":2262.18,"to":2265.48,"location":2,"content":"and what nonlinearities people actually"},{"from":2265.48,"to":2269.74,"location":2,"content":"use but you know something you might be"},{"from":2269.74,"to":2272.41,"location":2,"content":"wondering is well why does he always"},{"from":2272.41,"to":2274.3,"location":2,"content":"have these nonlinearities and say there"},{"from":2274.3,"to":2276.46,"location":2,"content":"has to be an F function there you know"},{"from":2276.46,"to":2277.96,"location":2,"content":"why don't we just"},{"from":2277.96,"to":2281.17,"location":2,"content":"calculate Z equals WX plus B in one"},{"from":2281.17,"to":2282.97,"location":2,"content":"layer and then go on to another layer"},{"from":2282.97,"to":2289.03,"location":2,"content":"that also does z2 equals W to z1 plus B"},{"from":2289.03,"to":2291.58,"location":2,"content":"and keep on going with layers like that"},{"from":2291.58,"to":2294.22,"location":2,"content":"and there's a very precise reason for"},{"from":2294.22,"to":2297.34,"location":2,"content":"that which is if you want to have a"},{"from":2297.34,"to":2300.21,"location":2,"content":"neural network learn anything"},{"from":2300.21,"to":2303.16,"location":2,"content":"interesting you have to stick in some"},{"from":2303.16,"to":2305.97,"location":2,"content":"function f which is a nonlinear function"},{"from":2305.97,"to":2309.34,"location":2,"content":"such as the logistic curve I showed"},{"from":2309.34,"to":2313.98,"location":2,"content":"before and the reason for that is that"},{"from":2313.98,"to":2317.46,"location":2,"content":"if you're sort of doing linear"},{"from":2317.46,"to":2321.6,"location":2,"content":"transforms like W X plus B and then W to"},{"from":2321.6,"to":2326.95,"location":2,"content":"z1 plus B w3 z2 plus B and you're doing"},{"from":2326.95,"to":2329.53,"location":2,"content":"a sequence of linear transforms well"},{"from":2329.53,"to":2332.41,"location":2,"content":"multiple linear transforms just composed"},{"from":2332.41,"to":2334.9,"location":2,"content":"to become a linear transform right so"},{"from":2334.9,"to":2338.41,"location":2,"content":"one linear transform is rotating and"},{"from":2338.41,"to":2340.96,"location":2,"content":"stretching the space somehow and you can"},{"from":2340.96,"to":2343.93,"location":2,"content":"rotate them stretch the space again but"},{"from":2343.93,"to":2345.85,"location":2,"content":"the result of that it's just one bigger"},{"from":2345.85,"to":2348.16,"location":2,"content":"rotate and stretch of the space so you"},{"from":2348.16,"to":2350.23,"location":2,"content":"don't get any extra power for a"},{"from":2350.23,"to":2353.08,"location":2,"content":"classifier by simply having multiple"},{"from":2353.08,"to":2356.17,"location":2,"content":"linear transforms but as soon as you"},{"from":2356.17,"to":2359.13,"location":2,"content":"stick in almost any kind of"},{"from":2359.13,"to":2362.53,"location":2,"content":"non-linearity then you get additional"},{"from":2362.53,"to":2366.7,"location":2,"content":"power and so you know for in general"},{"from":2366.7,"to":2369.07,"location":2,"content":"what we're doing when we're doing deep"},{"from":2369.07,"to":2372.61,"location":2,"content":"networks in the middle of them we're not"},{"from":2372.61,"to":2376.12,"location":2,"content":"thinking oh it's really important to"},{"from":2376.12,"to":2379.54,"location":2,"content":"have non-linearity thinking about"},{"from":2379.54,"to":2382.06,"location":2,"content":"probabilities or something like that our"},{"from":2382.06,"to":2385.24,"location":2,"content":"general picture is well we want to be"},{"from":2385.24,"to":2387.7,"location":2,"content":"able to do effective function"},{"from":2387.7,"to":2390.52,"location":2,"content":"approximation or curve fitting we'd like"},{"from":2390.52,"to":2393.07,"location":2,"content":"to learn a space like this and we can"},{"from":2393.07,"to":2395.65,"location":2,"content":"only do that if we're sort of putting in"},{"from":2395.65,"to":2398.29,"location":2,"content":"some nonlinearities which allow us to"},{"from":2398.29,"to":2401.16,"location":2,"content":"learn these kind of curvy decision"},{"from":2401.16,"to":2405.07,"location":2,"content":"patterns and so so if F is used"},{"from":2405.07,"to":2409.36,"location":2,"content":"effectively for doing accurate function"},{"from":2409.36,"to":2411.41,"location":2,"content":"approximation or sort of pattern"},{"from":2411.41,"to":2417.74,"location":2,"content":"matching as you go along okay I think"},{"from":2417.74,"to":2421.04,"location":2,"content":"I'm behind already okay so that was the"},{"from":2421.04,"to":2425.69,"location":2,"content":"intro to baby neural networks all good"},{"from":2425.69,"to":2434.21,"location":2,"content":"any questions yes you have feature one"},{"from":2434.21,"to":2436.85,"location":2,"content":"and feature four if you multiply them"},{"from":2436.85,"to":2438.38,"location":2,"content":"together it's highly indicative of like"},{"from":2438.38,"to":2440.48,"location":2,"content":"the label why can you get to that"},{"from":2440.48,"to":2446.63,"location":2,"content":"product relationships yeah good question"},{"from":2446.63,"to":2450.17,"location":2,"content":"so in conventional steps you have your"},{"from":2450.17,"to":2453.92,"location":2,"content":"basic input features and when people are"},{"from":2453.92,"to":2455.63,"location":2,"content":"building something like a logistic"},{"from":2455.63,"to":2459.17,"location":2,"content":"regression model by hand people often"},{"from":2459.17,"to":2461.15,"location":2,"content":"say well something that's really"},{"from":2461.15,"to":2464.33,"location":2,"content":"important for classification is looking"},{"from":2464.33,"to":2467.21,"location":2,"content":"at the pair of feature four and feature"},{"from":2467.21,"to":2470.6,"location":2,"content":"seven the you know if both of those are"},{"from":2470.6,"to":2472.15,"location":2,"content":"true at the same time something"},{"from":2472.15,"to":2474.53,"location":2,"content":"important happens and so that's referred"},{"from":2474.53,"to":2476.63,"location":2,"content":"to normally in stats as an interaction"},{"from":2476.63,"to":2479.8,"location":2,"content":"term and you can by hand air add"},{"from":2479.8,"to":2482.41,"location":2,"content":"interaction terms to your model so"},{"from":2482.41,"to":2485.39,"location":2,"content":"essentially a large part of the secret"},{"from":2485.39,"to":2488.9,"location":2,"content":"here is having these intermediate layers"},{"from":2488.9,"to":2492.44,"location":2,"content":"they can learn build interaction terms"},{"from":2492.44,"to":2496.63,"location":2,"content":"by themselves yeah so it's sort of"},{"from":2496.63,"to":2499.43,"location":2,"content":"automating the search for higher order"},{"from":2499.43,"to":2501.02,"location":2,"content":"terms that you want to put into your"},{"from":2501.02,"to":2510.2,"location":2,"content":"model okay I'll go on other questions"},{"from":2510.2,"to":2516.5,"location":2,"content":"okay so yeah so here's a brief little"},{"from":2516.5,"to":2519.56,"location":2,"content":"interlude on a teeny bit more of NLP"},{"from":2519.56,"to":2522.11,"location":2,"content":"which is sort of a kind of problem we're"},{"from":2522.11,"to":2523.97,"location":2,"content":"going to look at for a moment so this is"},{"from":2523.97,"to":2525.43,"location":2,"content":"the task of named entity recognition"},{"from":2525.43,"to":2528.44,"location":2,"content":"that I very briefly mentioned last time"},{"from":2528.44,"to":2535.04,"location":2,"content":"so if we have some text right wasn't"},{"from":2535.04,"to":2535.49,"location":2,"content":"piripi"},{"from":2535.49,"to":2537.98,"location":2,"content":"okay okay if we"},{"from":2537.98,"to":2540.14,"location":2,"content":"have some text something that in all"},{"from":2540.14,"to":2543.13,"location":2,"content":"sorts of places people want to do is"},{"from":2543.13,"to":2546.95,"location":2,"content":"they'd like to find the names of things"},{"from":2546.95,"to":2551.78,"location":2,"content":"that are mentioned and then normally as"},{"from":2551.78,"to":2553.46,"location":2,"content":"well as finding the names of things"},{"from":2553.46,"to":2556.04,"location":2,"content":"you'd actually like to classify them so"},{"from":2556.04,"to":2557.66,"location":2,"content":"it's like to say some of them are"},{"from":2557.66,"to":2560.71,"location":2,"content":"organization some of them are people"},{"from":2560.71,"to":2564.44,"location":2,"content":"some of them are places and so you know"},{"from":2564.44,"to":2567.14,"location":2,"content":"this has lots of users you know people"},{"from":2567.14,"to":2569.06,"location":2,"content":"like to track mentions of companies and"},{"from":2569.06,"to":2570.86,"location":2,"content":"people and newspapers and things like"},{"from":2570.86,"to":2573.73,"location":2,"content":"that people when they do"},{"from":2573.73,"to":2575.45,"location":2,"content":"question-answering that a lot of the"},{"from":2575.45,"to":2577.94,"location":2,"content":"time the answers to questions what we"},{"from":2577.94,"to":2580.28,"location":2,"content":"call named entities the names of people"},{"from":2580.28,"to":2583.76,"location":2,"content":"locations organizations pop songs movie"},{"from":2583.76,"to":2585.68,"location":2,"content":"names all of those kind of things are"},{"from":2585.68,"to":2590.09,"location":2,"content":"named entities and if you want to sort"},{"from":2590.09,"to":2591.65,"location":2,"content":"of start building up a knowledge base"},{"from":2591.65,"to":2593.93,"location":2,"content":"automatically from a lot of text well"},{"from":2593.93,"to":2595.61,"location":2,"content":"what you normally want to do is get out"},{"from":2595.61,"to":2598.85,"location":2,"content":"the named entities and get out relations"},{"from":2598.85,"to":2601.43,"location":2,"content":"between them so this is a common task so"},{"from":2601.43,"to":2605.06,"location":2,"content":"how can we go about doing that and a"},{"from":2605.06,"to":2608.33,"location":2,"content":"common way of doing that is to say well"},{"from":2608.33,"to":2611.51,"location":2,"content":"we're going to go through the words one"},{"from":2611.51,"to":2614.06,"location":2,"content":"at a time and they're going to be words"},{"from":2614.06,"to":2616.64,"location":2,"content":"that are in a context just like they"},{"from":2616.64,"to":2618.71,"location":2,"content":"were forward to back and what we're"},{"from":2618.71,"to":2621.14,"location":2,"content":"going to do is run a classifier and"},{"from":2621.14,"to":2623.75,"location":2,"content":"we're going to assign them a class so"},{"from":2623.75,"to":2625.49,"location":2,"content":"we're going to say first word is"},{"from":2625.49,"to":2627.89,"location":2,"content":"organization second words organization"},{"from":2627.89,"to":2630.71,"location":2,"content":"third word isn't a named entity fourth"},{"from":2630.71,"to":2632.72,"location":2,"content":"word as a person fifth word as a person"},{"from":2632.72,"to":2635.15,"location":2,"content":"and continue down so we're running a"},{"from":2635.15,"to":2638.39,"location":2,"content":"classification of a word within a"},{"from":2638.39,"to":2640.28,"location":2,"content":"position in the text so it's got"},{"from":2640.28,"to":2645.17,"location":2,"content":"surrounding words around it and so to"},{"from":2645.17,"to":2647.99,"location":2,"content":"say what the entities are many entities"},{"from":2647.99,"to":2651.05,"location":2,"content":"are multi word terms and so the simplest"},{"from":2651.05,"to":2653.39,"location":2,"content":"thing you can imagine doing is just say"},{"from":2653.39,"to":2655.01,"location":2,"content":"well take the sequence that are all"},{"from":2655.01,"to":2657.47,"location":2,"content":"classified the same and call that be it"},{"from":2657.47,"to":2660.43,"location":2,"content":"intuition gong or something like that"},{"from":2660.43,"to":2662.6,"location":2,"content":"there's a reason why that's slightly"},{"from":2662.6,"to":2664.52,"location":2,"content":"defective and so what people often use"},{"from":2664.52,"to":2667.79,"location":2,"content":"is that Biao encoding that I show on the"},{"from":2667.79,"to":2669.44,"location":2,"content":"right but I'll just going to run ahead"},{"from":2669.44,"to":2672.05,"location":2,"content":"and not do that now"},{"from":2672.05,"to":2674.24,"location":2,"content":"so it might seem at first that named"},{"from":2674.24,"to":2676.61,"location":2,"content":"entity recognition is trivial because"},{"from":2676.61,"to":2679.37,"location":2,"content":"you know you have company names Google"},{"from":2679.37,"to":2681.29,"location":2,"content":"and Facebook or company names and"},{"from":2681.29,"to":2683.63,"location":2,"content":"whenever you see Google or Facebook you"},{"from":2683.63,"to":2686.39,"location":2,"content":"just say company and how could you be"},{"from":2686.39,"to":2688.4,"location":2,"content":"wrong but in practice there's a lot of"},{"from":2688.4,"to":2690.02,"location":2,"content":"subtlety and it's easy to be wrong a"},{"from":2690.02,"to":2691.97,"location":2,"content":"named entity recognition so this is so"},{"from":2691.97,"to":2694.76,"location":2,"content":"just some of the hard cases so it's"},{"from":2694.76,"to":2698.03,"location":2,"content":"often hard to work out the boundaries of"},{"from":2698.03,"to":2700.31,"location":2,"content":"an entity so on this sentence First"},{"from":2700.31,"to":2703.04,"location":2,"content":"National Bank donate donate two vans to"},{"from":2703.04,"to":2705.62,"location":2,"content":"future school of Fort Smith so there's"},{"from":2705.62,"to":2708.14,"location":2,"content":"presumably the name of a bank there but"},{"from":2708.14,"to":2710.9,"location":2,"content":"is it National Bank and the first is"},{"from":2710.9,"to":2712.61,"location":2,"content":"just the first word of a sentence which"},{"from":2712.61,"to":2716.09,"location":2,"content":"is capitalized here like first she"},{"from":2716.09,"to":2718.94,"location":2,"content":"ordered some food or something so kind"},{"from":2718.94,"to":2721.55,"location":2,"content":"of unclear what it is sometimes it's"},{"from":2721.55,"to":2722.99,"location":2,"content":"hard to know whether something's an"},{"from":2722.99,"to":2725.54,"location":2,"content":"entity at all so at the end of this"},{"from":2725.54,"to":2729.17,"location":2,"content":"sentence is future school the name of"},{"from":2729.17,"to":2731.75,"location":2,"content":"some exciting kind of 21st century"},{"from":2731.75,"to":2734,"location":2,"content":"school or is it just meaning it's a"},{"from":2734,"to":2735.83,"location":2,"content":"future school that's going to be built"},{"from":2735.83,"to":2737.99,"location":2,"content":"in this town right is it an entity or"},{"from":2737.99,"to":2741.14,"location":2,"content":"not at all working out the class of an"},{"from":2741.14,"to":2744.17,"location":2,"content":"entity is often difficult so to find out"},{"from":2744.17,"to":2746.81,"location":2,"content":"more about Zig Ziglar and Reid features"},{"from":2746.81,"to":2750.65,"location":2,"content":"by you know what class is exhibiting you"},{"from":2750.65,"to":2752.48,"location":2,"content":"don't know I was actually a person's"},{"from":2752.48,"to":2756.95,"location":2,"content":"name and there are various entities that"},{"from":2756.95,"to":2761.18,"location":2,"content":"are ambiguous right so Charles Schwab in"},{"from":2761.18,"to":2765.8,"location":2,"content":"text is 90% of the time and organization"},{"from":2765.8,"to":2767.39,"location":2,"content":"name because there's Charles Schwab"},{"from":2767.39,"to":2769.43,"location":2,"content":"brokerage but in this particular"},{"from":2769.43,"to":2772.52,"location":2,"content":"sentence here in Woodside where Larry"},{"from":2772.52,"to":2774.2,"location":2,"content":"Ellison and Charles Schwab can live"},{"from":2774.2,"to":2777.14,"location":2,"content":"discreetly among wooded estates that is"},{"from":2777.14,"to":2778.76,"location":2,"content":"then a reference to Charles Schwab the"},{"from":2778.76,"to":2781.21,"location":2,"content":"person so it's sort of a fair bit of"},{"from":2781.21,"to":2783.89,"location":2,"content":"understanding variously that's needed to"},{"from":2783.89,"to":2788.42,"location":2,"content":"get it right okay so what are we going"},{"from":2788.42,"to":2792.13,"location":2,"content":"to do with that and so this suggests"},{"from":2792.13,"to":2795.98,"location":2,"content":"what we want to do is build classifiers"},{"from":2795.98,"to":2800.05,"location":2,"content":"for language that work inside a context"},{"from":2800.05,"to":2803.36,"location":2,"content":"so you know in general it's not very"},{"from":2803.36,"to":2805.7,"location":2,"content":"interesting classifying a word out"},{"from":2805.7,"to":2807.56,"location":2,"content":"out of context we don't actually do that"},{"from":2807.56,"to":2810.98,"location":2,"content":"much in NLP but once you're in a context"},{"from":2810.98,"to":2814.37,"location":2,"content":"there it's interesting to do and named"},{"from":2814.37,"to":2816.41,"location":2,"content":"entity recognition as one case there are"},{"from":2816.41,"to":2818.24,"location":2,"content":"lots of other places that comes up I"},{"from":2818.24,"to":2820.04,"location":2,"content":"mean here's a slightly cool one that"},{"from":2820.04,"to":2822.43,"location":2,"content":"there are some words that can mean"},{"from":2822.43,"to":2824.96,"location":2,"content":"themselves and their opposite at the"},{"from":2824.96,"to":2827.48,"location":2,"content":"same time right so to sanction something"},{"from":2827.48,"to":2830.33,"location":2,"content":"can either mean to allow something or it"},{"from":2830.33,"to":2832.67,"location":2,"content":"can mean to punish people who do things"},{"from":2832.67,"to":2836.93,"location":2,"content":"or to seed something can either mean to"},{"from":2836.93,"to":2839.06,"location":2,"content":"plant seeds and things so you're seeding"},{"from":2839.06,"to":2841.43,"location":2,"content":"the soil or it can take seeds out of"},{"from":2841.43,"to":2843.35,"location":2,"content":"something like a watermelon right you"},{"from":2843.35,"to":2845.12,"location":2,"content":"just need to know the context as to"},{"from":2845.12,"to":2849.44,"location":2,"content":"which it is okay so that suggests the"},{"from":2849.44,"to":2852.62,"location":2,"content":"task that we can classify a word in its"},{"from":2852.62,"to":2855.29,"location":2,"content":"context of neighboring words and any"},{"from":2855.29,"to":2857.24,"location":2,"content":"hours an example of that and the"},{"from":2857.24,"to":2859.79,"location":2,"content":"question is how might we do that and a"},{"from":2859.79,"to":2862.28,"location":2,"content":"very simple way to do it might be to say"},{"from":2862.28,"to":2865.36,"location":2,"content":"well we have a bunch of words in a row"},{"from":2865.36,"to":2868.37,"location":2,"content":"which each have a word vector from"},{"from":2868.37,"to":2871.1,"location":2,"content":"something like word to Veck maybe we"},{"from":2871.1,"to":2873.62,"location":2,"content":"could just average those word vectors"},{"from":2873.62,"to":2875.93,"location":2,"content":"and then classify the resulting vector"},{"from":2875.93,"to":2878.33,"location":2,"content":"and the problem is that doesn't work"},{"from":2878.33,"to":2881.03,"location":2,"content":"very well because you lose position"},{"from":2881.03,"to":2883.07,"location":2,"content":"information you don't actually know"},{"from":2883.07,"to":2885.95,"location":2,"content":"anymore which of those word vectors is"},{"from":2885.95,"to":2886.91,"location":2,"content":"the one that you're meant to be"},{"from":2886.91,"to":2890.09,"location":2,"content":"classifying so a simple way to do better"},{"from":2890.09,"to":2892.82,"location":2,"content":"than that is to say well why don't we"},{"from":2892.82,"to":2896.63,"location":2,"content":"make a big vector of a word window so"},{"from":2896.63,"to":2899.57,"location":2,"content":"here are words and they each have a word"},{"from":2899.57,"to":2903.23,"location":2,"content":"vector and so to classify the middle"},{"from":2903.23,"to":2905.9,"location":2,"content":"word in a context of here plus or minus"},{"from":2905.9,"to":2907.82,"location":2,"content":"two words we're simply going to"},{"from":2907.82,"to":2910.31,"location":2,"content":"concatenate these five vectors together"},{"from":2910.31,"to":2913.07,"location":2,"content":"and say now we have a bigger vector and"},{"from":2913.07,"to":2916.67,"location":2,"content":"let's build a classifier over that"},{"from":2916.67,"to":2918.65,"location":2,"content":"vector so we're classifying this X"},{"from":2918.65,"to":2922.67,"location":2,"content":"window which is then a vector in our 5d"},{"from":2922.67,"to":2925.46,"location":2,"content":"if we're using d dimensional word"},{"from":2925.46,"to":2930.47,"location":2,"content":"vectors and we can do that in the kind"},{"from":2930.47,"to":2936.13,"location":2,"content":"of way that we did previously which is"},{"from":2936.13,"to":2937.46,"location":2,"content":"that"},{"from":2937.46,"to":2940.67,"location":2,"content":"we could say okay for that big vector"},{"from":2940.67,"to":2943.43,"location":2,"content":"we're going to learn W weights and we're"},{"from":2943.43,"to":2944.93,"location":2,"content":"putting going to put it through a"},{"from":2944.93,"to":2947.72,"location":2,"content":"softmax classifier and then we're going"},{"from":2947.72,"to":2950.75,"location":2,"content":"to do the decisions that's a perfectly"},{"from":2950.75,"to":2955.7,"location":2,"content":"good way to do things and for the"},{"from":2955.7,"to":2957.71,"location":2,"content":"purpose of what I want to get to in the"},{"from":2957.71,"to":2960.62,"location":2,"content":"last part of this is to start looking at"},{"from":2960.62,"to":2964.61,"location":2,"content":"my matrix calculus and you know we could"},{"from":2964.61,"to":2967.97,"location":2,"content":"use this model and do a classifier and"},{"from":2967.97,"to":2970.85,"location":2,"content":"learn the weights of it and indeed for"},{"from":2970.85,"to":2972.95,"location":2,"content":"the handout on the website that we"},{"from":2972.95,"to":2976.67,"location":2,"content":"suggest you look at it does do it with"},{"from":2976.67,"to":2980.15,"location":2,"content":"the softmax classifier of precisely this"},{"from":2980.15,"to":2983.93,"location":2,"content":"kind but for the example I do in class I"},{"from":2983.93,"to":2987.65,"location":2,"content":"try to make it a bit simpler and I want"},{"from":2987.65,"to":2989.48,"location":2,"content":"to do this I think very quickly because"},{"from":2989.48,"to":2992.39,"location":2,"content":"I'm fast running out of time so one of"},{"from":2992.39,"to":2995.39,"location":2,"content":"the famous early papers of neural NLP"},{"from":2995.39,"to":2998.15,"location":2,"content":"was this paper by Cola bear in Weston"},{"from":2998.15,"to":3001.03,"location":2,"content":"which was first an icy ml paper in 2008"},{"from":3001.03,"to":3002.89,"location":2,"content":"which actually just a couple of weeks"},{"from":3002.89,"to":3007.53,"location":2,"content":"ago won the ICML 2018 test of time award"},{"from":3007.53,"to":3010.54,"location":2,"content":"and then at there's a more recent"},{"from":3010.54,"to":3015.46,"location":2,"content":"journal version of a 2011 and they use"},{"from":3015.46,"to":3018.79,"location":2,"content":"this idea of window classification to"},{"from":3018.79,"to":3022.15,"location":2,"content":"assign classes like named entities turn"},{"from":3022.15,"to":3026.02,"location":2,"content":"to words in context but they did it in a"},{"from":3026.02,"to":3029.56,"location":2,"content":"slightly different way so what they said"},{"from":3029.56,"to":3033.43,"location":2,"content":"is well we've got these windows and this"},{"from":3033.43,"to":3037.12,"location":2,"content":"is one with their location named entity"},{"from":3037.12,"to":3039.43,"location":2,"content":"in the middle and this is one without a"},{"from":3039.43,"to":3042.04,"location":2,"content":"location entity in the middle and so"},{"from":3042.04,"to":3045.85,"location":2,"content":"what we want to do is have a system that"},{"from":3045.85,"to":3048.64,"location":2,"content":"returns a score and it should return a"},{"from":3048.64,"to":3051.16,"location":2,"content":"high score just as a real number in this"},{"from":3051.16,"to":3053.86,"location":2,"content":"case and it can should return a low"},{"from":3053.86,"to":3057.85,"location":2,"content":"score if it if there isn't a location"},{"from":3057.85,"to":3059.44,"location":2,"content":"named in the middle of the window in"},{"from":3059.44,"to":3063.19,"location":2,"content":"this case and so explicitly the model"},{"from":3063.19,"to":3065.8,"location":2,"content":"just returned the score and so if you"},{"from":3065.8,"to":3069.09,"location":2,"content":"had the top level of your neural network"},{"from":3069.09,"to":3071.88,"location":2,"content":"a-and you just then dot-product it with"},{"from":3071.88,"to":3074.82,"location":2,"content":"a vector you you then kind of with that"},{"from":3074.82,"to":3078.09,"location":2,"content":"final dot product you just return a real"},{"from":3078.09,"to":3081.27,"location":2,"content":"number and they used that as the basis"},{"from":3081.27,"to":3083.58,"location":2,"content":"of their classifier so in full glory"},{"from":3083.58,"to":3086.79,"location":2,"content":"what you had is you had this window of"},{"from":3086.79,"to":3089.82,"location":2,"content":"words you looked up a word vector for"},{"from":3089.82,"to":3094.23,"location":2,"content":"each word you then multiplied that"},{"from":3094.23,"to":3097.05,"location":2,"content":"they've all you can concatenated the"},{"from":3097.05,"to":3099,"location":2,"content":"word vectors for the window you"},{"from":3099,"to":3101.28,"location":2,"content":"multiplied them by a matrix and added a"},{"from":3101.28,"to":3104.7,"location":2,"content":"bias to get a second hidden layer which"},{"from":3104.7,"to":3107.25,"location":2,"content":"is a and then you multiply that by a"},{"from":3107.25,"to":3109.89,"location":2,"content":"final vector and that gave you a score"},{"from":3109.89,"to":3112.56,"location":2,"content":"for the window and you wanted the score"},{"from":3112.56,"to":3115.29,"location":2,"content":"to be large if it was the location and"},{"from":3115.29,"to":3120.45,"location":2,"content":"small if it wasn't a location so in this"},{"from":3120.45,"to":3122.24,"location":2,"content":"sort of pretend example where we have"},{"from":3122.24,"to":3125.37,"location":2,"content":"four-dimensional word vectors that's"},{"from":3125.37,"to":3128.13,"location":2,"content":"meaning you know for the window this is"},{"from":3128.13,"to":3133.38,"location":2,"content":"a 20 by 1 vector for calculating the"},{"from":3133.38,"to":3135.42,"location":2,"content":"next hidden layer we've got an 8 by 20"},{"from":3135.42,"to":3138.18,"location":2,"content":"matrix plus a bias vector then we've got"},{"from":3138.18,"to":3139.98,"location":2,"content":"this sort of 8 dimensional second hidden"},{"from":3139.98,"to":3142.62,"location":2,"content":"layer and then we're computing a final"},{"from":3142.62,"to":3148.65,"location":2,"content":"real number okay and so crucially this"},{"from":3148.65,"to":3150.24,"location":2,"content":"is an example of what the question was"},{"from":3150.24,"to":3153.45,"location":2,"content":"about we've put in this extra layer here"},{"from":3153.45,"to":3155.22,"location":2,"content":"right we could have just said here's a"},{"from":3155.22,"to":3158.7,"location":2,"content":"word vector a big word vector of context"},{"from":3158.7,"to":3161.7,"location":2,"content":"let's just stick a softmax or logistic"},{"from":3161.7,"to":3164.13,"location":2,"content":"classification on top to say yes or no"},{"from":3164.13,"to":3166.41,"location":2,"content":"for location but by putting in that"},{"from":3166.41,"to":3169.86,"location":2,"content":"extra hidden layer precisely this extra"},{"from":3169.86,"to":3172.53,"location":2,"content":"hidden layer can calculate nonlinear"},{"from":3172.53,"to":3175.08,"location":2,"content":"interactions between the input word"},{"from":3175.08,"to":3177.96,"location":2,"content":"vectors so it can calculate things like"},{"from":3177.96,"to":3181.47,"location":2,"content":"if the first word is a word like Museum"},{"from":3181.47,"to":3183.9,"location":2,"content":"and the second and the second words a"},{"from":3183.9,"to":3186.98,"location":2,"content":"word like the preposition in or around"},{"from":3186.98,"to":3190.41,"location":2,"content":"then that's a very good signal that this"},{"from":3190.41,"to":3193.2,"location":2,"content":"should be location in the middle"},{"from":3193.2,"to":3196.26,"location":2,"content":"position of the window so extra layers"},{"from":3196.26,"to":3198.09,"location":2,"content":"are a neural network let us calculate"},{"from":3198.09,"to":3200.7,"location":2,"content":"these kind of interaction terms between"},{"from":3200.7,"to":3202.73,"location":2,"content":"our basic features"},{"from":3202.73,"to":3207.05,"location":2,"content":"okay so there was a few more slides here"},{"from":3207.05,"to":3208.7,"location":2,"content":"that sort of go through the details of"},{"from":3208.7,"to":3211.28,"location":2,"content":"their model but I'm gonna just skip"},{"from":3211.28,"to":3213.14,"location":2,"content":"those for now because I'm a little bit"},{"from":3213.14,"to":3216.05,"location":2,"content":"behind and at the end of it we've just"},{"from":3216.05,"to":3219.02,"location":2,"content":"got this score so this is our model"},{"from":3219.02,"to":3221.15,"location":2,"content":"which is the one that I just outlined"},{"from":3221.15,"to":3223.97,"location":2,"content":"where we're calculating the score and"},{"from":3223.97,"to":3227.12,"location":2,"content":"we're wanting a big score for a location"},{"from":3227.12,"to":3230.47,"location":2,"content":"and so what we're going to want to do is"},{"from":3230.47,"to":3237.38,"location":2,"content":"consider how we can use this model to"},{"from":3237.38,"to":3240.53,"location":2,"content":"learn our parameters and a neural"},{"from":3240.53,"to":3243.74,"location":2,"content":"network so in particular remember it's"},{"from":3243.74,"to":3246.77,"location":2,"content":"the same story we've had before we had a"},{"from":3246.77,"to":3249.89,"location":2,"content":"loss function J and we're wanting to"},{"from":3249.89,"to":3253.43,"location":2,"content":"work out the gradient with respect to"},{"from":3253.43,"to":3256.1,"location":2,"content":"our current theta parameters of the loss"},{"from":3256.1,"to":3258.44,"location":2,"content":"function then we want to sort of"},{"from":3258.44,"to":3262.73,"location":2,"content":"subtract a little multiple of that given"},{"from":3262.73,"to":3264.29,"location":2,"content":"by the learning rate from our current"},{"from":3264.29,"to":3267.23,"location":2,"content":"parameters to get updated parameters and"},{"from":3267.23,"to":3269.54,"location":2,"content":"if we repeatedly do then stochastic"},{"from":3269.54,"to":3271.52,"location":2,"content":"gradient descent will have better and"},{"from":3271.52,"to":3273.89,"location":2,"content":"better parameters which give higher"},{"from":3273.89,"to":3276.08,"location":2,"content":"probability to the things that we're"},{"from":3276.08,"to":3278.54,"location":2,"content":"actually observing in our training data"},{"from":3278.54,"to":3281.9,"location":2,"content":"and so the thing we want to know is well"},{"from":3281.9,"to":3285.07,"location":2,"content":"in general how can we do this"},{"from":3285.07,"to":3287.66,"location":2,"content":"differentiation and work out the"},{"from":3287.66,"to":3291.56,"location":2,"content":"gradient of our loss function and so I"},{"from":3291.56,"to":3293.45,"location":2,"content":"sort of wanted sort of this the"},{"from":3293.45,"to":3295.85,"location":2,"content":"remaining time in this lecture go"},{"from":3295.85,"to":3299.45,"location":2,"content":"through how we can do that by hand using"},{"from":3299.45,"to":3301.88,"location":2,"content":"math and then that'll lead into sort of"},{"from":3301.88,"to":3304.46,"location":2,"content":"discussing and more generally the back"},{"from":3304.46,"to":3307.63,"location":2,"content":"propagation algorithm for the next one"},{"from":3307.63,"to":3311.48,"location":2,"content":"okay so if we're doing gradients by hand"},{"from":3311.48,"to":3314.06,"location":2,"content":"while we're doing multi variable"},{"from":3314.06,"to":3317.03,"location":2,"content":"calculus multivariable derivatives but"},{"from":3317.03,"to":3319.79,"location":2,"content":"in particular normally the most useful"},{"from":3319.79,"to":3322.97,"location":2,"content":"way to think about this is as doing"},{"from":3322.97,"to":3325.04,"location":2,"content":"matrix calculus which means we're"},{"from":3325.04,"to":3327.23,"location":2,"content":"directly working with vectors and"},{"from":3327.23,"to":3330.83,"location":2,"content":"matrices to work out our gradients and"},{"from":3330.83,"to":3334.25,"location":2,"content":"that that's normally sort of much faster"},{"from":3334.25,"to":3336.65,"location":2,"content":"and more convenient for some"},{"from":3336.65,"to":3339.14,"location":2,"content":"rising annual Network layers than trying"},{"from":3339.14,"to":3341.93,"location":2,"content":"to do it in a non victories duay but"},{"from":3341.93,"to":3343.76,"location":2,"content":"that doesn't mean that's the only way to"},{"from":3343.76,"to":3345.92,"location":2,"content":"do it if you're sort of confused about"},{"from":3345.92,"to":3348.35,"location":2,"content":"what's going on sometimes thinking it"},{"from":3348.35,"to":3350.81,"location":2,"content":"through in the non vectorized way it can"},{"from":3350.81,"to":3352.25,"location":2,"content":"be a better way to understand what's"},{"from":3352.25,"to":3355.43,"location":2,"content":"going on and make more progress so like"},{"from":3355.43,"to":3359.47,"location":2,"content":"when last time I did the word to Vic"},{"from":3359.47,"to":3362.45,"location":2,"content":"derivatives when I was writing to small"},{"from":3362.45,"to":3365.21,"location":2,"content":"on that board sorry that was doing it in"},{"from":3365.21,"to":3367.76,"location":2,"content":"a non victories way of working out the"},{"from":3367.76,"to":3370.19,"location":2,"content":"weights talking about them individually"},{"from":3370.19,"to":3372.67,"location":2,"content":"but here we're going to do it with"},{"from":3372.67,"to":3375.95,"location":2,"content":"vectors and matrices and again look for"},{"from":3375.95,"to":3377.99,"location":2,"content":"the lecture notes to cover this material"},{"from":3377.99,"to":3381.02,"location":2,"content":"in more detail in particular so that no"},{"from":3381.02,"to":3383.48,"location":2,"content":"one misses ed I'm let me just clarify"},{"from":3383.48,"to":3385.43,"location":2,"content":"what I mean by lecture notes so if you"},{"from":3385.43,"to":3387.47,"location":2,"content":"look at the course syllabus on the"},{"from":3387.47,"to":3389.36,"location":2,"content":"left-hand column"},{"from":3389.36,"to":3391.67,"location":2,"content":"there's the slides that you can download"},{"from":3391.67,"to":3394.49,"location":2,"content":"and I'm straight under the slides it"},{"from":3394.49,"to":3396.44,"location":2,"content":"says lecture notes that's what I'm"},{"from":3396.44,"to":3398.72,"location":2,"content":"meaning by the lecture notes in the in"},{"from":3398.72,"to":3401.21,"location":2,"content":"the middle column it then has some"},{"from":3401.21,"to":3403.19,"location":2,"content":"readings and actually there are some"},{"from":3403.19,"to":3404.99,"location":2,"content":"different additional things there that"},{"from":3404.99,"to":3409.01,"location":2,"content":"cover similar material so there's so"},{"from":3409.01,"to":3410.81,"location":2,"content":"there will they might be helpful as well"},{"from":3410.81,"to":3413.45,"location":2,"content":"but that's the thing that's closest to"},{"from":3413.45,"to":3415.25,"location":2,"content":"what I'm about to present it's the"},{"from":3415.25,"to":3416.66,"location":2,"content":"lecture notes that appear immediately"},{"from":3416.66,"to":3420.44,"location":2,"content":"under the slides link okay"},{"from":3420.44,"to":3424.94,"location":2,"content":"so my hope here my hope here is the"},{"from":3424.94,"to":3428.21,"location":2,"content":"following if you can't remember - how to"},{"from":3428.21,"to":3431.48,"location":2,"content":"do single variable calculus sorry you're"},{"from":3431.48,"to":3433.03,"location":2,"content":"basically sunk and may as well leave now"},{"from":3433.03,"to":3435.83,"location":2,"content":"I'm assuming you know how to do single"},{"from":3435.83,"to":3438.26,"location":2,"content":"variable calculus and I'm assuming you"},{"from":3438.26,"to":3442.01,"location":2,"content":"know what a vector and a matrix is but"},{"from":3442.01,"to":3447.23,"location":2,"content":"you know I sort of hope that even if you"},{"from":3447.23,"to":3449.99,"location":2,"content":"never did multivariable calculus or you"},{"from":3449.99,"to":3452.57,"location":2,"content":"can't remember any of it it's sort of"},{"from":3452.57,"to":3454.79,"location":2,"content":"for what we have to do here not that"},{"from":3454.79,"to":3458.57,"location":2,"content":"hard and you can do it so here's what"},{"from":3458.57,"to":3463.43,"location":2,"content":"what you do right so if we have a simple"},{"from":3463.43,"to":3466.64,"location":2,"content":"function f of X equals x cubed right"},{"from":3466.64,"to":3469.76,"location":2,"content":"it's gradient and so the gradient is the"},{"from":3469.76,"to":3470.36,"location":2,"content":"slow"},{"from":3470.36,"to":3471.98,"location":2,"content":"right it's saying how steep or shallow"},{"from":3471.98,"to":3474.53,"location":2,"content":"is the slope of something and then we"},{"from":3474.53,"to":3477.2,"location":2,"content":"and also saw the direction of slope when"},{"from":3477.2,"to":3479.69,"location":2,"content":"we go into multiple dimensions its"},{"from":3479.69,"to":3481.85,"location":2,"content":"gradient is just its derivative so it's"},{"from":3481.85,"to":3485,"location":2,"content":"derivative is three x squared so if"},{"from":3485,"to":3487.4,"location":2,"content":"you're at the point x equals three that"},{"from":3487.4,"to":3490.58,"location":2,"content":"you know there's this 27 of slope eNOS"},{"from":3490.58,"to":3494.54,"location":2,"content":"it's very steep okay so well what if we"},{"from":3494.54,"to":3497.57,"location":2,"content":"have a function with one output but now"},{"from":3497.57,"to":3501.41,"location":2,"content":"it has many inputs so that we're sort of"},{"from":3501.41,"to":3505.55,"location":2,"content":"doing that sort of function that was"},{"from":3505.55,"to":3507.35,"location":2,"content":"like the dot products where we're doing"},{"from":3507.35,"to":3512.21,"location":2,"content":"the sort of U TV or WT x to calculate a"},{"from":3512.21,"to":3514.67,"location":2,"content":"value well then what we're going to"},{"from":3514.67,"to":3518.39,"location":2,"content":"calculate is a gradient which is a"},{"from":3518.39,"to":3520.94,"location":2,"content":"vector of partial derivatives with"},{"from":3520.94,"to":3525.41,"location":2,"content":"respect to each input so you take the"},{"from":3525.41,"to":3528.89,"location":2,"content":"slope of the function as you change X 1"},{"from":3528.89,"to":3532.01,"location":2,"content":"the slope of the function as you change"},{"from":3532.01,"to":3535.52,"location":2,"content":"X 2 through the slope of the function as"},{"from":3535.52,"to":3538.22,"location":2,"content":"you changed X N and each of these you"},{"from":3538.22,"to":3540.56,"location":2,"content":"can just calculate as if you're doing"},{"from":3540.56,"to":3543.05,"location":2,"content":"single variable calculus and you just"},{"from":3543.05,"to":3545.3,"location":2,"content":"put them all in a vector and that's then"},{"from":3545.3,"to":3547.55,"location":2,"content":"giving you the gradient and then the"},{"from":3547.55,"to":3551,"location":2,"content":"gradient and multi-dimensional space is"},{"from":3551,"to":3553.37,"location":2,"content":"then giving you the direction and slope"},{"from":3553.37,"to":3555.92,"location":2,"content":"of the sort of a surface that touches"},{"from":3555.92,"to":3560.75,"location":2,"content":"your multi-dimensional F function ok so"},{"from":3560.75,"to":3563.09,"location":2,"content":"that's going a bit scarier but it gets a"},{"from":3563.09,"to":3565.01,"location":2,"content":"little bit scarier than that because if"},{"from":3565.01,"to":3568.31,"location":2,"content":"we have a neural network layer we then"},{"from":3568.31,"to":3571.67,"location":2,"content":"have a function which will have n inputs"},{"from":3571.67,"to":3574.22,"location":2,"content":"which in the input neurons and it will"},{"from":3574.22,"to":3579.29,"location":2,"content":"have M outputs so if that's the case you"},{"from":3579.29,"to":3581.3,"location":2,"content":"then have a matrix of partial"},{"from":3581.3,"to":3583.52,"location":2,"content":"derivatives which is referred to as the"},{"from":3583.52,"to":3588.2,"location":2,"content":"Jacobian so in the Jacobian you're sort"},{"from":3588.2,"to":3591.89,"location":2,"content":"of taking these partial derivatives with"},{"from":3591.89,"to":3595.46,"location":2,"content":"respect to each output along the rows"},{"from":3595.46,"to":3599.42,"location":2,"content":"and with respect to each input down the"},{"from":3599.42,"to":3601.85,"location":2,"content":"columns and so you're getting these M by"},{"from":3601.85,"to":3604.25,"location":2,"content":"n partial derivatives"},{"from":3604.25,"to":3607.19,"location":2,"content":"considering every combination of an"},{"from":3607.19,"to":3611.33,"location":2,"content":"output and an input but again you can"},{"from":3611.33,"to":3613.97,"location":2,"content":"fill in every cell of this matrix just"},{"from":3613.97,"to":3615.5,"location":2,"content":"by doing single variable calculus"},{"from":3615.5,"to":3617.06,"location":2,"content":"providing you don't get yourself"},{"from":3617.06,"to":3622.61,"location":2,"content":"confused okay then we already saw when"},{"from":3622.61,"to":3625.34,"location":2,"content":"we're doing word to vac that sort of a"},{"from":3625.34,"to":3628.52,"location":2,"content":"central tool that we have to use to work"},{"from":3628.52,"to":3633.98,"location":2,"content":"out to work out our derivatives of"},{"from":3633.98,"to":3636.86,"location":2,"content":"something like a neural network model is"},{"from":3636.86,"to":3639.08,"location":2,"content":"we have a sequence of functions that we"},{"from":3639.08,"to":3642.44,"location":2,"content":"run up one after another so in a neural"},{"from":3642.44,"to":3643.58,"location":2,"content":"network you're sort of running a"},{"from":3643.58,"to":3645.26,"location":2,"content":"sequence of functions one after another"},{"from":3645.26,"to":3649.46,"location":2,"content":"so we have to use the chain rule to work"},{"from":3649.46,"to":3651.65,"location":2,"content":"out derivatives when we compose"},{"from":3651.65,"to":3654.17,"location":2,"content":"functions so if we have one variable"},{"from":3654.17,"to":3658.82,"location":2,"content":"functions so we have Z equals 3 y and y"},{"from":3658.82,"to":3662.98,"location":2,"content":"equals x squared if we want to work out"},{"from":3662.98,"to":3667.64,"location":2,"content":"the derivative of Z with respect to X we"},{"from":3667.64,"to":3669.98,"location":2,"content":"say AHA that's a composition of two"},{"from":3669.98,"to":3673.58,"location":2,"content":"functions so I use the chain rule and so"},{"from":3673.58,"to":3677.63,"location":2,"content":"that means what I do is I multiply the"},{"from":3677.63,"to":3684.37,"location":2,"content":"derivative so I take DZ dy so that's 2x"},{"from":3684.37,"to":3688.94,"location":2,"content":"wait sorry I said that wrong right"},{"from":3688.94,"to":3691.67,"location":2,"content":"it's my example wrong oh yeah right DZ"},{"from":3691.67,"to":3694.28,"location":2,"content":"dy so ya DZ dy it's just three that's"},{"from":3694.28,"to":3695.75,"location":2,"content":"all right that's the derivative of the"},{"from":3695.75,"to":3700.4,"location":2,"content":"top line and then dy DX is 2x and I"},{"from":3700.4,"to":3702.74,"location":2,"content":"multiply those together and I get the"},{"from":3702.74,"to":3706.13,"location":2,"content":"answer but the derivative of Z with"},{"from":3706.13,"to":3711.83,"location":2,"content":"respect to X is 6x ok this bit bin gets"},{"from":3711.83,"to":3713.9,"location":2,"content":"a little bit freakier but it's true if"},{"from":3713.9,"to":3717.8,"location":2,"content":"you have lots of variables at once you"},{"from":3717.8,"to":3721.01,"location":2,"content":"simply multiply the jacobians and you"},{"from":3721.01,"to":3723.29,"location":2,"content":"get the right answer so if we're now"},{"from":3723.29,"to":3725.84,"location":2,"content":"imagining our neural net well sort of"},{"from":3725.84,"to":3728.03,"location":2,"content":"this is our typical neural net right so"},{"from":3728.03,"to":3731.18,"location":2,"content":"we're doing the neural net layer where"},{"from":3731.18,"to":3733.34,"location":2,"content":"we have our weight matrix multiplied"},{"from":3733.34,"to":3736.43,"location":2,"content":"their input vector plus the bias and"},{"from":3736.43,"to":3737.84,"location":2,"content":"then we're putting it through"},{"from":3737.84,"to":3740.48,"location":2,"content":"and non-linearity and then if we want to"},{"from":3740.48,"to":3743.6,"location":2,"content":"know what's the partials of H with"},{"from":3743.6,"to":3746.06,"location":2,"content":"respect to X we just say huh it's a"},{"from":3746.06,"to":3748.19,"location":2,"content":"function composition so this is easy to"},{"from":3748.19,"to":3751.22,"location":2,"content":"do we work out our first Jacobian which"},{"from":3751.22,"to":3753.5,"location":2,"content":"is the pass rules of H with respect to Z"},{"from":3753.5,"to":3755.6,"location":2,"content":"and then we just multiply it by the"},{"from":3755.6,"to":3758.39,"location":2,"content":"partials of Z with respect to X and we"},{"from":3758.39,"to":3762.43,"location":2,"content":"get the right answer"},{"from":3762.43,"to":3764.42,"location":2,"content":"easy"},{"from":3764.42,"to":3769.67,"location":2,"content":"so here's sort of an example Jacobian"},{"from":3769.67,"to":3771.65,"location":2,"content":"which is a service special case that"},{"from":3771.65,"to":3774.77,"location":2,"content":"comes up a lot so it's just good to"},{"from":3774.77,"to":3778.1,"location":2,"content":"realize this one which we'll see with"},{"from":3778.1,"to":3780.32,"location":2,"content":"our neural net so well one of the things"},{"from":3780.32,"to":3781.79,"location":2,"content":"that we have of these element-wise"},{"from":3781.79,"to":3784.67,"location":2,"content":"activation functions so we have h equals"},{"from":3784.67,"to":3789.71,"location":2,"content":"f of z so what is the partial derivative"},{"from":3789.71,"to":3795.5,"location":2,"content":"of H with respect to Z well the thing"},{"from":3795.5,"to":3797.45,"location":2,"content":"remember that we sort of apply this"},{"from":3797.45,"to":3800,"location":2,"content":"element-wise they're actually saying H I"},{"from":3800,"to":3804.89,"location":2,"content":"equals F of Z I so you know formally"},{"from":3804.89,"to":3808.19,"location":2,"content":"this function has n inputs and an output"},{"from":3808.19,"to":3810.92,"location":2,"content":"so it's partial derivatives are going to"},{"from":3810.92,"to":3813.74,"location":2,"content":"be an N by n Jacobian but if we think"},{"from":3813.74,"to":3818.57,"location":2,"content":"about what's happening there what we're"},{"from":3818.57,"to":3820.85,"location":2,"content":"actually going to find is sort of when"},{"from":3820.85,"to":3823.58,"location":2,"content":"we're working out the terms of this so"},{"from":3823.58,"to":3826.42,"location":2,"content":"we're working out how does f of Z I"},{"from":3826.42,"to":3833.42,"location":2,"content":"change as you change Z J well if J is"},{"from":3833.42,"to":3836.06,"location":2,"content":"not equal to I it's going to make no"},{"from":3836.06,"to":3838.28,"location":2,"content":"difference at all right so if my F"},{"from":3838.28,"to":3839.78,"location":2,"content":"function is something like putting it"},{"from":3839.78,"to":3841.31,"location":2,"content":"through the logistic function or"},{"from":3841.31,"to":3843.92,"location":2,"content":"anything else absolute valuing a number"},{"from":3843.92,"to":3846.2,"location":2,"content":"it's going to make no difference to the"},{"from":3846.2,"to":3849.71,"location":2,"content":"calculation of F of Z I if I change Z J"},{"from":3849.71,"to":3851.69,"location":2,"content":"because it's just not in the equation"},{"from":3851.69,"to":3854.93,"location":2,"content":"and so therefore the only terms that are"},{"from":3854.93,"to":3857.72,"location":2,"content":"actually going to occur and be nonzero"},{"from":3857.72,"to":3862.58,"location":2,"content":"are the terms where I equals J so for"},{"from":3862.58,"to":3864.62,"location":2,"content":"working out these partial derivatives if"},{"from":3864.62,"to":3869.33,"location":2,"content":"I does not equal J it's 0 if I does"},{"from":3869.33,"to":3870.79,"location":2,"content":"equal J"},{"from":3870.79,"to":3873.4,"location":2,"content":"we have to work out as single variable"},{"from":3873.4,"to":3877.72,"location":2,"content":"calculus what's the derivative of the"},{"from":3877.72,"to":3884.26,"location":2,"content":"activation function for and so this is"},{"from":3884.26,"to":3887.92,"location":2,"content":"what our Jacobian looks like for an"},{"from":3887.92,"to":3889.33,"location":2,"content":"activation function"},{"from":3889.33,"to":3892.3,"location":2,"content":"it's a diagonal matrix everything else"},{"from":3892.3,"to":3895.03,"location":2,"content":"is zero and we've got this activation"},{"from":3895.03,"to":3897.37,"location":2,"content":"function we work out its derivative and"},{"from":3897.37,"to":3899.17,"location":2,"content":"then we calculate that for the"},{"from":3899.17,"to":3901.75,"location":2,"content":"difference we have it for the different"},{"from":3901.75,"to":3908.52,"location":2,"content":"kind of zi values okay"},{"from":3908.52,"to":3913.36,"location":2,"content":"so that's a jacobians for an activation"},{"from":3913.36,"to":3915.48,"location":2,"content":"function what are the other main cases"},{"from":3915.48,"to":3918.19,"location":2,"content":"that we need for a neural network and"},{"from":3918.19,"to":3920.44,"location":2,"content":"these are gone through a little bit more"},{"from":3920.44,"to":3924.01,"location":2,"content":"slowly in these same lecture notes but"},{"from":3924.01,"to":3925.72,"location":2,"content":"they're kind of similar to what we saw"},{"from":3925.72,"to":3928.48,"location":2,"content":"in the very first class so if we are"},{"from":3928.48,"to":3930.04,"location":2,"content":"wanting to work out the partial"},{"from":3930.04,"to":3932.92,"location":2,"content":"derivatives of W X plus B with respect"},{"from":3932.92,"to":3942.22,"location":2,"content":"to X what we get is W and if we want to"},{"from":3942.22,"to":3944.29,"location":2,"content":"work out the partial derivative of W X"},{"from":3944.29,"to":3948.49,"location":2,"content":"plus B with respect to B that's means"},{"from":3948.49,"to":3951.91,"location":2,"content":"that we get an identity matrix because B"},{"from":3951.91,"to":3954.31,"location":2,"content":"is sort of like a 1b right it's this"},{"from":3954.31,"to":3956.68,"location":2,"content":"almost always on vector so you're just"},{"from":3956.68,"to":3958.45,"location":2,"content":"getting the ones coming out to preserve"},{"from":3958.45,"to":3965.29,"location":2,"content":"the B this was the case that we saw when"},{"from":3965.29,"to":3967.87,"location":2,"content":"we were doing the word vectors that if"},{"from":3967.87,"to":3971.77,"location":2,"content":"you have a vector dot product of U and"},{"from":3971.77,"to":3974.2,"location":2,"content":"age and you say what's the partial"},{"from":3974.2,"to":3977.11,"location":2,"content":"derivatives of that with respect to U"},{"from":3977.11,"to":3982.75,"location":2,"content":"then you get out H transpose if you"},{"from":3982.75,"to":3985.51,"location":2,"content":"haven't seen those before"},{"from":3985.51,"to":3989.14,"location":2,"content":"look at the lecture note handouts and"},{"from":3989.14,"to":3991.06,"location":2,"content":"see if you can compute them and they"},{"from":3991.06,"to":3995.35,"location":2,"content":"make sense at home but for the moment"},{"from":3995.35,"to":3997.75,"location":2,"content":"we're going to believe those and use"},{"from":3997.75,"to":4000.87,"location":2,"content":"those to see how we can then work out"},{"from":4000.87,"to":4004,"location":2,"content":"derivatives inside in your network"},{"from":4004,"to":4007.34,"location":2,"content":"okay so here's this same neural network"},{"from":4007.34,"to":4010.28,"location":2,"content":"we saw before so we have a window of"},{"from":4010.28,"to":4013.31,"location":2,"content":"words we're looking upward vectors we're"},{"from":4013.31,"to":4015.02,"location":2,"content":"putting it through a hidden layer and"},{"from":4015.02,"to":4017.02,"location":2,"content":"then we're just doing a vector model"},{"from":4017.02,"to":4020.39,"location":2,"content":"vector dot product yet this final score"},{"from":4020.39,"to":4023.63,"location":2,"content":"and so what we want to do to be able to"},{"from":4023.63,"to":4026.57,"location":2,"content":"train our neural network is we want to"},{"from":4026.57,"to":4032.42,"location":2,"content":"find out how how s changes depending on"},{"from":4032.42,"to":4035.87,"location":2,"content":"all the parameters of the model the X"},{"from":4035.87,"to":4040.07,"location":2,"content":"the W the be the U and so we want to"},{"from":4040.07,"to":4043.82,"location":2,"content":"work out partial derivatives of s with"},{"from":4043.82,"to":4047,"location":2,"content":"respect to each of those because we can"},{"from":4047,"to":4050.78,"location":2,"content":"then work out okay if you move B up the"},{"from":4050.78,"to":4054.05,"location":2,"content":"score gets better which is good if it's"},{"from":4054.05,"to":4056.3,"location":2,"content":"actually a Paris in the middle and"},{"from":4056.3,"to":4058.24,"location":2,"content":"therefore we will want to nudge up"},{"from":4058.24,"to":4063.71,"location":2,"content":"elements of B appropriately okay and so"},{"from":4063.71,"to":4065.06,"location":2,"content":"I'm just doing the gradient with respect"},{"from":4065.06,"to":4067.01,"location":2,"content":"to the score here and I skipped over"},{"from":4067.01,"to":4070.58,"location":2,"content":"those couple of slides so if you just"},{"from":4070.58,"to":4072.68,"location":2,"content":"sort of staring at this picture and say"},{"from":4072.68,"to":4074.54,"location":2,"content":"well how do I work out the partial"},{"from":4074.54,"to":4078.37,"location":2,"content":"derivatives of s with respect to B"},{"from":4078.37,"to":4081.32,"location":2,"content":"probably doesn't look obvious so the"},{"from":4081.32,"to":4083.27,"location":2,"content":"first thing you that you want to do is"},{"from":4083.27,"to":4086.03,"location":2,"content":"sort of break up the equations into"},{"from":4086.03,"to":4088.01,"location":2,"content":"simple pieces that compose together"},{"from":4088.01,"to":4092.51,"location":2,"content":"right so you have the input X and then"},{"from":4092.51,"to":4096.38,"location":2,"content":"that goes into Z equals W X plus B and"},{"from":4096.38,"to":4099.14,"location":2,"content":"then you compose that with the next"},{"from":4099.14,"to":4102.26,"location":2,"content":"thing so H equals F of Z our activation"},{"from":4102.26,"to":4104.69,"location":2,"content":"function and then this H goes into the"},{"from":4104.69,"to":4107.99,"location":2,"content":"next thing of s equals u th so we've got"},{"from":4107.99,"to":4110.87,"location":2,"content":"these sequence of functions and pretty"},{"from":4110.87,"to":4114.29,"location":2,"content":"much you want to break things up as much"},{"from":4114.29,"to":4116.33,"location":2,"content":"as you can I mean I could have broken"},{"from":4116.33,"to":4119.06,"location":2,"content":"this up even further I could have said Z"},{"from":4119.06,"to":4124.25,"location":2,"content":"1 equals W X Z equals Z 1 plus B it"},{"from":4124.25,"to":4126.65,"location":2,"content":"turns out that if you've just got things"},{"from":4126.65,"to":4128.93,"location":2,"content":"added and subtracted you can sort of do"},{"from":4128.93,"to":4130.7,"location":2,"content":"that in one step because there's sort of"},{"from":4130.7,"to":4133.31,"location":2,"content":"pathways separate and there when doing"},{"from":4133.31,"to":4134.87,"location":2,"content":"the derivatives but sort of anything"},{"from":4134.87,"to":4137.09,"location":2,"content":"else that composes together you want to"},{"from":4137.09,"to":4137.57,"location":2,"content":"pull it"},{"from":4137.57,"to":4140.9,"location":2,"content":"for the pieces okay so now a neural net"},{"from":4140.9,"to":4144.11,"location":2,"content":"is doing a sequence of function"},{"from":4144.11,"to":4147.11,"location":2,"content":"compositions and when we say okay we"},{"from":4147.11,"to":4149.6,"location":2,"content":"know how to do that the chain rule so if"},{"from":4149.6,"to":4152.36,"location":2,"content":"you want to work out the partial of s"},{"from":4152.36,"to":4155.06,"location":2,"content":"with respect to B it's just going to be"},{"from":4155.06,"to":4158.18,"location":2,"content":"the product of the derivatives of each"},{"from":4158.18,"to":4161.05,"location":2,"content":"step along the way so it's going to be"},{"from":4161.05,"to":4163.97,"location":2,"content":"the partial of this with respect to H"},{"from":4163.97,"to":4166.91,"location":2,"content":"times H with respect to Z times Z with"},{"from":4166.91,"to":4169.31,"location":2,"content":"respect to B and that will give us the"},{"from":4169.31,"to":4172.31,"location":2,"content":"right answer so then all we have to do"},{"from":4172.31,"to":4178.13,"location":2,"content":"is actually compute that so I think this"},{"from":4178.13,"to":4179.84,"location":2,"content":"just sort of shows okay we're taking the"},{"from":4179.84,"to":4181.57,"location":2,"content":"partial sweet step of that composition"},{"from":4181.57,"to":4185.3,"location":2,"content":"okay so now we want to compute that and"},{"from":4185.3,"to":4187.07,"location":2,"content":"so this is where I'm going to sort of"},{"from":4187.07,"to":4190.19,"location":2,"content":"use the jacobians that I sort of"},{"from":4190.19,"to":4192.95,"location":2,"content":"asserted without much proof on the"},{"from":4192.95,"to":4198.83,"location":2,"content":"preceding slide okay so first of all we"},{"from":4198.83,"to":4202.4,"location":2,"content":"have D sdh well that's the dot product"},{"from":4202.4,"to":4208.85,"location":2,"content":"of two vectors so the the Jacobian for"},{"from":4208.85,"to":4211.94,"location":2,"content":"that is just H transpose okay that's a"},{"from":4211.94,"to":4216.2,"location":2,"content":"start then we have H equals F of Z well"},{"from":4216.2,"to":4220.3,"location":2,"content":"that's the activation function so the"},{"from":4220.3,"to":4223.16,"location":2,"content":"Jacobian of that is this diagonal matrix"},{"from":4223.16,"to":4227,"location":2,"content":"made of the element wise derivative of"},{"from":4227,"to":4231.29,"location":2,"content":"the function f and then we have the"},{"from":4231.29,"to":4233.78,"location":2,"content":"partial of Z with respect to B and"},{"from":4233.78,"to":4235.58,"location":2,"content":"that's the bit that comes out as the"},{"from":4235.58,"to":4238.04,"location":2,"content":"identity matrix and so that's then"},{"from":4238.04,"to":4243.44,"location":2,"content":"giving us our calculation of the partial"},{"from":4243.44,"to":4248.75,"location":2,"content":"of s with respect to B and so we can see"},{"from":4248.75,"to":4251.6,"location":2,"content":"that the the identity matrix sort of"},{"from":4251.6,"to":4254.3,"location":2,"content":"goes away so we end up with this"},{"from":4254.3,"to":4257.62,"location":2,"content":"composition of H T times F prime of Z"},{"from":4257.62,"to":4260.9,"location":2,"content":"okay suppose we then want to go on and"},{"from":4260.9,"to":4264.44,"location":2,"content":"compute now the partial of s with"},{"from":4264.44,"to":4267.8,"location":2,"content":"respect to W well our starting off point"},{"from":4267.8,"to":4270.74,"location":2,"content":"is exactly the same chain rule that we"},{"from":4270.74,"to":4271.28,"location":2,"content":"work"},{"from":4271.28,"to":4276.4,"location":2,"content":"each of the stages so that first of all"},{"from":4276.4,"to":4281.78,"location":2,"content":"you're working at the Z from the W X"},{"from":4281.78,"to":4283.4,"location":2,"content":"part then putting it through the"},{"from":4283.4,"to":4286.91,"location":2,"content":"non-linearity then doing the dot product"},{"from":4286.91,"to":4289.25,"location":2,"content":"of the vectors so that part is the same"},{"from":4289.25,"to":4293.18,"location":2,"content":"and what you should notice is that if"},{"from":4293.18,"to":4295.79,"location":2,"content":"you compare the partial of s with"},{"from":4295.79,"to":4299.8,"location":2,"content":"respect to W versus s with respect to B"},{"from":4299.8,"to":4302.93,"location":2,"content":"most of them are the same and it's only"},{"from":4302.93,"to":4305.66,"location":2,"content":"the part at the end that's different and"},{"from":4305.66,"to":4307.94,"location":2,"content":"that sort of makes sense in terms of our"},{"from":4307.94,"to":4311.33,"location":2,"content":"neural net right that when we had our"},{"from":4311.33,"to":4316.16,"location":2,"content":"neural net that the W and the B were"},{"from":4316.16,"to":4318.53,"location":2,"content":"coming in here and once you've sort of"},{"from":4318.53,"to":4320.75,"location":2,"content":"done some stuff with them you're putting"},{"from":4320.75,"to":4323.06,"location":2,"content":"things through the same activation"},{"from":4323.06,"to":4325.64,"location":2,"content":"function and doing the same dot product"},{"from":4325.64,"to":4327.77,"location":2,"content":"to create a score so you're sort of"},{"from":4327.77,"to":4329.87,"location":2,"content":"doing the same calculations that you are"},{"from":4329.87,"to":4331.88,"location":2,"content":"then composing with so it sort of makes"},{"from":4331.88,"to":4333.92,"location":2,"content":"sense that you should be getting the"},{"from":4333.92,"to":4337.16,"location":2,"content":"same derivatives that are occur same"},{"from":4337.16,"to":4338.96,"location":2,"content":"partial derivatives that occurring at"},{"from":4338.96,"to":4346.13,"location":2,"content":"that point and so effectively you know"},{"from":4346.13,"to":4349.85,"location":2,"content":"these partial derivatives correspond to"},{"from":4349.85,"to":4352.16,"location":2,"content":"the computations in the neural network"},{"from":4352.16,"to":4356.3,"location":2,"content":"that are above where W and B are and so"},{"from":4356.3,"to":4360.47,"location":2,"content":"those are commonly referred to as Delta"},{"from":4360.47,"to":4362.42,"location":2,"content":"note Delta which is different from"},{"from":4362.42,"to":4365.72,"location":2,"content":"partial derivative D and so Delta is"},{"from":4365.72,"to":4368.21,"location":2,"content":"referred to as the error signal a neural"},{"from":4368.21,"to":4370.97,"location":2,"content":"network torque so it's the what you're"},{"from":4370.97,"to":4373.82,"location":2,"content":"calculating as the partial derivatives"},{"from":4373.82,"to":4376.88,"location":2,"content":"above the parameters that you're working"},{"from":4376.88,"to":4378.41,"location":2,"content":"out the partial derivatives with respect"},{"from":4378.41,"to":4383.27,"location":2,"content":"to so a lot of the secret as we'll see"},{"from":4383.27,"to":4387.62,"location":2,"content":"next time a lot of the secret of what"},{"from":4387.62,"to":4392.09,"location":2,"content":"happens with back propagation is just we"},{"from":4392.09,"to":4395.3,"location":2,"content":"want to do efficient computation in the"},{"from":4395.3,"to":4397.07,"location":2,"content":"sort of way that's computer science"},{"from":4397.07,"to":4398.72,"location":2,"content":"people like to do efficient computation"},{"from":4398.72,"to":4401.81,"location":2,"content":"and so precisely what we want to notice"},{"from":4401.81,"to":4404.87,"location":2,"content":"is that there is one error signal that"},{"from":4404.87,"to":4405.14,"location":2,"content":"come"},{"from":4405.14,"to":4407.6,"location":2,"content":"from above and we want to compute at"},{"from":4407.6,"to":4410.15,"location":2,"content":"once and then reuse that when"},{"from":4410.15,"to":4412.91,"location":2,"content":"calculating both partial derivatives"},{"from":4412.91,"to":4418.6,"location":2,"content":"with respect to W and with B okay so"},{"from":4418.6,"to":4422.83,"location":2,"content":"there are sort of two things to still do"},{"from":4422.83,"to":4428.45,"location":2,"content":"so one is well it'd be kind of useful to"},{"from":4428.45,"to":4430.31,"location":2,"content":"know what the partial derivative of s"},{"from":4430.31,"to":4433.25,"location":2,"content":"with respect to W actually looks like I"},{"from":4433.25,"to":4436.48,"location":2,"content":"mean is that a number vector a matrix a"},{"from":4436.48,"to":4438.89,"location":2,"content":"three-dimensional tensor and then we"},{"from":4438.89,"to":4441.89,"location":2,"content":"actually want to work out its values and"},{"from":4441.89,"to":4445.07,"location":2,"content":"to work out its values we're going to"},{"from":4445.07,"to":4446.69,"location":2,"content":"still have to work out the partial"},{"from":4446.69,"to":4450.05,"location":2,"content":"derivative of Z with respect to W but if"},{"from":4450.05,"to":4452.36,"location":2,"content":"first of all we just try and work out"},{"from":4452.36,"to":4456.47,"location":2,"content":"its shape what kind of shape does it"},{"from":4456.47,"to":4459.05,"location":2,"content":"have and this is actually sort of a bit"},{"from":4459.05,"to":4462.26,"location":2,"content":"tricky and is sort of a dirty underbelly"},{"from":4462.26,"to":4465.53,"location":2,"content":"of doing this kind of matrix calculus so"},{"from":4465.53,"to":4469.1,"location":2,"content":"since our weight vector is an N by M"},{"from":4469.1,"to":4474.56,"location":2,"content":"matrix the end result of the partial of"},{"from":4474.56,"to":4476.6,"location":2,"content":"s with respect to W is we have a"},{"from":4476.6,"to":4480.53,"location":2,"content":"function with n times M inputs all of"},{"from":4480.53,"to":4483.89,"location":2,"content":"the elements of W and simply one output"},{"from":4483.89,"to":4487.07,"location":2,"content":"which is our score so that makes it"},{"from":4487.07,"to":4488.81,"location":2,"content":"sound like according to what I said"},{"from":4488.81,"to":4492.97,"location":2,"content":"before we should have a 1 by n times m"},{"from":4492.97,"to":4495.65,"location":2,"content":"Jacobian but it turns out that's not"},{"from":4495.65,"to":4497.33,"location":2,"content":"really what we want right because what"},{"from":4497.33,"to":4501.14,"location":2,"content":"we wanted to do is use what we calculate"},{"from":4501.14,"to":4504.82,"location":2,"content":"inside this stochastic gradient descent"},{"from":4504.82,"to":4509.69,"location":2,"content":"update algorithm and if we're doing this"},{"from":4509.69,"to":4513.74,"location":2,"content":"we'd sort of like to have the old white"},{"from":4513.74,"to":4516.8,"location":2,"content":"matrix and we'd like to subtract a bit"},{"from":4516.8,"to":4519.41,"location":2,"content":"format to get a new weight matrix so"},{"from":4519.41,"to":4522.02,"location":2,"content":"it'd be kind of nice if the shape of our"},{"from":4522.02,"to":4527.74,"location":2,"content":"Jacobian was the same shape as W and so"},{"from":4527.74,"to":4531.41,"location":2,"content":"we we and in general what you always"},{"from":4531.41,"to":4534.44,"location":2,"content":"want to do with neural nets is follow"},{"from":4534.44,"to":4537.38,"location":2,"content":"what we call the shape convention which"},{"from":4537.38,"to":4538.94,"location":2,"content":"is we're going to sort of"},{"from":4538.94,"to":4542.69,"location":2,"content":"represent the jacobian so it's in the"},{"from":4542.69,"to":4547.19,"location":2,"content":"same shape as the inputs and this whole"},{"from":4547.19,"to":4552.68,"location":2,"content":"thing is kind of the bad part of the bad"},{"from":4552.68,"to":4556.28,"location":2,"content":"part of doing matrix calculus like"},{"from":4556.28,"to":4558.38,"location":2,"content":"there's a lot of inconsistency as to how"},{"from":4558.38,"to":4561.02,"location":2,"content":"people represent matrix calculus that in"},{"from":4561.02,"to":4562.25,"location":2,"content":"general if you just go to different"},{"from":4562.25,"to":4564.32,"location":2,"content":"fields like economics and physics some"},{"from":4564.32,"to":4566.12,"location":2,"content":"people use a numerator convention some"},{"from":4566.12,"to":4567.98,"location":2,"content":"people use a denominator convention"},{"from":4567.98,"to":4569.6,"location":2,"content":"we're using neither of those we're going"},{"from":4569.6,"to":4571.85,"location":2,"content":"to use this shape convention so we match"},{"from":4571.85,"to":4573.74,"location":2,"content":"the shape of the input so it makes it"},{"from":4573.74,"to":4579.19,"location":2,"content":"easy to do our weight updates ok so"},{"from":4579.19,"to":4581.9,"location":2,"content":"right so that's what we want the answer"},{"from":4581.9,"to":4584.42,"location":2,"content":"to look like so then the final thing we"},{"from":4584.42,"to":4587.15,"location":2,"content":"need to do to work out and the partial"},{"from":4587.15,"to":4589.22,"location":2,"content":"of s with respect to W is we have the"},{"from":4589.22,"to":4590.75,"location":2,"content":"error signal Delta that's going to be"},{"from":4590.75,"to":4592.82,"location":2,"content":"part of the answer and then we want to"},{"from":4592.82,"to":4595.67,"location":2,"content":"work out the partial of Z with respect"},{"from":4595.67,"to":4604.67,"location":2,"content":"to W well what's that going to be well"},{"from":4604.67,"to":4608.03,"location":2,"content":"it turns out and I'm about to be Saved"},{"from":4608.03,"to":4610.37,"location":2,"content":"by the Bell here since I'm down to two"},{"from":4610.37,"to":4613.46,"location":2,"content":"minutes left it turns out that what we"},{"from":4613.46,"to":4617.75,"location":2,"content":"end up with for that is we take the"},{"from":4617.75,"to":4621.23,"location":2,"content":"product of the partial the product of"},{"from":4621.23,"to":4624.38,"location":2,"content":"Delta times X so effectively we've got a"},{"from":4624.38,"to":4628.04,"location":2,"content":"local Eris's signal above W and then we"},{"from":4628.04,"to":4630.59,"location":2,"content":"have the inputs X and we're working out"},{"from":4630.59,"to":4635.9,"location":2,"content":"an outer product of them and the sort of"},{"from":4635.9,"to":4638.18,"location":2,"content":"way to think about this is sort of for"},{"from":4638.18,"to":4641.42,"location":2,"content":"the W's you know we've got the elements"},{"from":4641.42,"to":4644.27,"location":2,"content":"of the W matrix are these different"},{"from":4644.27,"to":4647.21,"location":2,"content":"connections between our neurons and so"},{"from":4647.21,"to":4649.94,"location":2,"content":"each one of these is connecting one"},{"from":4649.94,"to":4653.66,"location":2,"content":"output to one input and so we're going"},{"from":4653.66,"to":4656.78,"location":2,"content":"to be sort of making this in by M matrix"},{"from":4656.78,"to":4659.15,"location":2,"content":"of our partial derivatives there's going"},{"from":4659.15,"to":4662.18,"location":2,"content":"to be the product of the error signal"},{"from":4662.18,"to":4664.63,"location":2,"content":"for the but the appropriate output"},{"from":4664.63,"to":4667.61,"location":2,"content":"multiplied by our input and those goes"},{"from":4667.61,"to":4672.44,"location":2,"content":"give us the partial derivatives"},{"from":4672.44,"to":4674.93,"location":2,"content":"I'm skipping ahead quickly in my last 1"},{"from":4674.93,"to":4680.48,"location":2,"content":"minute okay so right so this is sort of"},{"from":4680.48,"to":4682.01,"location":2,"content":"what I said have used the shape can"},{"from":4682.01,"to":4684.43,"location":2,"content":"convince and I'm going to skip that"},{"from":4684.43,"to":4688.97,"location":2,"content":"okay so I've I ran out of time a teeny"},{"from":4688.97,"to":4691.04,"location":2,"content":"bit at the end but I mean I think"},{"from":4691.04,"to":4694.52,"location":2,"content":"hopefully it's conveyed most of the idea"},{"from":4694.52,"to":4697.55,"location":2,"content":"of how you can sort of use the chain"},{"from":4697.55,"to":4700.82,"location":2,"content":"rule and work out the derivatives and"},{"from":4700.82,"to":4703.46,"location":2,"content":"work them out in terms of these vector"},{"from":4703.46,"to":4707.45,"location":2,"content":"and matrix derivatives and essentially"},{"from":4707.45,"to":4709.28,"location":2,"content":"what we want to do for back propagation"},{"from":4709.28,"to":4712.22,"location":2,"content":"is to say how can we do and get a"},{"from":4712.22,"to":4715.19,"location":2,"content":"computer to do this automatically for us"},{"from":4715.19,"to":4717.83,"location":2,"content":"and to do it efficiently and that's what"},{"from":4717.83,"to":4719.42,"location":2,"content":"sort of the deep learning frameworks"},{"from":4719.42,"to":4722.27,"location":2,"content":"like tensor flow and PI torch do and how"},{"from":4722.27,"to":4723.92,"location":2,"content":"you can do that we will look at more"},{"from":4723.92,"to":4726.28,"location":2,"content":"next time"}]}